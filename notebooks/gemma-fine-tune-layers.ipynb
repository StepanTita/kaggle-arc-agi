{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b614b60613d7dbd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:02:35.287740Z",
     "start_time": "2024-08-31T20:02:35.163856Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.14\r\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92cd751ac5bf895",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:02:36.154740Z",
     "start_time": "2024-08-31T20:02:36.151446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6968b3350900c8ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:02:36.314606Z",
     "start_time": "2024-08-31T20:02:36.311412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getenv('CUDA_VISIBLE_DEVICES'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b00e926d2b14766c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:02:36.492122Z",
     "start_time": "2024-08-31T20:02:36.489627Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install --force-reinstall \"xformers<0.0.27\"\n",
    "# ! pip install matplotlib\n",
    "# ! pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79b4c7a8138bf95b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:02:36.688771Z",
     "start_time": "2024-08-31T20:02:36.686101Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('space-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b724f487f25ef1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:02:55.716784Z",
     "start_time": "2024-08-31T20:02:36.880815Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stepan/.conda/envs/llm-py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "\n",
    "# Install Flash Attention 2 for softcapping support\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# if torch.cuda.get_device_capability()[0] >= 8:\n",
    "#     !pip install --no-deps packaging ninja einops \"flash-attn>=2.6.3\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from logger import get_logger\n",
    "\n",
    "from space_model.model import SpaceModel\n",
    "import space_model.loss as losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e38733b979eaea35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:02:55.720610Z",
     "start_time": "2024-08-31T20:02:55.718228Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install --no-deps --upgrade \"flash-attn>=2.6.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdbac104dec81c19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:02:55.750605Z",
     "start_time": "2024-08-31T20:02:55.721492Z"
    }
   },
   "outputs": [],
   "source": [
    "EVAL_BATCH_SIZE = 1\n",
    "MAX_SEQ_LENGTH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9465c79334251d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:02:55.755716Z",
     "start_time": "2024-08-31T20:02:55.751995Z"
    }
   },
   "outputs": [],
   "source": [
    "log = get_logger(f'logs/gemma-2-2b', 'space-imdb-ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80575e3e20bdcc9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:02:55.761338Z",
     "start_time": "2024-08-31T20:02:55.756670Z"
    }
   },
   "outputs": [],
   "source": [
    "device_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2016af626136b350",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:02:55.768768Z",
     "start_time": "2024-08-31T20:02:55.762212Z"
    }
   },
   "outputs": [],
   "source": [
    "class SpaceModelForSequenceClassification(torch.nn.Module):\n",
    "    def __init__(self, base_model, n_embed=3, n_latent=3, n_concept_spaces=2, l1=1e-3, l2=1e-4, ce_w=1.0,\n",
    "                 fine_tune=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if fine_tune:\n",
    "            for p in base_model.parameters():\n",
    "                p.requires_grad_(False)\n",
    "\n",
    "        self.device = base_model.device\n",
    "\n",
    "        self.base_model = base_model\n",
    "\n",
    "        self.space_model = SpaceModel(n_embed, n_latent, n_concept_spaces, output_concept_spaces=True)\n",
    "\n",
    "        self.classifier = torch.nn.Linear(n_concept_spaces * n_latent, n_concept_spaces)\n",
    "\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.ce_w = ce_w\n",
    "\n",
    "    def to(self, device):\n",
    "        self.device = device\n",
    "        super().to(device)\n",
    "        return self\n",
    "\n",
    "    def to_inference(self):\n",
    "        FastLanguageModel.for_inference(self.base_model)\n",
    "\n",
    "    def to_training(self):\n",
    "        FastLanguageModel.for_training(self.base_model)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        embed = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=1,\n",
    "            output_hidden_states=True\n",
    "        ).hidden_states[-1].float()  # (B, max_seq_len, 2304)\n",
    "\n",
    "        out = self.space_model(embed)  # SpaceModelOutput(logits=(B, n_concept_spaces * n_latent), ...)\n",
    "\n",
    "        concept_hidden = out.logits\n",
    "\n",
    "        logits = self.classifier(concept_hidden)\n",
    "\n",
    "        loss = 0.0\n",
    "        if labels is not None:\n",
    "            loss = self.ce_w * F.cross_entropy(logits, labels)\n",
    "            loss += self.l1 * losses.inter_space_loss(out.concept_spaces, labels) + self.l2 * losses.intra_space_loss(\n",
    "                out.concept_spaces)\n",
    "\n",
    "        return {\"logits\": logits, \"loss\": loss}\n",
    "\n",
    "    def from_pretrained(self, path):\n",
    "        self.space_model.load_state_dict(torch.load(f\"{path}/space_model.pth\"))\n",
    "        self.classifier.load_state_dict(torch.load(f\"{path}/classifier.pth\"))\n",
    "        return self\n",
    "\n",
    "    def save_pretrained(self, path):\n",
    "        self.base_model.save_pretrained(f\"{path}/base\")\n",
    "        torch.save(self.space_model.state_dict(), f\"{path}/space_model.pth\")\n",
    "        torch.save(self.classifier.state_dict(), f\"{path}/classifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8d5e2d7b55cc84a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:02:55.773518Z",
     "start_time": "2024-08-31T20:02:55.769636Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_tokenizer(max_seq_length=1024, dtype=None, load_in_4bit=True, add_lora=False, load_from=None):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"unsloth/gemma-2-2b\",\n",
    "        # model_name=\"models/space-gemma-2-2b/base\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        device_map={'': 0},\n",
    "        # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    )\n",
    "\n",
    "    if add_lora:\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r=8,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                            \"gate_proj\", \"up_proj\", \"down_proj\", ],\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "            bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
    "            # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "            use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "            random_state=3407,\n",
    "            use_rslora=False,  # We support rank stabilized LoRA\n",
    "            loftq_config=None,  # And LoftQ\n",
    "        )\n",
    "\n",
    "    tokenizer.truncation_side = 'left'\n",
    "    tokenizer.padding_side = 'left'\n",
    "\n",
    "    space_model = SpaceModelForSequenceClassification(\n",
    "        model,\n",
    "        n_embed=2304,\n",
    "        n_latent=256,\n",
    "        n_concept_spaces=2,\n",
    "        l1=1e-3,\n",
    "        l2=1e-7,\n",
    "        ce_w=1.0,\n",
    "        fine_tune=False\n",
    "    )\n",
    "\n",
    "    if load_from:\n",
    "        space_model.from_pretrained(load_from)\n",
    "\n",
    "    space_model.to(f\"cuda:{device_id}\")\n",
    "\n",
    "    return space_model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a9c2ee4404c9602",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:02:55.777492Z",
     "start_time": "2024-08-31T20:02:55.774390Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval(f):\n",
    "    def wrapper(model, *args, **kwargs):\n",
    "        model.to_inference()\n",
    "        with torch.no_grad():\n",
    "            return f(model, *args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def train(f):\n",
    "    def wrapper(model, *args, **kwargs):\n",
    "        model.to_training()\n",
    "        return f(model, *args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "827043690be05487",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:03:00.131494Z",
     "start_time": "2024-08-31T20:02:55.778437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: If you want to finetune Gemma 2, install flash-attn to make it faster!\n",
      "To install flash-attn, do the below:\n",
      "\n",
      "pip install --no-deps --upgrade \"flash-attn>=2.6.3\"\n",
      "==((====))==  Unsloth 2024.8: Fast Gemma2 patching. Transformers = 4.43.4.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.679 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_model_tokenizer(\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    add_lora=False,\n",
    "    # load_from=\"models/space-gemma-2-2b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73625297a80917ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:03:00.139248Z",
     "start_time": "2024-08-31T20:03:00.133765Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1180674"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "204e2191ca8c8110",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:03:00.168967Z",
     "start_time": "2024-08-31T20:03:00.140164Z"
    }
   },
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "HATE_TOKEN = 'Hate'\n",
    "NORMAL_TOKEN = 'Normal'\n",
    "\n",
    "FAKE_TOKEN = 'Fake'\n",
    "TRUTH_TOKEN = 'Truth'\n",
    "\n",
    "POS_TOKEN = 'Positive'\n",
    "NEG_TOKEN = 'Negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed12273d6642f5ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:03:00.178666Z",
     "start_time": "2024-08-31T20:03:00.169857Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_imdb(tokenizer, device, seed):\n",
    "    # Load the IMDb dataset\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "\n",
    "    # Split the training set into training (80%) and validation (20%) sets\n",
    "    train_testvalid = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "    # Assign datasets\n",
    "    train_dataset = train_testvalid['train']\n",
    "    val_dataset = train_testvalid['test']\n",
    "    test_dataset = dataset['test']\n",
    "\n",
    "    dataset = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'test': test_dataset,\n",
    "        'val': val_dataset\n",
    "    })\n",
    "\n",
    "    PROMPT = '''### Text:\n",
    "    {}\n",
    "    \n",
    "    ### Classification:\n",
    "    {}'''\n",
    "\n",
    "    def formatting_prompts_func(examples):\n",
    "        inputs = examples[\"text\"]\n",
    "        outputs = [\n",
    "            NEG_TOKEN if label == 0 else POS_TOKEN for label in examples[\"label\"]\n",
    "        ]\n",
    "        texts = []\n",
    "        prompts = []\n",
    "        for input, output in zip(inputs, outputs):\n",
    "            text = PROMPT.format(input, output) + EOS_TOKEN\n",
    "            texts.append(text)\n",
    "            prompts.append(PROMPT.format(input, \"\"))\n",
    "        return {\"ref\": texts, 'prompt': prompts}\n",
    "\n",
    "    dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def prepare_hateoffensive(tokenizer, device, seed):\n",
    "    # 3 classes\n",
    "    dataset = load_dataset(\"tdavidson/hate_speech_offensive\")\n",
    "\n",
    "    PROMPT = '''### Tweet:\n",
    "    {}\n",
    "    \n",
    "    ### Classification:\n",
    "    {}'''\n",
    "\n",
    "    def formatting_prompts_func(examples):\n",
    "        inputs = examples[\"tweet\"]\n",
    "        outputs = [\n",
    "            HATE_TOKEN if label in [1, 0] else NORMAL_TOKEN for label in examples[\"class\"]\n",
    "        ]\n",
    "        texts = []\n",
    "        prompts = []\n",
    "        for input, output in zip(inputs, outputs):\n",
    "            text = PROMPT.format(input.strip(), output) + EOS_TOKEN\n",
    "            texts.append(text)\n",
    "            prompts.append(PROMPT.format(input, \"\"))\n",
    "        return {\"ref\": texts, 'prompt': prompts}\n",
    "\n",
    "    dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "    # Split the training set into training (80%) and validation (20%) sets\n",
    "    train_testvalid = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "    test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "    # Assign datasets\n",
    "    train_dataset = train_testvalid['train']\n",
    "    val_dataset = test_valid['test']\n",
    "    test_dataset = test_valid['train']\n",
    "\n",
    "    dataset = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'test': test_dataset,\n",
    "        'val': val_dataset\n",
    "    })\n",
    "    return dataset.rename_columns({'class': 'label'})\n",
    "\n",
    "\n",
    "def prepare_fake(tokenizer, device, seed):\n",
    "    train_df = pd.read_csv('data/fake_train.csv', index_col=0)\n",
    "    train_df['final'] = 'Title: ' + train_df['title'] + ' Text: ' + train_df['text']\n",
    "\n",
    "    train_df = train_df[train_df['title'].notnull() & train_df['text'].notnull()]\n",
    "\n",
    "    test_df = pd.read_csv('data/fake_test.csv', index_col=0)\n",
    "    test_df['final'] = 'Title: ' + test_df['title'] + ' Text: ' + test_df['text']\n",
    "\n",
    "    train_split, test_split = train_test_split(train_df, test_size=0.2, random_state=seed)\n",
    "    test_split, val_split = train_test_split(test_split, test_size=0.5, random_state=seed)\n",
    "\n",
    "    dataset = DatasetDict({\n",
    "        'train': Dataset.from_pandas(train_split[['final', 'label']]),\n",
    "        'test': Dataset.from_pandas(test_split[['final', 'label']]),\n",
    "        'val': Dataset.from_pandas(val_split[['final', 'label']])\n",
    "    })\n",
    "\n",
    "    PROMPT = '''### Text:\n",
    "    {}\n",
    "    \n",
    "    ### Classification:\n",
    "    {}'''\n",
    "\n",
    "    def formatting_prompts_func(examples):\n",
    "        inputs = examples[\"final\"]\n",
    "        outputs = [\n",
    "            FAKE_TOKEN if label == 0 else TRUTH_TOKEN for label in examples[\"label\"]\n",
    "        ]\n",
    "        texts = []\n",
    "        prompts = []\n",
    "        for input, output in zip(inputs, outputs):\n",
    "            text = PROMPT.format(input, output) + EOS_TOKEN\n",
    "            texts.append(text)\n",
    "            prompts.append(PROMPT.format(input, \"\"))\n",
    "        return {\"ref\": texts, 'prompt': prompts}\n",
    "\n",
    "    dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43b165ef37158ac5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:03:07.127218Z",
     "start_time": "2024-08-31T20:03:00.179531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'ref', 'prompt'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'ref', 'prompt'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'label', 'ref', 'prompt'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = prepare_imdb(tokenizer, f\"cuda:{device_id}\", seed=3407)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bfb068cfbbccc15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:03:07.174568Z",
     "start_time": "2024-08-31T20:03:07.128657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Text:\\n    I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clich√©d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.\\n    \\n    ### Classification:\\n    Negative<eos>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test']['ref'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3184e6dc5dcca38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:03:07.209778Z",
     "start_time": "2024-08-31T20:03:07.175708Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
       "          (rotary_emb): GemmaFixedRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm()\n",
       "        (post_attention_layernorm): Gemma2RMSNorm()\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm()\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b750e3fc0e966d2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:03:07.216116Z",
     "start_time": "2024-08-31T20:03:07.211022Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.inference() # Enable native 2x faster inference\n",
    "# inputs = tokenizer(\n",
    "#     [\n",
    "#         dataset['test']['prompt'][12],\n",
    "#     ], return_tensors=\"pt\").to(f\"cuda:{device_id}\")\n",
    "# \n",
    "# outputs = model(**inputs)\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b54eca12e6d19d4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:03:07.221733Z",
     "start_time": "2024-08-31T20:03:07.217320Z"
    }
   },
   "outputs": [],
   "source": [
    "# outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "933a0aa405f661fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:03:07.225763Z",
     "start_time": "2024-08-31T20:03:07.223018Z"
    }
   },
   "outputs": [],
   "source": [
    "TOKENS = {\n",
    "    'Positive': ...,\n",
    "    'Negative': ...,\n",
    "\n",
    "    'Hate': ...,\n",
    "    'Normal': ...,\n",
    "    'Fake': ...,\n",
    "    'Truth': ...\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d84dfeec8947bdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:03:07.230954Z",
     "start_time": "2024-08-31T20:03:07.227018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: [35202]\n",
      "Negative: [39654]\n",
      "Hate: [88060]\n",
      "Normal: [15273]\n",
      "Fake: [41181]\n",
      "Truth: [55882]\n"
     ]
    }
   ],
   "source": [
    "for key, val in TOKENS.items():\n",
    "    code = tokenizer.encode(key, add_special_tokens=False)\n",
    "    print(f\"{key}: {code}\")\n",
    "    TOKENS[key] = code[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ade6117741b37bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:03:07.235686Z",
     "start_time": "2024-08-31T20:03:07.232198Z"
    }
   },
   "outputs": [],
   "source": [
    "def gpu_stats(device_id=0):\n",
    "    #@title Show current memory stats\n",
    "    gpu_stats = torch.cuda.get_device_properties(device_id)\n",
    "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "    return {'gpu': gpu_stats.name, 'max_memory': max_memory, 'start_gpu_memory': start_gpu_memory}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2deff542f908e222",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:03:07.243286Z",
     "start_time": "2024-08-31T20:03:07.236953Z"
    }
   },
   "outputs": [],
   "source": [
    "@eval\n",
    "def evaluate(model, tokenizer, dataset, batch_size, threshold=0.5):\n",
    "    eval_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for i, batch in tqdm(enumerate(eval_dataloader), total=len(eval_dataloader)):\n",
    "        texts = batch[\"prompt\"]\n",
    "\n",
    "        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SEQ_LENGTH).to(\n",
    "            model.device)\n",
    "\n",
    "        outputs = model(**inputs)  # (B, 2)\n",
    "\n",
    "        preds.extend(torch.argmax(outputs['logits'], dim=-1).cpu())\n",
    "        labels.extend(batch[\"label\"])\n",
    "\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            log.warn(f'GPU Stats: {gpu_stats(device_id)}')\n",
    "\n",
    "    val_acc = accuracy_score(labels, preds)\n",
    "    val_f1 = f1_score(labels, preds, average='macro')\n",
    "    val_precision = precision_score(labels, preds, average='macro')\n",
    "    val_recall = recall_score(labels, preds, average='macro')\n",
    "\n",
    "    log.info(f\"Accuracy: {val_acc}, F1: {val_f1}, Precision: {val_precision}, Recall: {val_recall}\")\n",
    "    return val_acc, val_f1, val_precision, val_recall, preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af06f073951d6fd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:03:07.246834Z",
     "start_time": "2024-08-31T20:03:07.244562Z"
    }
   },
   "outputs": [],
   "source": [
    "# eval_acc, eval_f1, eval_precision, eval_recall, preds, labels = evaluate(model, tokenizer, dataset[\"test\"],\n",
    "# batch_size = EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a107fc13fb830c15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:03:07.251806Z",
     "start_time": "2024-08-31T20:03:07.248102Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "    precision = precision_score(labels, preds, average='macro')\n",
    "    recall = recall_score(labels, preds, average='macro')\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bee9c46da8f039f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:03:07.257753Z",
     "start_time": "2024-08-31T20:03:07.253049Z"
    }
   },
   "outputs": [],
   "source": [
    "@train\n",
    "def training(model, tokenizer, dataset):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['val'],\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=4,\n",
    "            gradient_accumulation_steps=4,\n",
    "            eval_steps=500,\n",
    "            warmup_steps=500,\n",
    "            num_train_epochs=2,  # Set this for 1 full training run.\n",
    "            # max_steps=60,\n",
    "            evaluation_strategy='steps',\n",
    "            learning_rate=2e-5,\n",
    "            fp16=not is_bfloat16_supported(),\n",
    "            bf16=is_bfloat16_supported(),\n",
    "            logging_steps=250,\n",
    "            optim=\"adamw_8bit\",\n",
    "            weight_decay=0.01,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            seed=3407,\n",
    "            output_dir=\"models/gemma-2-2b\",\n",
    "            save_strategy='no',\n",
    "        ),\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    stats = trainer.train()\n",
    "    return trainer, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13b0314de25005fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:03:08.008795Z",
     "start_time": "2024-08-31T20:03:07.258973Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"prompt\"], padding=\"max_length\", truncation=True, max_length=MAX_SEQ_LENGTH)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d5a022397f2aa31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 1:34:03, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.653300</td>\n",
       "      <td>0.640074</td>\n",
       "      <td>0.657400</td>\n",
       "      <td>0.638718</td>\n",
       "      <td>0.699690</td>\n",
       "      <td>0.657950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.599000</td>\n",
       "      <td>0.582474</td>\n",
       "      <td>0.737000</td>\n",
       "      <td>0.736720</td>\n",
       "      <td>0.737863</td>\n",
       "      <td>0.736926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.556200</td>\n",
       "      <td>0.557908</td>\n",
       "      <td>0.752200</td>\n",
       "      <td>0.751661</td>\n",
       "      <td>0.754187</td>\n",
       "      <td>0.752092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.538200</td>\n",
       "      <td>0.548993</td>\n",
       "      <td>0.754600</td>\n",
       "      <td>0.752898</td>\n",
       "      <td>0.762239</td>\n",
       "      <td>0.754804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.528900</td>\n",
       "      <td>0.539992</td>\n",
       "      <td>0.760800</td>\n",
       "      <td>0.760796</td>\n",
       "      <td>0.760841</td>\n",
       "      <td>0.760814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=0.58227255859375, metrics={'train_runtime': 5650.5626, 'train_samples_per_second': 7.079, 'train_steps_per_second': 0.442, 'total_flos': 0.0, 'train_loss': 0.58227255859375, 'epoch': 2.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer, stats = training(model, tokenizer, tokenized_datasets)\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47fe0562771eeacc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T22:03:03.192286Z",
     "start_time": "2024-08-31T21:37:19.163190Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 998/25000 [01:05<26:48, 14.92it/s]  GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      "  8%|‚ñä         | 1998/25000 [02:06<25:09, 15.24it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      " 12%|‚ñà‚ñè        | 2998/25000 [03:07<22:15, 16.48it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      " 16%|‚ñà‚ñå        | 3998/25000 [04:08<21:05, 16.60it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      " 20%|‚ñà‚ñâ        | 4998/25000 [05:09<18:50, 17.69it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      " 24%|‚ñà‚ñà‚ñç       | 5998/25000 [06:11<19:01, 16.65it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      " 28%|‚ñà‚ñà‚ñä       | 6998/25000 [07:12<18:18, 16.39it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 7998/25000 [08:13<15:48, 17.92it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      " 36%|‚ñà‚ñà‚ñà‚ñå      | 8998/25000 [09:16<19:08, 13.93it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      " 40%|‚ñà‚ñà‚ñà‚ñâ      | 9998/25000 [10:17<14:30, 17.23it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      " 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 10998/25000 [11:18<15:28, 15.08it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 11998/25000 [12:20<12:27, 17.41it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 12998/25000 [13:21<12:52, 15.54it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 13998/25000 [14:22<12:05, 15.18it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 14998/25000 [15:24<10:56, 15.24it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 15998/25000 [16:26<09:14, 16.24it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 16998/25000 [17:27<08:24, 15.87it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 17998/25000 [18:29<07:26, 15.69it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      " 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 18998/25000 [19:31<05:51, 17.06it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 19998/25000 [20:31<05:00, 16.65it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 20998/25000 [21:32<04:42, 14.15it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 21998/25000 [22:34<03:03, 16.39it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 22998/25000 [23:36<01:55, 17.33it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 23998/25000 [24:37<01:00, 16.62it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 24998/25000 [25:39<00:00, 15.31it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.679, 'start_gpu_memory': 8.662}\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [25:39<00:00, 16.24it/s]\n",
      "Accuracy: 0.82132, F1: 0.8209790636493755, Precision: 0.8237865405908545, Recall: 0.82132\n"
     ]
    }
   ],
   "source": [
    "eval_acc, eval_f1, eval_precision, eval_recall, preds, labels = evaluate(model, tokenizer, dataset[\"test\"], batch_size=EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e193c1d54c0699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
