{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 19 18:15:48 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A5000               On  | 00000000:2B:00.0 Off |                  Off |\n",
      "| 30%   33C    P8              20W / 230W |    244MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A5000               On  | 00000000:41:00.0 Off |                  Off |\n",
      "| 30%   37C    P8              21W / 230W |     20MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1810      G   /usr/lib/xorg/Xorg                           79MiB |\n",
      "|    0   N/A  N/A      1988      G   /usr/bin/gnome-shell                          9MiB |\n",
      "|    0   N/A  N/A      2347      G   /usr/lib/xorg/Xorg                           22MiB |\n",
      "|    1   N/A  N/A      1810      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A      2347      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/home/stepan/kaggle-arc-agi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env CUDA_VISIBLE_DEVICES=0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getenv('CUDA_VISIBLE_DEVICES'))\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(BASE_PATH)\n",
    "sys.path.append(f'{BASE_PATH}/scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "from datasets import DatasetDict, Dataset # type: ignore\n",
    "\n",
    "from tqdm.auto import tqdm # type: ignore\n",
    "\n",
    "import transformers # type: ignore\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig # type: ignore\n",
    "\n",
    "import torch # type: ignore\n",
    "\n",
    "from logger import get_logger # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(f'{BASE_PATH}/logs/gemma-2-2b', 'arc-agi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = f\"{BASE_PATH}/models/gemma-2-2b-it/checkpoint-500\"\n",
    "# MODEL_ID = \"unsloth/gemma-2-2b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_tokenizer(max_seq_length, load_in_4bit=True):\n",
    "    quantization_config = BitsAndBytesConfig(load_in_4bit=load_in_4bit)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side='left')\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID, \n",
    "        # quantization_config=quantization_config,\n",
    "        # attn_implementation='flash_attention_2',\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"cuda:0\",\n",
    "    )\n",
    "    \n",
    "    # model.generation_config.cache_implementation = \"static\"\n",
    "\n",
    "    # model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "# def get_model_tokenizer(max_seq_length):\n",
    "#     model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#         # model_name=\"unsloth/gemma-2-2b-it\",\n",
    "#         model_name=f\"{BASE_PATH}/models/gemma-2-2b-it/checkpoint-500\",\n",
    "#         max_seq_length=max_seq_length,\n",
    "#         dtype=torch.bfloat16,\n",
    "#         load_in_4bit=True,\n",
    "#         device_map={'': 0},\n",
    "#         # attn_implementation='flash_attention_2',\n",
    "#         # token = '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', # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "#     )\n",
    "#     return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6144"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_NEW_TOKENS = 2048\n",
    "MAX_SEQ_LENGTH = 8192 - MAX_NEW_TOKENS\n",
    "MAX_SEQ_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2SdpaAttention(\n",
       "          (q_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=2304, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=9216, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (up_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=9216, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (down_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=9216, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=2304, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm()\n",
       "        (post_attention_layernorm): Gemma2RMSNorm()\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm()\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tokenizer = get_model_tokenizer(MAX_SEQ_LENGTH)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from JSON files\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def split_train_examples(train_examples, max_size=4096-32):\n",
    "    total_size = sum(len(example['input']) * len(example['input'][0]) + len(example['output']) * len(example['output'][0]) for example in train_examples)\n",
    "    if total_size <= max_size:\n",
    "        return [train_examples]\n",
    "    \n",
    "    split_size = max(1, max_size // total_size)\n",
    "    return [train_examples[i:i+split_size] for i in range(0, len(train_examples), split_size)]\n",
    "\n",
    "def to_dataset(data, solutions=None, fit_dataset=False):\n",
    "    restructured_data = {\n",
    "        'id': [],\n",
    "        'challenge': [],\n",
    "    }\n",
    "    if solutions is not None:\n",
    "        restructured_data['solution'] = []\n",
    "\n",
    "    for challenge_id, challenge_data in data.items(): # for all challenges\n",
    "        for test_id, task in enumerate(challenge_data['test']): # for all test tasks in this challenge we want to expand dataset so that each test task is separate dataset record\n",
    "            if fit_dataset:\n",
    "                for split_id, split_train in enumerate(split_train_examples(challenge_data['train'])): # if fit_dataset is true, we split each training example into multiple records so that each record has less than MAX_SEQ_LENGTH tokens\n",
    "                    restructured_data['id'].append(challenge_id)\n",
    "                    restructured_data['challenge'].append({'train': split_train, 'test': task, 'order': test_id})\n",
    "                    if solutions is not None:\n",
    "                        restructured_data['solution'].append(solutions[challenge_id][test_id])\n",
    "            else:\n",
    "                restructured_data['id'].append(challenge_id)\n",
    "                restructured_data['challenge'].append({'train': challenge_data['train'], 'test': task, 'order': test_id})\n",
    "                if solutions is not None:\n",
    "                    restructured_data['solution'].append(solutions[challenge_id][test_id])\n",
    "\n",
    "    return Dataset.from_dict(restructured_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs(dct):\n",
    "    input_str = '\\n'.join(''.join(map(str, row)) for row in dct[\"input\"])\n",
    "    output_str = '\\n'.join(''.join(map(str, row)) for row in dct[\"output\"]) if \"output\" in dct else \"\"\n",
    "    text = f'<input>\\n{input_str}\\n</input>\\n\\n<output>\\n{output_str}\\n</output>'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(tokenizer, use_system_prompt=False, fit_dataset=False):\n",
    "    # The system_prompt defines the initial instructions for the model, setting the context for solving ARC tasks.\n",
    "    system_prompt = '''You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.'''\n",
    "\n",
    "    # User message template is a template for creating user prompts. It includes placeholders for training data and test input data, guiding the model to learn the rule and apply it to solve the given puzzle.\n",
    "    user_message_template = '''Here are the example input and output pairs from which you should learn the underlying rule to later predict the output for the given test input:\n",
    "-----------------\n",
    "{training_data}\n",
    "-----------------\n",
    "Now, solve the following puzzle based on its input grid by applying the rules you have learned from the training data.:\n",
    "-----------------\n",
    "{input_test_data}\n",
    "-----------------\n",
    "What is the output grid? Only provide the output grid in the form as in the example input and output pairs. Do not provide any additional information:'''\n",
    "    \n",
    "    # Load all datasets\n",
    "    training_challenges = load_data(f'{BASE_PATH}/arc-prize-2024/arc-agi_training_challenges.json')\n",
    "    training_solutions = load_data(f'{BASE_PATH}/arc-prize-2024/arc-agi_training_solutions.json')\n",
    "    evaluation_challenges = load_data(f'{BASE_PATH}/arc-prize-2024/arc-agi_evaluation_challenges.json')\n",
    "    evaluation_solutions = load_data(f'{BASE_PATH}/arc-prize-2024/arc-agi_evaluation_solutions.json')\n",
    "    test_challenges = load_data(f'{BASE_PATH}/arc-prize-2024/arc-agi_test_challenges.json')\n",
    "    \n",
    "    train_dataset = to_dataset(training_challenges, training_solutions, fit_dataset=fit_dataset)\n",
    "    eval_dataset = to_dataset(evaluation_challenges, evaluation_solutions, fit_dataset=fit_dataset)\n",
    "    pred_dataset = to_dataset(test_challenges, fit_dataset=fit_dataset)\n",
    "\n",
    "    def create_chat(challenge, solution=None):\n",
    "        user_content = user_message_template.format(\n",
    "            training_data='\\n\\n'.join([prepare_inputs(ex) for ex in challenge['train']]),\n",
    "            input_test_data=prepare_inputs(challenge['test'])\n",
    "        )\n",
    "        \n",
    "        if use_system_prompt:\n",
    "            messages = [\n",
    "                {'role': 'system', 'content': system_prompt},\n",
    "                {'role': 'user', 'content': user_content}\n",
    "            ]\n",
    "        else:\n",
    "            messages = [{'role': 'user', 'content': f\"{system_prompt}\\n\\n{user_content}\"}]\n",
    "        \n",
    "        if solution:\n",
    "            messages.append({'role': 'assistant', 'content': \"<output>\\n\" + '\\n'.join(''.join(map(str, row)) for row in solution) + \"\\n</output>\"})\n",
    "        \n",
    "        return messages\n",
    "\n",
    "    def process_dataset(examples, solutions=None):\n",
    "        # Create messages for each challenge-solution pair\n",
    "        chats = []\n",
    "        for challenge, solution in zip(examples['challenge'], solutions or [None] * len(examples['challenge'])):\n",
    "            chat = create_chat(challenge, solution)\n",
    "            chats.append(chat)\n",
    "        \n",
    "        # Apply chat template to each message\n",
    "        texts = [tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False) for chat in chats]\n",
    "        \n",
    "        return {\n",
    "            'texts': texts,\n",
    "            'messages': chats\n",
    "        }\n",
    "\n",
    "    train_dataset = train_dataset.map(lambda x: process_dataset(x, train_dataset['solution']), batched=True)\n",
    "    pred_dataset = pred_dataset.map(lambda x: process_dataset(x), batched=True)\n",
    "\n",
    "    eval_dataset = eval_dataset.map(lambda x: process_dataset(x, eval_dataset['solution']), batched=True)\n",
    "    test_dataset = eval_dataset.train_test_split(test_size=0.3)\n",
    "\n",
    "    dataset = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'test': test_dataset['train'],\n",
    "        'val': test_dataset['test'],\n",
    "        'predict': pred_dataset\n",
    "    })\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6bc56533b7742918bf8efc215cfc849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/430 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a600f824ce544081a7cc6605c00734dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c818a59965a04c50acf9eafb11dfaf76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/459 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'challenge', 'solution', 'texts', 'messages'],\n",
       "        num_rows: 430\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'challenge', 'solution', 'texts', 'messages'],\n",
       "        num_rows: 321\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['id', 'challenge', 'solution', 'texts', 'messages'],\n",
       "        num_rows: 138\n",
       "    })\n",
       "    predict: Dataset({\n",
       "        features: ['id', 'challenge', 'texts', 'messages'],\n",
       "        num_rows: 112\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = prepare_dataset(tokenizer, fit_dataset=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_stats(device_id=0):\n",
    "    #@title Show current memory stats\n",
    "    gpu_stats = torch.cuda.get_device_properties(device_id)\n",
    "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "    return {'gpu': gpu_stats.name, 'max_memory': max_memory, 'start_gpu_memory': start_gpu_memory}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_output(text):\n",
    "    # Extract the content inside <output></output> tags\n",
    "    output_match = re.search(r'<output>(.*?)</output>', text, re.DOTALL)\n",
    "    if not output_match:\n",
    "        return None\n",
    "    \n",
    "    output_content = output_match.group(1).strip()\n",
    "    \n",
    "    # Split the content into lines and convert each line to a list of single-digit integers\n",
    "    try:\n",
    "        grid = []\n",
    "        for line in output_content.split('\\n'):\n",
    "            row = [int(char) for char in line.strip() if char.isdigit()]\n",
    "            if row:\n",
    "                grid.append(row)\n",
    "        \n",
    "        # Ensure all rows have the same length\n",
    "        if grid and all(len(row) == len(grid[0]) for row in grid):\n",
    "            return grid\n",
    "        else:\n",
    "            return None\n",
    "    except ValueError:\n",
    "        return None\n",
    "    \n",
    "def tensor_to_int(value):\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        return tensor_to_int(value.item())\n",
    "    elif isinstance(value, list):\n",
    "        return [tensor_to_int(item) for item in value]\n",
    "    else:\n",
    "        return value\n",
    "    \n",
    "def calculate_partial_match(pred, label):\n",
    "    if not isinstance(pred, list) or not isinstance(label, list):\n",
    "        return 0  # No match if either is not a list\n",
    "\n",
    "    if len(pred) != len(label):\n",
    "        return 0  # No match if outer dimensions differ\n",
    "\n",
    "    total_elements = 0\n",
    "    correct_elements = 0\n",
    "\n",
    "    for p_row, l_row in zip(pred, label):\n",
    "        if not isinstance(p_row, list) or not isinstance(l_row, list) or len(p_row) != len(l_row):\n",
    "            return 0  # No match if any row is not a list or dimensions differ\n",
    "\n",
    "        total_elements += len(l_row)\n",
    "        correct_elements += sum(p == l for p, l in zip(p_row, l_row))\n",
    "\n",
    "    return correct_elements / total_elements if total_elements > 0 else 0\n",
    "\n",
    "def calculate_metrics(preds, labels):\n",
    "    total_samples = len(labels)\n",
    "    \n",
    "    correct = sum(1 for p, l in zip(preds, labels) if p == l)\n",
    "    accuracy = correct / total_samples\n",
    "    \n",
    "    partial_match_scores = [calculate_partial_match(p, l) if p is not None else 0 for p, l in zip(preds, labels)]\n",
    "    \n",
    "    avg_partial_match = sum(partial_match_scores) / total_samples\n",
    "    \n",
    "    return accuracy, avg_partial_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(mode, tokenizer):\n",
    "    def collate_fn(batch):\n",
    "        # Separate the different components of the batch\n",
    "        ids = [item['id'] for item in batch]\n",
    "        challenges = [item['challenge'] for item in batch]\n",
    "        \n",
    "        # For 'test' mode, remove the last assistant message from each entry\n",
    "        if mode == 'test':\n",
    "            messages = [item['messages'][:-1] for item in batch] # last message is always assistant message - solution, we don't need it for evaluation\n",
    "        else:\n",
    "            messages = [item['messages'] for item in batch]\n",
    "        \n",
    "        # Tokenize the texts\n",
    "        encodings = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_dict=True,\n",
    "            padding=True, \n",
    "            # truncation=True\n",
    "        )\n",
    "        \n",
    "        # If 'solution' is present (for training/validation data)\n",
    "        if 'solution' in batch[0]:\n",
    "            solutions = [item['solution'] for item in batch]\n",
    "            return {\n",
    "                'id': ids,\n",
    "                'challenge': challenges,\n",
    "                'solution': solutions,\n",
    "                'input_ids': encodings['input_ids'].to(\"cuda\"),\n",
    "                'attention_mask': encodings['attention_mask'].to(\"cuda\")\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'id': ids,\n",
    "                'challenge': challenges,\n",
    "                'input_ids': encodings['input_ids'].to(\"cuda\"),\n",
    "                'attention_mask': encodings['attention_mask'].to(\"cuda\")\n",
    "            }\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_batch(model, tokenizer, batch, num_seq=5):\n",
    "    inputs = {\n",
    "        'input_ids': batch['input_ids'],\n",
    "        'attention_mask': batch['attention_mask']\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=MAX_NEW_TOKENS, \n",
    "            do_sample=True, \n",
    "            use_cache=True, \n",
    "            num_beams=5, \n",
    "            num_return_sequences=num_seq, \n",
    "            temperature=0.5, \n",
    "            top_k=50\n",
    "        )\n",
    "\n",
    "    input_ids_length = inputs['input_ids'].shape[1] # sequence length without new tokens\n",
    "    new_tokens = outputs[:, input_ids_length:]\n",
    "    \n",
    "    generated_texts = tokenizer.batch_decode(new_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sequences(generated_texts, num_seq):\n",
    "    parsed_outputs = [parse_output(text) for text in generated_texts]\n",
    "    res = []\n",
    "    for i in range(0, len(parsed_outputs), num_seq):\n",
    "        options = [opt for opt in parsed_outputs[i:i + num_seq] if opt is not None]\n",
    "        if not options:\n",
    "            res.append((None, None))\n",
    "            continue\n",
    "        \n",
    "        # Group options by their structure (rows x columns)\n",
    "        structure_groups = {}\n",
    "        for option in options:\n",
    "            rows = len(option)\n",
    "            cols = len(option[0]) if rows > 0 else 0\n",
    "            structure = (rows, cols)\n",
    "            if structure not in structure_groups:\n",
    "                structure_groups[structure] = []\n",
    "            structure_groups[structure].append(option)\n",
    "        \n",
    "        # Select the group with the most options\n",
    "        most_common_structure = max(structure_groups, key=lambda x: len(structure_groups[x]))\n",
    "        selected_options = structure_groups[most_common_structure]\n",
    "        \n",
    "        # Get dimensions of the most common structure\n",
    "        rows, cols = most_common_structure\n",
    "        \n",
    "        # Perform element-wise voting\n",
    "        voted_option = [[None for _ in range(cols)] for _ in range(rows)]\n",
    "        for row in range(rows):\n",
    "            for col in range(cols):\n",
    "                elements = [option[row][col] for option in selected_options]\n",
    "                voted_option[row][col] = max(set(elements), key=elements.count)\n",
    "        \n",
    "        # Select the top 2 options based on similarity to the voted option\n",
    "        def similarity_score(option):\n",
    "            return sum(option[r][c] == voted_option[r][c] for r in range(rows) for c in range(cols))\n",
    "        \n",
    "        top_2_options = sorted(selected_options, key=similarity_score, reverse=True)[:2]\n",
    "        res.append(tuple(top_2_options)) # TODO this or top2 + voted\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, dataset, batch_size, num_seq=5):\n",
    "    eval_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate(mode='test', tokenizer=tokenizer))\n",
    "\n",
    "    challenge_ids = []\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for i, batch in tqdm(enumerate(eval_dataloader), total=len(eval_dataloader)):\n",
    "        generated_texts = evaluate_batch(model, tokenizer, batch, num_seq=num_seq) # (batch_size * num_return_sequences, seq_len)\n",
    "\n",
    "        # Ensure solutions is always a list\n",
    "        ids = batch[\"id\"]\n",
    "        challenges = batch[\"challenge\"]\n",
    "        solutions = batch[\"solution\"]\n",
    "        \n",
    "        processed_outputs = process_sequences(generated_texts, num_seq)\n",
    "\n",
    "        # I don't like how complicated this is, but I don't see an easier way to do it right now\n",
    "        for (parsed_output1, parsed_output2), label, challenge_id, challenge in zip(processed_outputs, solutions, ids, challenges):\n",
    "            \n",
    "            if parsed_output1 is None or parsed_output2 is None:\n",
    "                preds.append(None)\n",
    "            else:\n",
    "                # Choose the best prediction based on partial match score\n",
    "                score1 = calculate_partial_match(parsed_output1, tensor_to_int(label)) if parsed_output1 is not None else 0\n",
    "                score2 = calculate_partial_match(parsed_output2, tensor_to_int(label)) if parsed_output2 is not None else 0\n",
    "                best_pred = parsed_output1 if score1 >= score2 else parsed_output2\n",
    "                preds.append(best_pred)\n",
    "            \n",
    "            labels.append(tensor_to_int(label))\n",
    "            challenge_ids.append((challenge_id, challenge['order']))\n",
    "        \n",
    "        if i % 2 == 0 and i > 0:\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        'ids': challenge_ids,\n",
    "        'preds': preds,\n",
    "        'labels': labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = evaluate(model, tokenizer, dataset['test'], batch_size=1)\n",
    "# # Calculate metrics\n",
    "# accuracy, avg_partial_match = calculate_metrics(results['preds'], results['labels'])\n",
    "\n",
    "# log.info(f\"Exact match accuracy: {accuracy:.4f}\")\n",
    "# log.info(f\"Average partial match score: {avg_partial_match:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.apply_chat_template(dataset['test'][0]['messages'], tokenize=True, add_generation_prompt=True, return_tensors='pt', padding=False, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stepan/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/stepan/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **{\n",
    "            'input_ids': inputs['input_ids'].to(\"cuda\"),\n",
    "            'attention_mask': inputs['attention_mask'].to(\"cuda\")\n",
    "        }, \n",
    "        max_new_tokens=MAX_NEW_TOKENS, \n",
    "        do_sample=False, \n",
    "        use_cache=True, \n",
    "        num_beams=5, \n",
    "        num_return_sequences=5,\n",
    "        temperature=0.5, \n",
    "        top_k=50,\n",
    "        top_p=0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.\n",
      "\n",
      "Here are the example input and output pairs from which you should learn the underlying rule to later predict the output for the given test input:\n",
      "-----------------\n",
      "<input>\n",
      "0000000000000000000000\n",
      "0000002000000000000000\n",
      "0020002320000000000000\n",
      "0272000020000000000000\n",
      "0020000000000000000000\n",
      "0000020000111111111110\n",
      "0000242000111111111110\n",
      "0000022000111111113110\n",
      "0000000000111111111110\n",
      "0222000000111111111110\n",
      "0282000000111141111110\n",
      "0202000000111111111110\n",
      "0000000000111111111110\n",
      "0000000000111111111110\n",
      "0000000000118111171110\n",
      "0000000000111111111110\n",
      "0000000000111111111110\n",
      "0000000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "11111111111\n",
      "11111112111\n",
      "11111112321\n",
      "11111111121\n",
      "11112111111\n",
      "11124211111\n",
      "11112211111\n",
      "11111111111\n",
      "12221112111\n",
      "12821127211\n",
      "12121112111\n",
      "11111111111\n",
      "</output>\n",
      "\n",
      "<input>\n",
      "00000000000000000000\n",
      "00000000000000000000\n",
      "00000000000200000000\n",
      "00000000002120000000\n",
      "00000000002000002020\n",
      "00000000000000000320\n",
      "08888880000000002020\n",
      "08888880000000000000\n",
      "08188880000000000000\n",
      "08888880000202000000\n",
      "08888880000042000000\n",
      "08888880000222000000\n",
      "08888880000000000000\n",
      "08884880000000000000\n",
      "08888880000000000000\n",
      "00000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "888888\n",
      "828888\n",
      "212888\n",
      "288888\n",
      "888888\n",
      "888888\n",
      "882828\n",
      "888428\n",
      "882228\n",
      "</output>\n",
      "\n",
      "<input>\n",
      "000000000000000000\n",
      "033333333333300000\n",
      "033333343333300000\n",
      "033333333333300000\n",
      "033133333333300000\n",
      "033333333383300000\n",
      "033333333333300000\n",
      "000000000000000000\n",
      "000000000000002220\n",
      "000000000000000400\n",
      "000000000000002220\n",
      "000220000212000000\n",
      "000280000020000000\n",
      "000002000000000000\n",
      "000000000000000000\n",
      "000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "333332223333\n",
      "333333433333\n",
      "333332223333\n",
      "321233332233\n",
      "332333332833\n",
      "333333333323\n",
      "</output>\n",
      "-----------------\n",
      "Now, solve the following puzzle based on its input grid by applying the rules you have learned from the training data.:\n",
      "-----------------\n",
      "<input>\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "002720000002200000000000\n",
      "002200000002600000000000\n",
      "000000000000020000000000\n",
      "000000000000000000000000\n",
      "000002000000000000000000\n",
      "000021200000000000000000\n",
      "000002000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "044444444444444400000000\n",
      "044444444444444400022000\n",
      "044414444444474400008200\n",
      "044444444444444400002200\n",
      "044444444444444400000000\n",
      "044444444464444400000000\n",
      "044444444444444400000000\n",
      "043444444444448400000000\n",
      "044444444444444400000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000022000000000000000\n",
      "000000232000000000000000\n",
      "000000220000000000000000\n",
      "000000000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "\n",
      "</output>\n",
      "-----------------\n",
      "What is the output grid? Only provide the output grid in the form as in the example input and output pairs. Do not provide any additional information:\n",
      "model\n",
      "<output>\n",
      "444444444444444\n",
      "444244444444444\n",
      "442124444442724\n",
      "444244444442244\n",
      "444444442244444\n",
      "444444442644444\n",
      "422444444424224\n",
      "232444444444482\n",
      "224444444444422\n",
      "</output>\n",
      "model\n",
      "You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.\n",
      "\n",
      "Here are the example input and output pairs from which you should learn the underlying rule to later predict the output for the given test input:\n",
      "-----------------\n",
      "<input>\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "\n",
      "</output>\n",
      "-----------------\n",
      "Now, solve the following puzzle based on its input grid by applying the rules you have learned from the training data.:\n",
      "-----------------\n",
      "<input>\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "\n",
      "</output>\n",
      "-----------------\n",
      "What is the output grid? Only provide the output grid in the form as in the example input and output pairs. Do not provide any additional information:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "user\n",
      "You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.\n",
      "\n",
      "Here are the example input and output pairs from which you should learn the underlying rule to later predict the output for the given test input:\n",
      "-----------------\n",
      "<input>\n",
      "0000000000000000000000\n",
      "0000002000000000000000\n",
      "0020002320000000000000\n",
      "0272000020000000000000\n",
      "0020000000000000000000\n",
      "0000020000111111111110\n",
      "0000242000111111111110\n",
      "0000022000111111113110\n",
      "0000000000111111111110\n",
      "0222000000111111111110\n",
      "0282000000111141111110\n",
      "0202000000111111111110\n",
      "0000000000111111111110\n",
      "0000000000111111111110\n",
      "0000000000118111171110\n",
      "0000000000111111111110\n",
      "0000000000111111111110\n",
      "0000000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "11111111111\n",
      "11111112111\n",
      "11111112321\n",
      "11111111121\n",
      "11112111111\n",
      "11124211111\n",
      "11112211111\n",
      "11111111111\n",
      "12221112111\n",
      "12821127211\n",
      "12121112111\n",
      "11111111111\n",
      "</output>\n",
      "\n",
      "<input>\n",
      "00000000000000000000\n",
      "00000000000000000000\n",
      "00000000000200000000\n",
      "00000000002120000000\n",
      "00000000002000002020\n",
      "00000000000000000320\n",
      "08888880000000002020\n",
      "08888880000000000000\n",
      "08188880000000000000\n",
      "08888880000202000000\n",
      "08888880000042000000\n",
      "08888880000222000000\n",
      "08888880000000000000\n",
      "08884880000000000000\n",
      "08888880000000000000\n",
      "00000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "888888\n",
      "828888\n",
      "212888\n",
      "288888\n",
      "888888\n",
      "888888\n",
      "882828\n",
      "888428\n",
      "882228\n",
      "</output>\n",
      "\n",
      "<input>\n",
      "000000000000000000\n",
      "033333333333300000\n",
      "033333343333300000\n",
      "033333333333300000\n",
      "033133333333300000\n",
      "033333333383300000\n",
      "033333333333300000\n",
      "000000000000000000\n",
      "000000000000002220\n",
      "000000000000000400\n",
      "000000000000002220\n",
      "000220000212000000\n",
      "000280000020000000\n",
      "000002000000000000\n",
      "000000000000000000\n",
      "000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "333332223333\n",
      "333333433333\n",
      "333332223333\n",
      "321233332233\n",
      "332333332833\n",
      "333333333323\n",
      "</output>\n",
      "-----------------\n",
      "Now, solve the following puzzle based on its input grid by applying the rules you have learned from the training data.:\n",
      "-----------------\n",
      "<input>\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "002720000002200000000000\n",
      "002200000002600000000000\n",
      "000000000000020000000000\n",
      "000000000000000000000000\n",
      "000002000000000000000000\n",
      "000021200000000000000000\n",
      "000002000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "044444444444444400000000\n",
      "044444444444444400022000\n",
      "044414444444474400008200\n",
      "044444444444444400002200\n",
      "044444444444444400000000\n",
      "044444444464444400000000\n",
      "044444444444444400000000\n",
      "043444444444448400000000\n",
      "044444444444444400000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000022000000000000000\n",
      "000000232000000000000000\n",
      "000000220000000000000000\n",
      "000000000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "\n",
      "</output>\n",
      "-----------------\n",
      "What is the output grid? Only provide the output grid in the form as in the example input and output pairs. Do not provide any additional information:\n",
      "model\n",
      "<output>\n",
      "444444444444444\n",
      "444244444444444\n",
      "442124444442724\n",
      "444244444442244\n",
      "444444442244444\n",
      "444444442644444\n",
      "422444444424224\n",
      "232444444444482\n",
      "224444444444422\n",
      "</output>\n",
      "model\n",
      "You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.\n",
      "\n",
      "Here are the example input and output pairs from which you should learn the underlying rule to later predict the output for the given test input:\n",
      "-----------------\n",
      "<input>\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "\n",
      "</output>\n",
      "-----------------\n",
      "Now, solve the following puzzle based on its input grid by applying the rules you have learned from the training data.:\n",
      "-----------------\n",
      "<input>\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "\n",
      "</output>\n",
      "-----------------\n",
      "What is the output grid? Only provide the output grid in the form as in the example input and output pairs. Do not provide any additional information:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "user\n",
      "You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.\n",
      "\n",
      "Here are the example input and output pairs from which you should learn the underlying rule to later predict the output for the given test input:\n",
      "-----------------\n",
      "<input>\n",
      "0000000000000000000000\n",
      "0000002000000000000000\n",
      "0020002320000000000000\n",
      "0272000020000000000000\n",
      "0020000000000000000000\n",
      "0000020000111111111110\n",
      "0000242000111111111110\n",
      "0000022000111111113110\n",
      "0000000000111111111110\n",
      "0222000000111111111110\n",
      "0282000000111141111110\n",
      "0202000000111111111110\n",
      "0000000000111111111110\n",
      "0000000000111111111110\n",
      "0000000000118111171110\n",
      "0000000000111111111110\n",
      "0000000000111111111110\n",
      "0000000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "11111111111\n",
      "11111112111\n",
      "11111112321\n",
      "11111111121\n",
      "11112111111\n",
      "11124211111\n",
      "11112211111\n",
      "11111111111\n",
      "12221112111\n",
      "12821127211\n",
      "12121112111\n",
      "11111111111\n",
      "</output>\n",
      "\n",
      "<input>\n",
      "00000000000000000000\n",
      "00000000000000000000\n",
      "00000000000200000000\n",
      "00000000002120000000\n",
      "00000000002000002020\n",
      "00000000000000000320\n",
      "08888880000000002020\n",
      "08888880000000000000\n",
      "08188880000000000000\n",
      "08888880000202000000\n",
      "08888880000042000000\n",
      "08888880000222000000\n",
      "08888880000000000000\n",
      "08884880000000000000\n",
      "08888880000000000000\n",
      "00000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "888888\n",
      "828888\n",
      "212888\n",
      "288888\n",
      "888888\n",
      "888888\n",
      "882828\n",
      "888428\n",
      "882228\n",
      "</output>\n",
      "\n",
      "<input>\n",
      "000000000000000000\n",
      "033333333333300000\n",
      "033333343333300000\n",
      "033333333333300000\n",
      "033133333333300000\n",
      "033333333383300000\n",
      "033333333333300000\n",
      "000000000000000000\n",
      "000000000000002220\n",
      "000000000000000400\n",
      "000000000000002220\n",
      "000220000212000000\n",
      "000280000020000000\n",
      "000002000000000000\n",
      "000000000000000000\n",
      "000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "333332223333\n",
      "333333433333\n",
      "333332223333\n",
      "321233332233\n",
      "332333332833\n",
      "333333333323\n",
      "</output>\n",
      "-----------------\n",
      "Now, solve the following puzzle based on its input grid by applying the rules you have learned from the training data.:\n",
      "-----------------\n",
      "<input>\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "002720000002200000000000\n",
      "002200000002600000000000\n",
      "000000000000020000000000\n",
      "000000000000000000000000\n",
      "000002000000000000000000\n",
      "000021200000000000000000\n",
      "000002000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "044444444444444400000000\n",
      "044444444444444400022000\n",
      "044414444444474400008200\n",
      "044444444444444400002200\n",
      "044444444444444400000000\n",
      "044444444464444400000000\n",
      "044444444444444400000000\n",
      "043444444444448400000000\n",
      "044444444444444400000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000022000000000000000\n",
      "000000232000000000000000\n",
      "000000220000000000000000\n",
      "000000000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "\n",
      "</output>\n",
      "-----------------\n",
      "What is the output grid? Only provide the output grid in the form as in the example input and output pairs. Do not provide any additional information:\n",
      "model\n",
      "<output>\n",
      "444444444444444\n",
      "444244444444444\n",
      "442124444442724\n",
      "444244444442244\n",
      "444444442244444\n",
      "444444442644444\n",
      "422444444424224\n",
      "232444444444482\n",
      "224444444444422\n",
      "</output>\n",
      "model\n",
      "You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.\n",
      "\n",
      "Here are the example input and output pairs from which you should learn the underlying rule to later predict the output for the given test input:\n",
      "-----------------\n",
      "<input>\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "\n",
      "</output>\n",
      "-----------------\n",
      "Now, solve the following puzzle based on its input grid by applying the rules you have learned from the training data.:\n",
      "-----------------\n",
      "<input>\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "\n",
      "</output>\n",
      "-----------------\n",
      "What is the output grid? Only provide the output grid in the form as in the example input and output pairs. Do not provide any additional information:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "user\n",
      "You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.\n",
      "\n",
      "Here are the example input and output pairs from which you should learn the underlying rule to later predict the output for the given test input:\n",
      "-----------------\n",
      "<input>\n",
      "0000000000000000000000\n",
      "0000002000000000000000\n",
      "0020002320000000000000\n",
      "0272000020000000000000\n",
      "0020000000000000000000\n",
      "0000020000111111111110\n",
      "0000242000111111111110\n",
      "0000022000111111113110\n",
      "0000000000111111111110\n",
      "0222000000111111111110\n",
      "0282000000111141111110\n",
      "0202000000111111111110\n",
      "0000000000111111111110\n",
      "0000000000111111111110\n",
      "0000000000118111171110\n",
      "0000000000111111111110\n",
      "0000000000111111111110\n",
      "0000000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "11111111111\n",
      "11111112111\n",
      "11111112321\n",
      "11111111121\n",
      "11112111111\n",
      "11124211111\n",
      "11112211111\n",
      "11111111111\n",
      "12221112111\n",
      "12821127211\n",
      "12121112111\n",
      "11111111111\n",
      "</output>\n",
      "\n",
      "<input>\n",
      "00000000000000000000\n",
      "00000000000000000000\n",
      "00000000000200000000\n",
      "00000000002120000000\n",
      "00000000002000002020\n",
      "00000000000000000320\n",
      "08888880000000002020\n",
      "08888880000000000000\n",
      "08188880000000000000\n",
      "08888880000202000000\n",
      "08888880000042000000\n",
      "08888880000222000000\n",
      "08888880000000000000\n",
      "08884880000000000000\n",
      "08888880000000000000\n",
      "00000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "888888\n",
      "828888\n",
      "212888\n",
      "288888\n",
      "888888\n",
      "888888\n",
      "882828\n",
      "888428\n",
      "882228\n",
      "</output>\n",
      "\n",
      "<input>\n",
      "000000000000000000\n",
      "033333333333300000\n",
      "033333343333300000\n",
      "033333333333300000\n",
      "033133333333300000\n",
      "033333333383300000\n",
      "033333333333300000\n",
      "000000000000000000\n",
      "000000000000002220\n",
      "000000000000000400\n",
      "000000000000002220\n",
      "000220000212000000\n",
      "000280000020000000\n",
      "000002000000000000\n",
      "000000000000000000\n",
      "000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "333332223333\n",
      "333333433333\n",
      "333332223333\n",
      "321233332233\n",
      "332333332833\n",
      "333333333323\n",
      "</output>\n",
      "-----------------\n",
      "Now, solve the following puzzle based on its input grid by applying the rules you have learned from the training data.:\n",
      "-----------------\n",
      "<input>\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "002720000002200000000000\n",
      "002200000002600000000000\n",
      "000000000000020000000000\n",
      "000000000000000000000000\n",
      "000002000000000000000000\n",
      "000021200000000000000000\n",
      "000002000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "044444444444444400000000\n",
      "044444444444444400022000\n",
      "044414444444474400008200\n",
      "044444444444444400002200\n",
      "044444444444444400000000\n",
      "044444444464444400000000\n",
      "044444444444444400000000\n",
      "043444444444448400000000\n",
      "044444444444444400000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000022000000000000000\n",
      "000000232000000000000000\n",
      "000000220000000000000000\n",
      "000000000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "\n",
      "</output>\n",
      "-----------------\n",
      "What is the output grid? Only provide the output grid in the form as in the example input and output pairs. Do not provide any additional information:\n",
      "model\n",
      "<output>\n",
      "444444444444444\n",
      "444244444444444\n",
      "442124444442724\n",
      "444244444442244\n",
      "444444442244444\n",
      "444444442644444\n",
      "422444444424224\n",
      "232444444444482\n",
      "224444444444422\n",
      "</output>\n",
      "model\n",
      "You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.\n",
      "\n",
      "Here are the example input and output pairs from which you should learn the underlying rule to later predict the output for the given test input:\n",
      "-----------------\n",
      "<input>\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "\n",
      "</output>\n",
      "-----------------\n",
      "Now, solve the following puzzle based on its input grid by applying the rules you have learned from the training data.:\n",
      "-----------------\n",
      "<input>\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "\n",
      "</output>\n",
      "-----------------\n",
      "What is the output grid? Only provide the output grid in the form as in the example input and output pairs. Do not provide any additional information:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "user\n",
      "You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.\n",
      "\n",
      "Here are the example input and output pairs from which you should learn the underlying rule to later predict the output for the given test input:\n",
      "-----------------\n",
      "<input>\n",
      "0000000000000000000000\n",
      "0000002000000000000000\n",
      "0020002320000000000000\n",
      "0272000020000000000000\n",
      "0020000000000000000000\n",
      "0000020000111111111110\n",
      "0000242000111111111110\n",
      "0000022000111111113110\n",
      "0000000000111111111110\n",
      "0222000000111111111110\n",
      "0282000000111141111110\n",
      "0202000000111111111110\n",
      "0000000000111111111110\n",
      "0000000000111111111110\n",
      "0000000000118111171110\n",
      "0000000000111111111110\n",
      "0000000000111111111110\n",
      "0000000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "11111111111\n",
      "11111112111\n",
      "11111112321\n",
      "11111111121\n",
      "11112111111\n",
      "11124211111\n",
      "11112211111\n",
      "11111111111\n",
      "12221112111\n",
      "12821127211\n",
      "12121112111\n",
      "11111111111\n",
      "</output>\n",
      "\n",
      "<input>\n",
      "00000000000000000000\n",
      "00000000000000000000\n",
      "00000000000200000000\n",
      "00000000002120000000\n",
      "00000000002000002020\n",
      "00000000000000000320\n",
      "08888880000000002020\n",
      "08888880000000000000\n",
      "08188880000000000000\n",
      "08888880000202000000\n",
      "08888880000042000000\n",
      "08888880000222000000\n",
      "08888880000000000000\n",
      "08884880000000000000\n",
      "08888880000000000000\n",
      "00000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "888888\n",
      "828888\n",
      "212888\n",
      "288888\n",
      "888888\n",
      "888888\n",
      "882828\n",
      "888428\n",
      "882228\n",
      "</output>\n",
      "\n",
      "<input>\n",
      "000000000000000000\n",
      "033333333333300000\n",
      "033333343333300000\n",
      "033333333333300000\n",
      "033133333333300000\n",
      "033333333383300000\n",
      "033333333333300000\n",
      "000000000000000000\n",
      "000000000000002220\n",
      "000000000000000400\n",
      "000000000000002220\n",
      "000220000212000000\n",
      "000280000020000000\n",
      "000002000000000000\n",
      "000000000000000000\n",
      "000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "333332223333\n",
      "333333433333\n",
      "333332223333\n",
      "321233332233\n",
      "332333332833\n",
      "333333333323\n",
      "</output>\n",
      "-----------------\n",
      "Now, solve the following puzzle based on its input grid by applying the rules you have learned from the training data.:\n",
      "-----------------\n",
      "<input>\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "002720000002200000000000\n",
      "002200000002600000000000\n",
      "000000000000020000000000\n",
      "000000000000000000000000\n",
      "000002000000000000000000\n",
      "000021200000000000000000\n",
      "000002000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "044444444444444400000000\n",
      "044444444444444400022000\n",
      "044414444444474400008200\n",
      "044444444444444400002200\n",
      "044444444444444400000000\n",
      "044444444464444400000000\n",
      "044444444444444400000000\n",
      "043444444444448400000000\n",
      "044444444444444400000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000022000000000000000\n",
      "000000232000000000000000\n",
      "000000220000000000000000\n",
      "000000000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "\n",
      "</output>\n",
      "-----------------\n",
      "What is the output grid? Only provide the output grid in the form as in the example input and output pairs. Do not provide any additional information:\n",
      "model\n",
      "<output>\n",
      "444444444444444\n",
      "444244444444444\n",
      "442124444442724\n",
      "444244444442244\n",
      "444444442244444\n",
      "444444442644444\n",
      "422444444424224\n",
      "232444444444482\n",
      "224444444444422\n",
      "</output>\n",
      "model\n",
      "You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.\n",
      "\n",
      "Here are the example input and output pairs from which you should learn the underlying rule to later predict the output for the given test input:\n",
      "-----------------\n",
      "<input>\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "\n",
      "</output>\n",
      "-----------------\n",
      "Now, solve the following puzzle based on its input grid by applying the rules you have learned from the training data.:\n",
      "-----------------\n",
      "<input>\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "000000000000000000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "\n",
      "</output>\n",
      "-----------------\n",
      "What is the output grid? Only provide the output grid in the form as in the example input and output pairs. Do not provide any additional information:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    print(tokenizer.decode(output, skip_special_tokens=True))\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
