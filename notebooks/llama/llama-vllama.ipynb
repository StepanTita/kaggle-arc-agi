{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1\n",
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/home/stepan/kaggle-arc-agi\"\n",
    "# MODEL_ID = f\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"\n",
    "MODEL_ID = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
    "# VLLM_MODEL_ID = \"unsloth/Llama-3.2-11B-Vision-Instruct\"\n",
    "VLLM_MODEL_ID = \"4bit/Qwen2-VL-7B-Instruct\"\n",
    "MAX_NEW_TOKENS = 2048\n",
    "MAX_SEQ_LENGTH = 32768 - MAX_NEW_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(BASE_PATH)\n",
    "sys.path.append(f\"{BASE_PATH}/scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stepan/.conda/envs/llm-py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig  # type: ignore\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "from tqdm.auto import tqdm  # type: ignore\n",
    "\n",
    "import data_utils # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n",
    "    llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        max_memory = {0: \"23.5GiB\", \"cpu\": \"16GiB\"},\n",
    "        flash_attention_2=True,\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(VLLM_MODEL_ID)\n",
    "    vllm_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        VLLM_MODEL_ID,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        max_memory = {1: \"23.5GiB\", \"cpu\": \"16GiB\"},\n",
    "        flash_attention_2=True,\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'llm': llm_model,\n",
    "        'tokenizer': tokenizer,\n",
    "        'vllm': vllm_model,\n",
    "        'processor': processor\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/home/stepan/.conda/envs/llm-py310/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:777: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.57it/s]\n"
     ]
    }
   ],
   "source": [
    "models = get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 105/105 [00:00<00:00, 523.28 examples/s]\n",
      "Map: 100%|██████████| 416/416 [00:00<00:00, 1468.51 examples/s]\n",
      "Map: 100%|██████████| 419/419 [00:00<00:00, 984.38 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'challenge', 'solution', 'texts', 'messages'],\n",
       "        num_rows: 416\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'challenge', 'solution', 'texts', 'messages'],\n",
       "        num_rows: 293\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['id', 'challenge', 'solution', 'texts', 'messages'],\n",
       "        num_rows: 126\n",
       "    })\n",
       "    predict: Dataset({\n",
       "        features: ['id', 'challenge', 'texts', 'messages'],\n",
       "        num_rows: 105\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = data_utils.prepare_dataset(models['tokenizer'], fit_dataset=False, base_path=BASE_PATH, final_training=False, prepare_inputs_func=data_utils.prepare_inputs)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 0, 7], [7, 0, 7], [7, 7, 0]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['challenge']['test']['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(f):\n",
    "    def wrapper(model, *args, **kwargs):\n",
    "        if hasattr(model, 'to_inference'):\n",
    "            model.to_inference()\n",
    "        else:\n",
    "            model.eval()\n",
    "        with torch.no_grad():\n",
    "            return f(model, *args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def train(f):\n",
    "    def wrapper(model, *args, **kwargs):\n",
    "        if hasattr(model, 'to_training'):\n",
    "            model.to_training()\n",
    "        else:\n",
    "            model.train()\n",
    "        return f(model, *args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eval\n",
    "def describe_puzzle(model, processor, image, prompt):\n",
    "    # Create prompt\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(text=[text], images=[image], return_tensors=\"pt\")\n",
    "    inputs = inputs.to(model.device)\n",
    "\n",
    "    # Run inference\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    generated_ids = generated_ids[0, inputs.input_ids.shape[1]:]\n",
    "    generated_text = processor.decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def list_to_image(integer_list_2d, target_size=30):\n",
    "    # Convert the 2D list to a NumPy array\n",
    "    array = np.array(integer_list_2d, dtype=np.uint8)\n",
    "    \n",
    "    # Normalize the array to use the full range of 0-255\n",
    "    if array.max() > 0:\n",
    "        array = (array * (255 / array.max())).astype(np.uint8)\n",
    "    \n",
    "    # Create an image from the array\n",
    "    image = Image.fromarray(array, mode='L')\n",
    "    \n",
    "    # Create a new blank image with the target size\n",
    "    new_image = Image.new('L', (target_size, target_size), color=0)\n",
    "    \n",
    "    # Paste the original image onto the new image\n",
    "    new_image.paste(image, (0, 0))\n",
    "    \n",
    "    new_image = new_image.resize((target_size * 10, target_size * 10), Image.NEAREST)\n",
    "    \n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAAAAABcFtGpAAADa0lEQVR4Ae3bwW3bQBQEUDFwIepEUiW2K5FciZVKJHWiTpyT5/sgBBhfEhovlwyWQ8J4+wlsCGfZbT7/3D7Dl7/fk1+TLkmnpJek6WVpc0x8S1pj+LXGH/pf/cywCnlYsAqBomqyYBUCRdVkwSoEiqrJKrCW6c45+57FfdKszSn8I1fPSdek30lz5j9kbY3BZBW7BgtWIVBUTRasQqComixYhUBRNVmwCoGi+vT30/U+j7onTZjj/3sWz0lzgj9lbd3Ba1jsHyxYhUBRNVmwCoGiarJgFQJF1WTBKgSK6jJn7+Kub1dfv33n/3Cj17DYBViwCoGiarJgFQJF1WTBKgSKqsmCVQgU1eU55XPSo2/r21w9Jd2S5t8Bc0Y/5urcMU/OxRUFr2GxWbBgFQJF1WTBKgSKqsmCVQgUVZMFqxAoqg+P1HP23uZRczLP0pf/oTq9SYcp/pDkNSw2EhasQqComixYhUBRNVmwCoGiarJgFQJFdbmkfEq6Jc239WvWzklz/J/enPSf07snzZOztKLgNSw2CxasQqComixYhUBRNVmwCoGiarJgFQJF1e/BF1heQ1iFQFE1WbAKgaJqsmAVAkXVZMEqBIqqySqw5jP6Zr6Zv+QB96T5tr7L2nxRf/QN/pLeIWndwWQV+wcLViFQVE0WrEKgqJosWIVAUTVZsAqBoro8Oo/P/XMyv2fxLekj6Zw0J/0sbaa3zOIKk9ew2DRYsAqBomqyYBUCRdVkwSoEiqrJglUIFNWna8pzuj5mbcI+8Z40d8xJf5er26RT0rqD17DYP1iwCoGiarJgFQJF1WTBKgSKqsmCVQioEiBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgMAPFfgDmjYwOC2+ZFgAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=300x300>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = list_to_image(dataset['train'][10]['challenge']['train'][0]['input'])\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stepan/.conda/envs/llm-py310/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:777: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The image appears to be a pixelated or low-resolution representation of a grid or pattern. The grid consists of small, square elements that vary in shades of gray, creating a somewhat abstract or textured appearance. The overall design is reminiscent of a digital or computer-generated image, possibly representing a simple pattern or texture.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe_puzzle(models['vllm'], models['processor'], image, \"Describe the image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, latent_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.condition_dim = condition_dim\n",
    "        self.attention = nn.MultiheadAttention(input_dim, num_heads=4)\n",
    "        \n",
    "        self.query = nn.Linear(input_dim, hidden_dim)\n",
    "        self.key = nn.Linear(input_dim, hidden_dim)\n",
    "        self.value = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim + condition_dim, hidden_dim)\n",
    "\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)      # Mean of the latent space\n",
    "        self.fc_var = nn.Linear(hidden_dim, latent_dim)     # Variance of the latent space\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        # Add the condition to the input\n",
    "        condition = condition.unsqueeze(1).repeat(1, x.size(1), 1)  # Repeat condition for each time step if needed\n",
    "        x_cond = torch.cat([x, condition], dim=2)\n",
    "\n",
    "        # Apply attention\n",
    "        attn_output, _ = self.attention(self.query(x_cond), self.key(x_cond), self.value(x_cond))\n",
    "        h = F.relu(self.fc1(attn_output.mean(dim=1)))  # Reduce to a single representation per sample\n",
    "\n",
    "        # Compute the mean and variance for the latent space\n",
    "        mu = self.fc_mu(h)\n",
    "        log_var = self.fc_var(h)\n",
    "\n",
    "        return mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu, log_var):\n",
    "    std = torch.exp(0.5 * log_var)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps * std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, condition_dim, output_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.condition_dim = condition_dim\n",
    "        self.fc1 = nn.Linear(latent_dim + condition_dim, hidden_dim)\n",
    "        \n",
    "        self.query = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.key = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.value = nn.Linear(latent_dim, hidden_dim)\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4)\n",
    "        self.fc_output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, z, condition, output_len):\n",
    "        # Combine latent variable z and condition\n",
    "        condition = condition.unsqueeze(1).repeat(1, output_len, 1)  # Repeat condition for each time step if needed\n",
    "        z_cond = torch.cat([z.unsqueeze(1).repeat(1, output_len, 1), condition], dim=2)\n",
    "\n",
    "        h = F.relu(self.fc1(z_cond))\n",
    "        \n",
    "        # Apply attention to guide the generation process\n",
    "        attn_output, _ = self.attention(self.query(h), self.key(h), self.value(h))\n",
    "\n",
    "        # Generate output\n",
    "        output = torch.sigmoid(self.fc_output(attn_output))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, latent_dim, output_dim, hidden_dim):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, condition_dim, latent_dim, hidden_dim)\n",
    "        self.decoder = Decoder(latent_dim, condition_dim, output_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, condition, output_len):\n",
    "        # Encode\n",
    "        mu, log_var = self.encoder(x, condition)\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        z = reparameterize(mu, log_var)\n",
    "\n",
    "        # Decode\n",
    "        output = self.decoder(z, condition, output_len)\n",
    "\n",
    "        return output, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARCModel(torch.nn.Module):\n",
    "    def __init__(self, llm_model, vllm_model):\n",
    "        super().__init__()\n",
    "        self.llm_model = llm_model\n",
    "        self.vllm_model = vllm_model\n",
    "        \n",
    "        self.text_proj = nn.Linear(4096, 2304)\n",
    "        self.image_proj = nn.Linear(3584, 2304)\n",
    "        \n",
    "        self.cvae = CVAE(input_dim=2304, condition_dim=2304, latent_dim=512, output_dim=30, hidden_dim=1024)\n",
    "\n",
    "    def to(self, device):\n",
    "        self.device = device\n",
    "        super().to(device)\n",
    "        return self\n",
    "\n",
    "    def to_inference(self):\n",
    "        self.llm_model.eval()\n",
    "        self.vllm_model.eval()\n",
    "\n",
    "    def to_training(self):\n",
    "        self.llm_model.train()\n",
    "        self.vllm_model.train()\n",
    "        \n",
    "    def cvae_loss(self,recon_x, x, mu, log_var):\n",
    "        recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum') # TODO: try BCELoss\n",
    "        # KL Divergence loss\n",
    "        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        return recon_loss + kl_loss\n",
    "    \n",
    "    def encode(self, text_inputs, image_inputs):\n",
    "        text_features = self.llm_model(**text_inputs.to(self.llm_model.device)).hidden_states[-1] # (batch_size, seq_len, 4096)\n",
    "        image_features = self.vllm_model(**image_inputs.to(self.vllm_model.device)).hidden_states[-1] # (batch_size, vid_len, 3584)\n",
    "        \n",
    "        text_features = self.text_proj(text_features.to(self.device))\n",
    "        image_features = self.image_proj(image_features.to(self.device))\n",
    "        \n",
    "        features = torch.cat([text_features, image_features], dim=1) # (batch_size, seq_len + vid_len, 2304)\n",
    "        return features\n",
    "\n",
    "    def forward(self, train_inputs, test_inputs, targets=None):\n",
    "        train_features = self.encode(text_inputs=train_inputs['text'], image_inputs=train_inputs['image'])\n",
    "        test_features = self.encode(text_inputs=test_inputs['text'], image_inputs=test_inputs['image'])\n",
    "        \n",
    "        return self.cvae(train_features, test_features, targets)\n",
    "\n",
    "    def from_pretrained(self, path):\n",
    "        # self.space_model.load_state_dict(torch.load(f\"{path}/space_model.pth\"))\n",
    "        # self.classifier.load_state_dict(torch.load(f\"{path}/classifier.pth\"))\n",
    "        # return self\n",
    "        ...\n",
    "\n",
    "    def save_pretrained(self, path):\n",
    "        # self.base_model.save_pretrained(f\"{path}/base\")\n",
    "        # torch.save(self.space_model.state_dict(), f\"{path}/space_model.pth\")\n",
    "        # torch.save(self.classifier.state_dict(), f\"{path}/classifier.pth\")\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ARCModel(\n",
       "  (llm_model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(128256, 3072)\n",
       "      (layers): ModuleList(\n",
       "        (0-27): 28 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaSdpaAttention(\n",
       "            (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "  )\n",
       "  (vllm_model): Qwen2VLForConditionalGeneration(\n",
       "    (visual): Qwen2VisionTransformerPretrainedModel(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "      )\n",
       "      (rotary_pos_emb): VisionRotaryEmbedding()\n",
       "      (blocks): ModuleList(\n",
       "        (0-31): 32 x Qwen2VLVisionBlock(\n",
       "          (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): VisionSdpaAttention(\n",
       "            (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "            (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (mlp): VisionMlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): QuickGELUActivation()\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (merger): PatchMerger(\n",
       "        (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=3584, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (model): Qwen2VLModel(\n",
       "      (embed_tokens): Embedding(152064, 3584)\n",
       "      (layers): ModuleList(\n",
       "        (0-27): 28 x Qwen2VLDecoderLayer(\n",
       "          (self_attn): Qwen2VLSdpaAttention(\n",
       "            (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "            (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "            (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "            (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "            (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "            (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       "  )\n",
       "  (text_proj): Linear(in_features=4096, out_features=2304, bias=True)\n",
       "  (image_proj): Linear(in_features=3584, out_features=2304, bias=True)\n",
       "  (cvae): CVAE(\n",
       "    (encoder): Encoder(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=2304, out_features=2304, bias=True)\n",
       "      )\n",
       "      (query): Linear(in_features=2304, out_features=1024, bias=True)\n",
       "      (key): Linear(in_features=2304, out_features=1024, bias=True)\n",
       "      (value): Linear(in_features=2304, out_features=1024, bias=True)\n",
       "      (fc1): Linear(in_features=4608, out_features=1024, bias=True)\n",
       "      (fc_mu): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (fc_var): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (fc1): Linear(in_features=2816, out_features=1024, bias=True)\n",
       "      (query): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (key): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (value): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (fc_output): Linear(in_features=1024, out_features=30, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arc_model = ARCModel(models['llm'], models['vllm'])\n",
    "arc_model.to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_model.to_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": \"Describe the image\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "image_features = models['processor'].apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "image_features = models['processor'](text=[image_features], images=[image], return_tensors=\"pt\")\n",
    "image_features = image_features.to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43marc_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokenizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtexts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_features\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43mtest_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokenizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtexts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_features\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm-py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm-py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[83], line 42\u001b[0m, in \u001b[0;36mARCModel.forward\u001b[0;34m(self, train_inputs, test_inputs, targets)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_inputs, test_inputs, targets):\n\u001b[0;32m---> 42\u001b[0m     train_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     test_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(text_inputs\u001b[38;5;241m=\u001b[39mtest_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], image_inputs\u001b[38;5;241m=\u001b[39mtest_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcvae(train_features, test_features, targets)\n",
      "Cell \u001b[0;32mIn[83], line 32\u001b[0m, in \u001b[0;36mARCModel.encode\u001b[0;34m(self, text_inputs, image_inputs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, image_inputs):\n\u001b[0;32m---> 32\u001b[0m     text_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtext_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# (batch_size, seq_len, 4096)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mimage_inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_model\u001b[38;5;241m.\u001b[39mdevice))\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# (batch_size, vid_len, 3584)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     text_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_proj(text_features\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))\n",
      "File \u001b[0;32m~/.conda/envs/llm-py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm-py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llm-py310/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/llm-py310/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1189\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   1186\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/llm-py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm-py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llm-py310/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/llm-py310/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:946\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    943\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 946\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;66;03m# kept for BC (non `Cache` `past_key_values` inputs)\u001b[39;00m\n\u001b[1;32m    949\u001b[0m return_legacy_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llm-py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm-py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llm-py310/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/llm-py310/lib/python3.10/site-packages/torch/nn/modules/sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm-py310/lib/python3.10/site-packages/torch/nn/functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "arc_model(\n",
    "    train_inputs={\n",
    "    'text': models['tokenizer'](dataset['train'][0]['texts'], return_tensors='pt'),\n",
    "    'image': image_features\n",
    "}, \n",
    "test_inputs={\n",
    "    'text': models['tokenizer'](dataset['train'][0]['texts'], return_tensors='pt'),\n",
    "    'image': image_features\n",
    "},\n",
    "targets=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cvae(model, dataset, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for json_object in dataset:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Prepare inputs for the CVAE\n",
    "            train_features, test_features = prepare_inputs(json_object)\n",
    "            \n",
    "            # Forward pass through the CVAE\n",
    "            generated_output, mu, log_var = model(train_features, test_features)\n",
    "            \n",
    "            # Compute loss (reconstruction + KL divergence)\n",
    "            test_output = extract_llm_features([json_object['test_example']['output']])\n",
    "            loss = cvae_loss(generated_output, test_output, mu, log_var)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}, Loss: {total_loss / len(dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm - 4096\n",
    "# vllm - 3584"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
