{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["# ! pip install torch==2.4.1 torchvision==0.19.0\n","# ! pip install accelerate==0.34.2\n","# ! pip install transformers==4.45.1\n","# ! pip install unsloth==2024.9.post3\n","! pip install bitsandbytes==0.44.0\n","! pip install qwen-vl-utils\n","! pip install optimum\n","! pip install auto-gptq"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:45:46.834705Z","iopub.status.busy":"2024-10-19T21:45:46.834127Z","iopub.status.idle":"2024-10-19T21:45:46.845960Z","shell.execute_reply":"2024-10-19T21:45:46.845031Z","shell.execute_reply.started":"2024-10-19T21:45:46.834659Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["env: CUDA_VISIBLE_DEVICES=0,1\n","env: TOKENIZERS_PARALLELISM=false\n"]}],"source":["%env CUDA_VISIBLE_DEVICES=0,1\n","%env TOKENIZERS_PARALLELISM=false"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:45:47.022694Z","iopub.status.busy":"2024-10-19T21:45:47.022377Z","iopub.status.idle":"2024-10-19T21:45:47.027326Z","shell.execute_reply":"2024-10-19T21:45:47.026308Z","shell.execute_reply.started":"2024-10-19T21:45:47.022659Z"},"trusted":true},"outputs":[],"source":["BASE_PATH = \"/kaggle/input\"\n","# MODEL_ID = f\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"\n","MODEL_ID = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n","# VLLM_MODEL_ID = \"unsloth/Llama-3.2-11B-Vision-Instruct\"\n","VLLM_MODEL_ID = \"Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4\"\n","MAX_NEW_TOKENS = 2048\n","MAX_SEQ_LENGTH = 32768 - MAX_NEW_TOKENS"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:45:47.204747Z","iopub.status.busy":"2024-10-19T21:45:47.204460Z","iopub.status.idle":"2024-10-19T21:45:47.209116Z","shell.execute_reply":"2024-10-19T21:45:47.207956Z","shell.execute_reply.started":"2024-10-19T21:45:47.204717Z"},"trusted":true},"outputs":[],"source":["import sys\n","\n","# sys.path.append(BASE_PATH)\n","# sys.path.append(f\"{BASE_PATH}/scripts\")\n","sys.path.append('/kaggle/input/arc-agi-python-utilities')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:45:47.367264Z","iopub.status.busy":"2024-10-19T21:45:47.366953Z","iopub.status.idle":"2024-10-19T21:45:54.700277Z","shell.execute_reply":"2024-10-19T21:45:54.699457Z","shell.execute_reply.started":"2024-10-19T21:45:47.367230Z"},"trusted":true},"outputs":[],"source":["import io\n","import json\n","import base64\n","from PIL import Image\n","\n","import numpy as np\n","from tqdm.auto import tqdm\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.distributions import Normal\n","\n","import transformers\n","from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig  # type: ignore\n","from transformers import MllamaForConditionalGeneration, Qwen2VLForConditionalGeneration, AutoProcessor\n","from transformers import get_linear_schedule_with_warmup\n","\n","from datasets import Dataset, DatasetDict  # type: ignore\n","from datasets import concatenate_datasets  # type: ignore\n","\n","from qwen_vl_utils import process_vision_info # type: ignore\n","\n","import data_utils  # type: ignore"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:45:54.702749Z","iopub.status.busy":"2024-10-19T21:45:54.702008Z","iopub.status.idle":"2024-10-19T21:45:54.707196Z","shell.execute_reply":"2024-10-19T21:45:54.706164Z","shell.execute_reply.started":"2024-10-19T21:45:54.702700Z"},"trusted":true},"outputs":[],"source":["dtype = torch.bfloat16"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:45:54.708973Z","iopub.status.busy":"2024-10-19T21:45:54.708563Z","iopub.status.idle":"2024-10-19T21:45:54.717299Z","shell.execute_reply":"2024-10-19T21:45:54.716271Z","shell.execute_reply.started":"2024-10-19T21:45:54.708925Z"},"trusted":true},"outputs":[],"source":["def get_models():\n","    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n","    llm_model = AutoModelForCausalLM.from_pretrained(\n","        MODEL_ID,\n","        torch_dtype=dtype,\n","        device_map=\"auto\",\n","        max_memory={0: \"15GiB\", \"cpu\": \"16GiB\"},\n","        attn_implementation=\"sdpa\",\n","        output_hidden_states=True,\n","        return_dict_in_generate=True,\n","        quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n","    )\n","\n","    processor = AutoProcessor.from_pretrained(VLLM_MODEL_ID)\n","    vllm_model = Qwen2VLForConditionalGeneration.from_pretrained(\n","        VLLM_MODEL_ID,\n","        torch_dtype=dtype,\n","        device_map=\"auto\",\n","        max_memory={1: \"15GiB\", \"cpu\": \"16GiB\"},\n","        attn_implementation=\"sdpa\",\n","        output_hidden_states=True,\n","        return_dict_in_generate=True,\n","#         quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n","    )\n","\n","    return {\"llm\": llm_model, \"tokenizer\": tokenizer, \"vllm\": vllm_model, \"processor\": processor}"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:45:54.720694Z","iopub.status.busy":"2024-10-19T21:45:54.720017Z","iopub.status.idle":"2024-10-19T21:46:05.371321Z","shell.execute_reply":"2024-10-19T21:46:05.370090Z","shell.execute_reply.started":"2024-10-19T21:45:54.720635Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","/opt/conda/lib/python3.10/site-packages/transformers/quantizers/auto.py:182: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n","  warnings.warn(warning_msg)\n","Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n","`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n","/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:4779: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n","  warnings.warn(\n"]}],"source":["models = get_models()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:46:05.373048Z","iopub.status.busy":"2024-10-19T21:46:05.372728Z","iopub.status.idle":"2024-10-19T21:46:05.388016Z","shell.execute_reply":"2024-10-19T21:46:05.386879Z","shell.execute_reply.started":"2024-10-19T21:46:05.373011Z"},"trusted":true},"outputs":[],"source":["TRAIN_IMAGE_SYSTEM_PROMPT = (\n","    \"\"\"You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet. \n","Your task is to analyze this task in details.\n","In your analysis follow these steps:\n","1. Initial Observation:\n","- How many images are in the set?\n","- What is the general structure of each image (grid size, number of colored squares)?\n","- What colors are present across all images?\n","\n","2. Analyze Individual Images:\n","For each image, note:\n","- The arrangement of colored squares\n","- The frequency of each color\n","- Any patterns or structures formed by the colored squares\n","\n","3. Compare Images:\n","Look for similarities and differences between images:\n","- Are certain color combinations more common?\n","- Do specific patterns of colored squares appear in multiple images?\n","- Are there consistent relationships between colors across images?\n","\n","4. Identify Potential Rules or Patterns:\n","- Consider various types of logical rules that might apply:\n","- Color replacement rules (e.g., blue always becomes red)\n","- Positional rules (e.g., corners always share a color)\n","- Quantity rules (e.g., number of squares of each color)\n","- Shape or pattern rules (e.g., colors forming specific shapes)\n","- Relational rules (e.g., red squares always adjacent to blue)\n","\n","5. Test Pattern Hypotheses:\n","For each potential rule or pattern:\n","- Check if it consistently applies across all images\n","- Note any exceptions or special cases\n","- Consider if multiple rules might be working together\n","\n","6. Refine and Generalize the Pattern:\n","- Formulate a general description of the pattern(s) that applies to all images\n","- Ensure the description accounts for all observed variations\n","\n","7. Describe the Pattern in Detail:\n","- Provide a clear, step-by-step explanation of the pattern\n","- Use precise language to describe color relationships, positions, or transformations\n","- Include any conditions or exceptions to the main rule\n","\n","8. Verify the Description:\n","- Mentally apply your pattern description to the images. Does it accurately describe all of them?\n","- Consider if your description would work for hypothetical new images following the same logic\n","\n","9. Summarize the Pattern:\n","- Offer a concise yet comprehensive summary of the underlying logic\n","- Highlight the key aspects that define the puzzle's pattern\"\"\"\n",")\n","TEST_IMAGE_SYSTEM_PROMPT = (\n","    \"\"\"You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.\n","Your task is to analyze this task in details. Follow these steps:\n","1. Initial Observation:\n","   - What is the size of the grid (e.g., 3x3, 5x5)?\n","   - How many colored squares are present?\n","   - What colors are used in this image?\n","\n","2. Color Distribution:\n","   - Count the number of squares for each color\n","   - Note if any colors are dominant or rare\n","\n","3. Spatial Analysis:\n","   - Describe the position of each colored square:\n","     * Use precise coordinates (e.g., top-left is (0,0))\n","     * Note any patterns in color placement (e.g., diagonal, clustered)\n","\n","4. Edge and Corner Analysis:\n","   - Describe the colors present on the edges of the grid\n","   - Note the colors in each corner\n","   - Identify any patterns specific to edges or corners\n","\n","5. Symmetry and Balance:\n","   - Is the image symmetrical? If so, in what way? (horizontal, vertical, rotational)\n","   - Is there a balance in color distribution across the grid?\n","\n","6. Patterns and Structures:\n","   - Identify any repeating patterns of colors\n","   - Note any shapes or structures formed by same-colored squares\n","   - Describe any linear sequences or progressions of colors\n","\n","7. Color Relationships:\n","   - Analyze how different colors relate to each other:\n","     * Are certain colors always adjacent?\n","     * Do some colors appear in specific arrangements relative to others?\n","\n","8. Unique Features:\n","   - Highlight any standout characteristics that seem unusual or significant\n","   - Note any squares that appear to break an otherwise consistent pattern\n","\n","9. Quantitative Aspects:\n","   - Calculate ratios or percentages of different colors\n","   - Note any numerical patterns in color distribution\n","\n","10. Comparative Elements:\n","    - If there are elements that might be compared (e.g., left vs right, top vs bottom), describe these comparisons\n","\n","11. Abstraction:\n","    - Try to describe the image in more abstract terms, as if explaining a rule\n","    - Consider how this image might be part of a larger pattern or rule set\n","\n","12. Summary:\n","    - Provide a concise yet comprehensive summary of the image's key features\n","    - Highlight the most notable or potentially significant aspects of the image\"\"\"\n",")\n","\n","TRAIN_TEXT_SYSTEM_PROMPT = \"\"\"You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet. \n","Your task is to analyze this task in details.\n","In your analysis follow these steps:\n","1. Initial Observation:\n","   - How many matrices are in the set?\n","   - What is the general structure of each matrix (dimensions, range of numbers)?\n","   - What numbers are present across all matrices?\n","\n","2. Analyze Individual Matrices:\n","   - For each matrix, note:\n","     * The arrangement of numbers\n","     * The frequency of each number\n","     * Any patterns or structures formed by the numbers\n","\n","3. Compare Matrices:\n","   - Look for similarities and differences between matrices:\n","     * Are certain number combinations more common?\n","     * Do specific patterns of numbers appear in multiple matrices?\n","     * Are there consistent relationships between numbers across matrices?\n","\n","4. Identify Potential Rules or Patterns:\n","   - Consider various types of logical rules that might apply:\n","     * Number replacement rules (e.g., 1 always becomes 2)\n","     * Positional rules (e.g., corners always share a value)\n","     * Quantity rules (e.g., count of each number)\n","     * Shape or pattern rules (e.g., numbers forming specific shapes)\n","     * Relational rules (e.g., even numbers always adjacent to odd)\n","     * Mathematical operations (e.g., addition, multiplication, modulo)\n","\n","5. Test Pattern Hypotheses:\n","   - For each potential rule or pattern:\n","     * Check if it consistently applies across all matrices\n","     * Note any exceptions or special cases\n","   - Consider if multiple rules might be working together\n","\n","6. Refine and Generalize the Pattern:\n","   - Formulate a general description of the pattern(s) that applies to all matrices\n","   - Ensure the description accounts for all observed variations\n","\n","7. Describe the Pattern in Detail:\n","   - Provide a clear, step-by-step explanation of the pattern\n","   - Use precise language to describe number relationships, positions, or transformations\n","   - Include any conditions or exceptions to the main rule\n","\n","8. Verify the Description:\n","   - Mentally apply your pattern description to the matrices. Does it accurately describe all of them?\n","   - Consider if your description would work for hypothetical new matrices following the same logic\n","\n","9. Summarize the Pattern:\n","   - Offer a concise yet comprehensive summary of the underlying logic\n","   - Highlight the key aspects that define the puzzle's pattern\"\"\"\n","\n","TEST_TEXT_SYSTEM_PROMPT = \"\"\"You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet. \n","Your task is to analyze this task in details.\n","In your analysis follow these steps:\n","1. Initial Observation:\n","   - What are the dimensions of the matrix (e.g., 3x3, 5x5)?\n","   - What is the range of numbers present?\n","   - Are there any immediately noticeable patterns or repetitions?\n","\n","2. Number Distribution:\n","   - Count the frequency of each number\n","   - Note if any numbers are particularly common or rare\n","\n","3. Spatial Analysis:\n","   - Describe the position of key numbers:\n","     * Use precise coordinates (e.g., top-left is (0,0))\n","     * Note any patterns in number placement (e.g., diagonal, clustered)\n","\n","4. Edge and Corner Analysis:\n","   - Describe the numbers present on the edges of the matrix\n","   - Note the numbers in each corner\n","   - Identify any patterns specific to edges or corners\n","\n","5. Symmetry and Balance:\n","   - Is the matrix symmetrical? If so, in what way? (horizontal, vertical, rotational)\n","   - Is there a balance in number distribution across the matrix?\n","\n","6. Patterns and Structures:\n","   - Identify any repeating patterns of numbers\n","   - Note any shapes or structures formed by identical or related numbers\n","   - Describe any sequences or progressions of numbers (e.g., arithmetic, geometric)\n","\n","7. Number Relationships:\n","   - Analyze how different numbers relate to each other:\n","     * Are certain numbers always adjacent?\n","     * Do some numbers appear in specific arrangements relative to others?\n","   - Look for mathematical relationships (e.g., sums, products, differences)\n","\n","8. Unique Features:\n","   - Highlight any standout characteristics that seem unusual or significant\n","   - Note any numbers that appear to break an otherwise consistent pattern\n","\n","9. Quantitative Aspects:\n","   - Calculate any relevant statistics (e.g., mean, median, mode)\n","   - Note any numerical patterns in the overall distribution\n","\n","10. Comparative Elements:\n","    - If there are elements that might be compared (e.g., rows vs columns, quadrants), describe these comparisons\n","\n","11. Abstraction:\n","    - Try to describe the matrix in more abstract terms, as if explaining a rule\n","    - Consider how this matrix might be part of a larger pattern or rule set\n","\n","12. Summary:\n","    - Provide a concise yet comprehensive summary of the matrix's key features\n","    - Highlight the most notable or potentially significant aspects of the matrix\"\"\"\n","\n","TRAIN_TEXT_PROMPT = \"\"\"Here are the example input and output pairs from which you should learn the underlying rule to later predict the output for the given test input:\n","-----------------\n","{training_data}\"\"\"\n","\n","TEST_TEXT_PROMPT = \"\"\"Here is the input test data:\n","-----------------\n","{input_test_data}\"\"\"\n","\n","TRAIN_IMAGE_PROMPT = \"Analyze the images\"\n","TEST_IMAGE_PROMPT = \"Analyze the image\""]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:46:05.389795Z","iopub.status.busy":"2024-10-19T21:46:05.389486Z","iopub.status.idle":"2024-10-19T21:46:05.399540Z","shell.execute_reply":"2024-10-19T21:46:05.398565Z","shell.execute_reply.started":"2024-10-19T21:46:05.389761Z"},"trusted":true},"outputs":[],"source":["def list_to_image(integer_list_2d, target_size=30):\n","    # Convert the 2D list to a NumPy array\n","    array = np.array(integer_list_2d)\n","\n","    # Get the unique values in the array\n","    unique_values = np.unique(array)\n","\n","    # Create a colormap\n","    cmap = plt.get_cmap(\"tab10\")\n","\n","    # Create a color lookup dictionary\n","    color_lookup = {value: cmap(i % 10)[:3] for i, value in enumerate(unique_values)}\n","\n","    # Create an RGB array\n","    rgb_array = np.array([[color_lookup[val] for val in row] for row in array])\n","\n","    # Convert to 8-bit color values\n","    rgb_array = (rgb_array * 255).astype(np.uint8)\n","\n","    # Create an image from the colored array\n","    image = Image.fromarray(rgb_array, mode=\"RGB\")\n","\n","    # Create a new blank image with the target size\n","    new_image = Image.new(\"RGB\", (target_size, target_size), color=(0, 0, 0))\n","\n","    # Paste the original image onto the new image\n","    new_image.paste(image, (0, 0))\n","\n","    new_image = new_image.resize((target_size * 15, target_size * 15), Image.NEAREST)\n","\n","    return new_image\n","\n","def pil_image_to_base64(image):\n","    buffered = io.BytesIO()\n","    image.save(buffered, format=\"PNG\")\n","    return 'data:image;base64,' + base64.b64encode(buffered.getvalue()).decode('utf-8')\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:46:05.401505Z","iopub.status.busy":"2024-10-19T21:46:05.400786Z","iopub.status.idle":"2024-10-19T21:46:05.410279Z","shell.execute_reply":"2024-10-19T21:46:05.409335Z","shell.execute_reply.started":"2024-10-19T21:46:05.401458Z"},"trusted":true},"outputs":[],"source":["def prepare_inputs(dct, prepare_solution=False):\n","    if prepare_solution:\n","        return \"<output>\\n\" + \"\\n\".join(\" \".join(map(str, row)) for row in dct) + \"\\n</output>\"\n","    else:\n","        input_str = \"\\n\".join(\" \".join(map(str, row)) for row in dct[\"input\"])\n","        output_str = \"\\n\".join(\" \".join(map(str, row)) for row in dct[\"output\"]) if \"output\" in dct else \"\"\n","        text = f\"<input>\\n{input_str}\\n</input>\"\n","        if output_str:\n","            text += f\"\\n\\n<output>\\n{output_str}\\n</output>\"\n","        return text"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:46:05.411945Z","iopub.status.busy":"2024-10-19T21:46:05.411580Z","iopub.status.idle":"2024-10-19T21:46:05.436820Z","shell.execute_reply":"2024-10-19T21:46:05.435947Z","shell.execute_reply.started":"2024-10-19T21:46:05.411910Z"},"trusted":true},"outputs":[],"source":["def to_dataset(data, solutions=None):\n","    restructured_data = {\n","        \"id\": [],\n","        \"challenge\": [],\n","    }\n","    if solutions is not None:\n","        restructured_data[\"solution\"] = []\n","\n","    for challenge_id, challenge_data in data.items():  # for all challenges\n","        for test_id, task in enumerate(\n","            challenge_data[\"test\"]\n","        ):  # for all test tasks in this challenge we want to expand dataset so that each test task is separate dataset record\n","            restructured_data[\"id\"].append(challenge_id)\n","            restructured_data[\"challenge\"].append({\"train\": challenge_data[\"train\"], \"test\": task, \"order\": test_id})\n","            if solutions is not None:\n","                restructured_data[\"solution\"].append(solutions[challenge_id][test_id])\n","\n","    return Dataset.from_dict(restructured_data)\n","\n","\n","def prepare_inputs(dct, prepare_solution=False):\n","    if prepare_solution:\n","        return \"<output>\\n\" + \"\\n\".join(\" \".join(map(str, row)) for row in dct) + \"\\n</output>\"\n","    else:\n","        input_str = \"\\n\".join(\" \".join(map(str, row)) for row in dct[\"input\"])\n","        output_str = \"\\n\".join(\" \".join(map(str, row)) for row in dct[\"output\"]) if \"output\" in dct else \"\"\n","        text = f\"<input>\\n{input_str}\\n</input>\"\n","        if output_str:\n","            text += f\"\\n\\n<output>\\n{output_str}\\n</output>\"\n","        return text\n","\n","\n","def prepare_dataset(tokenizer, base_path=None, final_training=False):\n","    # Load all datasets\n","    training_challenges = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_training_challenges.json\")\n","    training_solutions = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_training_solutions.json\")\n","    evaluation_challenges = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_evaluation_challenges.json\")\n","    evaluation_solutions = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_evaluation_solutions.json\")\n","    test_challenges = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_test_challenges.json\")\n","\n","    train_dataset = to_dataset(training_challenges, training_solutions)\n","    eval_dataset = to_dataset(evaluation_challenges, evaluation_solutions)\n","    pred_dataset = to_dataset(test_challenges)\n","    \n","    def create_train_image_content(challenge):\n","        content = [{\"type\": \"text\", \"text\": TRAIN_IMAGE_SYSTEM_PROMPT}]\n","        \n","        for i, example in enumerate(challenge[\"train\"]):\n","            content.extend([\n","                {\"type\": \"text\", \"text\": f\"Input Task {i+1}\"},\n","                {\"type\": \"image\", \"image\": pil_image_to_base64(list_to_image(example[\"input\"]))},\n","                {\"type\": \"text\", \"text\": f\"Output Task {i+1}\"},\n","                {\"type\": \"image\", \"image\": pil_image_to_base64(list_to_image(example[\"output\"]))}\n","            ])\n","        \n","        content.append({\"type\": \"text\", \"text\": TRAIN_IMAGE_PROMPT})\n","        return content\n","\n","    def create_chat(challenge, solution=None):\n","        train_input = TRAIN_TEXT_SYSTEM_PROMPT.format(\n","            training_data=\"\\n\\n\".join([prepare_inputs(ex) for ex in challenge[\"train\"]]),\n","        )\n","        test_input = TEST_TEXT_SYSTEM_PROMPT.format(\n","            input_test_data=prepare_inputs(challenge[\"test\"]),\n","        )\n","\n","        train_text_messages = [\n","            {\"role\": \"system\", \"content\": TRAIN_TEXT_SYSTEM_PROMPT},\n","            {\"role\": \"user\", \"content\": train_input},\n","        ]\n","\n","        test_text_messages = [\n","            {\"role\": \"system\", \"content\": TEST_TEXT_SYSTEM_PROMPT},\n","            {\"role\": \"user\", \"content\": test_input},\n","        ]\n","\n","        train_image_messages = [\n","            {\n","                \"role\": \"user\",\n","                \"content\": create_train_image_content(challenge),\n","            },\n","        ]\n","\n","        test_image_messages = [\n","            {\n","                \"role\": \"user\",\n","                \"content\": [\n","                    {\"type\": \"text\", \"text\": TEST_IMAGE_SYSTEM_PROMPT},\n","                    {\"type\": \"text\", \"text\": f\"Input Test Task\"},\n","                    {\"type\": \"image\", \"image\": pil_image_to_base64(list_to_image(challenge[\"test\"][\"input\"]))},\n","                    {\"type\": \"text\", \"text\": TEST_IMAGE_PROMPT},\n","                ],\n","            },\n","        ]\n","\n","        if solution:\n","            test_text_messages.append(\n","                {\n","                    \"role\": \"assistant\",\n","                    \"content\": prepare_inputs(solution, prepare_solution=True),\n","                }\n","            )\n","\n","        return {\n","            \"train_text_messages\": train_text_messages,\n","            \"test_text_messages\": test_text_messages,\n","            \"train_image_messages\": train_image_messages,\n","            \"test_image_messages\": test_image_messages,\n","        }\n","\n","    def process_dataset(examples, solutions=None):\n","        # Create messages for each challenge-solution pair\n","        chats = []\n","        for challenge, solution in zip(examples[\"challenge\"], solutions or [None] * len(examples[\"challenge\"])):\n","            chat = create_chat(challenge, solution)\n","            chats.append(chat)\n","\n","        return {\"messages\": chats}\n","\n","    pred_dataset = pred_dataset.map(lambda x: process_dataset(x), batched=True)\n","    train_dataset = train_dataset.map(lambda x: process_dataset(x, train_dataset[\"solution\"]), batched=True)\n","    eval_dataset = eval_dataset.map(lambda x: process_dataset(x, eval_dataset[\"solution\"]), batched=True)\n","\n","    if final_training:  # if final training, we need to add the validation dataset to the training dataset\n","        train_dataset = concatenate_datasets([train_dataset, eval_dataset]).shuffle(seed=42)\n","\n","        return DatasetDict(\n","            {\n","                \"train\": train_dataset,\n","                \"predict\": pred_dataset,\n","            }\n","        )\n","\n","    test_dataset = eval_dataset.train_test_split(test_size=0.3)\n","\n","    dataset = DatasetDict(\n","        {\n","            \"train\": train_dataset,\n","            \"test\": test_dataset[\"train\"],\n","            \"val\": test_dataset[\"test\"],\n","            \"predict\": pred_dataset,\n","        }\n","    )\n","\n","    return dataset"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:46:05.438204Z","iopub.status.busy":"2024-10-19T21:46:05.437863Z","iopub.status.idle":"2024-10-19T21:46:59.262576Z","shell.execute_reply":"2024-10-19T21:46:59.261665Z","shell.execute_reply.started":"2024-10-19T21:46:05.438169Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cf87cffe1c494e3f84bbb961e3271587","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/105 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"90849111332e444a9a6ee5150ae27d00","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/416 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"723716afe04f48a380a55fb58ddfebb9","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/419 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'challenge', 'solution', 'messages'],\n","        num_rows: 416\n","    })\n","    test: Dataset({\n","        features: ['id', 'challenge', 'solution', 'messages'],\n","        num_rows: 293\n","    })\n","    val: Dataset({\n","        features: ['id', 'challenge', 'solution', 'messages'],\n","        num_rows: 126\n","    })\n","    predict: Dataset({\n","        features: ['id', 'challenge', 'messages'],\n","        num_rows: 105\n","    })\n","})"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["dataset = prepare_dataset(models[\"tokenizer\"], base_path=BASE_PATH, final_training=False)\n","dataset"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:46:59.267153Z","iopub.status.busy":"2024-10-19T21:46:59.266801Z","iopub.status.idle":"2024-10-19T21:46:59.280285Z","shell.execute_reply":"2024-10-19T21:46:59.279180Z","shell.execute_reply.started":"2024-10-19T21:46:59.267082Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'test_image_messages': [{'content': [{'image': None,\n","     'text': \"You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.\\nYour task is to analyze this task in details. Follow these steps:\\n1. Initial Observation:\\n   - What is the size of the grid (e.g., 3x3, 5x5)?\\n   - How many colored squares are present?\\n   - What colors are used in this image?\\n\\n2. Color Distribution:\\n   - Count the number of squares for each color\\n   - Note if any colors are dominant or rare\\n\\n3. Spatial Analysis:\\n   - Describe the position of each colored square:\\n     * Use precise coordinates (e.g., top-left is (0,0))\\n     * Note any patterns in color placement (e.g., diagonal, clustered)\\n\\n4. Edge and Corner Analysis:\\n   - Describe the colors present on the edges of the grid\\n   - Note the colors in each corner\\n   - Identify any patterns specific to edges or corners\\n\\n5. Symmetry and Balance:\\n   - Is the image symmetrical? If so, in what way? (horizontal, vertical, rotational)\\n   - Is there a balance in color distribution across the grid?\\n\\n6. Patterns and Structures:\\n   - Identify any repeating patterns of colors\\n   - Note any shapes or structures formed by same-colored squares\\n   - Describe any linear sequences or progressions of colors\\n\\n7. Color Relationships:\\n   - Analyze how different colors relate to each other:\\n     * Are certain colors always adjacent?\\n     * Do some colors appear in specific arrangements relative to others?\\n\\n8. Unique Features:\\n   - Highlight any standout characteristics that seem unusual or significant\\n   - Note any squares that appear to break an otherwise consistent pattern\\n\\n9. Quantitative Aspects:\\n   - Calculate ratios or percentages of different colors\\n   - Note any numerical patterns in color distribution\\n\\n10. Comparative Elements:\\n    - If there are elements that might be compared (e.g., left vs right, top vs bottom), describe these comparisons\\n\\n11. Abstraction:\\n    - Try to describe the image in more abstract terms, as if explaining a rule\\n    - Consider how this image might be part of a larger pattern or rule set\\n\\n12. Summary:\\n    - Provide a concise yet comprehensive summary of the image's key features\\n    - Highlight the most notable or potentially significant aspects of the image\",\n","     'type': 'text'},\n","    {'image': None, 'text': 'Input Test Task', 'type': 'text'},\n","    {'image': 'data:image;base64,iVBORw0KGgoAAAANSUhEUgAAAcIAAAHCCAIAAADzel4SAAAC9klEQVR4nO3WsQ3CQBQFQYwIHFEDBVGBm0E047qogYgMWjhrA4yYiZ9OP1rd9L6dD8Mur3V8/JiX8fF0f46PAfbj+O0DAH6bjAIkMgqQyChAIqMAiYwCJDIKkMgoQCKjAImMAiQyCpDIKEAiowCJjAIkMgqQyChAIqMAiYwCJDIKkMgoQCKjAImMAiQyCpDIKEAiowCJjAIkMgqQyChAIqMAyWnT+jEv4+PLa93y9nXTJQA74TcKkMgoQCKjAImMAiQyCpDIKEAiowCJjAIkMgqQyChAIqMAiYwCJDIKkMgoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAv/sAO6oJqPQjafYAAAAASUVORK5CYII=',\n","     'text': None,\n","     'type': 'image'},\n","    {'image': None, 'text': 'Analyze the image', 'type': 'text'}],\n","   'role': 'user'}],\n"," 'test_text_messages': [{'content': \"You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet. \\nYour task is to analyze this task in details.\\nIn your analysis follow these steps:\\n1. Initial Observation:\\n   - What are the dimensions of the matrix (e.g., 3x3, 5x5)?\\n   - What is the range of numbers present?\\n   - Are there any immediately noticeable patterns or repetitions?\\n\\n2. Number Distribution:\\n   - Count the frequency of each number\\n   - Note if any numbers are particularly common or rare\\n\\n3. Spatial Analysis:\\n   - Describe the position of key numbers:\\n     * Use precise coordinates (e.g., top-left is (0,0))\\n     * Note any patterns in number placement (e.g., diagonal, clustered)\\n\\n4. Edge and Corner Analysis:\\n   - Describe the numbers present on the edges of the matrix\\n   - Note the numbers in each corner\\n   - Identify any patterns specific to edges or corners\\n\\n5. Symmetry and Balance:\\n   - Is the matrix symmetrical? If so, in what way? (horizontal, vertical, rotational)\\n   - Is there a balance in number distribution across the matrix?\\n\\n6. Patterns and Structures:\\n   - Identify any repeating patterns of numbers\\n   - Note any shapes or structures formed by identical or related numbers\\n   - Describe any sequences or progressions of numbers (e.g., arithmetic, geometric)\\n\\n7. Number Relationships:\\n   - Analyze how different numbers relate to each other:\\n     * Are certain numbers always adjacent?\\n     * Do some numbers appear in specific arrangements relative to others?\\n   - Look for mathematical relationships (e.g., sums, products, differences)\\n\\n8. Unique Features:\\n   - Highlight any standout characteristics that seem unusual or significant\\n   - Note any numbers that appear to break an otherwise consistent pattern\\n\\n9. Quantitative Aspects:\\n   - Calculate any relevant statistics (e.g., mean, median, mode)\\n   - Note any numerical patterns in the overall distribution\\n\\n10. Comparative Elements:\\n    - If there are elements that might be compared (e.g., rows vs columns, quadrants), describe these comparisons\\n\\n11. Abstraction:\\n    - Try to describe the matrix in more abstract terms, as if explaining a rule\\n    - Consider how this matrix might be part of a larger pattern or rule set\\n\\n12. Summary:\\n    - Provide a concise yet comprehensive summary of the matrix's key features\\n    - Highlight the most notable or potentially significant aspects of the matrix\",\n","   'role': 'system'},\n","  {'content': \"You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet. \\nYour task is to analyze this task in details.\\nIn your analysis follow these steps:\\n1. Initial Observation:\\n   - What are the dimensions of the matrix (e.g., 3x3, 5x5)?\\n   - What is the range of numbers present?\\n   - Are there any immediately noticeable patterns or repetitions?\\n\\n2. Number Distribution:\\n   - Count the frequency of each number\\n   - Note if any numbers are particularly common or rare\\n\\n3. Spatial Analysis:\\n   - Describe the position of key numbers:\\n     * Use precise coordinates (e.g., top-left is (0,0))\\n     * Note any patterns in number placement (e.g., diagonal, clustered)\\n\\n4. Edge and Corner Analysis:\\n   - Describe the numbers present on the edges of the matrix\\n   - Note the numbers in each corner\\n   - Identify any patterns specific to edges or corners\\n\\n5. Symmetry and Balance:\\n   - Is the matrix symmetrical? If so, in what way? (horizontal, vertical, rotational)\\n   - Is there a balance in number distribution across the matrix?\\n\\n6. Patterns and Structures:\\n   - Identify any repeating patterns of numbers\\n   - Note any shapes or structures formed by identical or related numbers\\n   - Describe any sequences or progressions of numbers (e.g., arithmetic, geometric)\\n\\n7. Number Relationships:\\n   - Analyze how different numbers relate to each other:\\n     * Are certain numbers always adjacent?\\n     * Do some numbers appear in specific arrangements relative to others?\\n   - Look for mathematical relationships (e.g., sums, products, differences)\\n\\n8. Unique Features:\\n   - Highlight any standout characteristics that seem unusual or significant\\n   - Note any numbers that appear to break an otherwise consistent pattern\\n\\n9. Quantitative Aspects:\\n   - Calculate any relevant statistics (e.g., mean, median, mode)\\n   - Note any numerical patterns in the overall distribution\\n\\n10. Comparative Elements:\\n    - If there are elements that might be compared (e.g., rows vs columns, quadrants), describe these comparisons\\n\\n11. Abstraction:\\n    - Try to describe the matrix in more abstract terms, as if explaining a rule\\n    - Consider how this matrix might be part of a larger pattern or rule set\\n\\n12. Summary:\\n    - Provide a concise yet comprehensive summary of the matrix's key features\\n    - Highlight the most notable or potentially significant aspects of the matrix\",\n","   'role': 'user'},\n","  {'content': '<output>\\n7 0 7 0 0 0 7 0 7\\n7 0 7 0 0 0 7 0 7\\n7 7 0 0 0 0 7 7 0\\n7 0 7 0 0 0 7 0 7\\n7 0 7 0 0 0 7 0 7\\n7 7 0 0 0 0 7 7 0\\n7 0 7 7 0 7 0 0 0\\n7 0 7 7 0 7 0 0 0\\n7 7 0 7 7 0 0 0 0\\n</output>',\n","   'role': 'assistant'}],\n"," 'train_image_messages': [{'content': [{'image': None,\n","     'text': \"You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet. \\nYour task is to analyze this task in details.\\nIn your analysis follow these steps:\\n1. Initial Observation:\\n- How many images are in the set?\\n- What is the general structure of each image (grid size, number of colored squares)?\\n- What colors are present across all images?\\n\\n2. Analyze Individual Images:\\nFor each image, note:\\n- The arrangement of colored squares\\n- The frequency of each color\\n- Any patterns or structures formed by the colored squares\\n\\n3. Compare Images:\\nLook for similarities and differences between images:\\n- Are certain color combinations more common?\\n- Do specific patterns of colored squares appear in multiple images?\\n- Are there consistent relationships between colors across images?\\n\\n4. Identify Potential Rules or Patterns:\\n- Consider various types of logical rules that might apply:\\n- Color replacement rules (e.g., blue always becomes red)\\n- Positional rules (e.g., corners always share a color)\\n- Quantity rules (e.g., number of squares of each color)\\n- Shape or pattern rules (e.g., colors forming specific shapes)\\n- Relational rules (e.g., red squares always adjacent to blue)\\n\\n5. Test Pattern Hypotheses:\\nFor each potential rule or pattern:\\n- Check if it consistently applies across all images\\n- Note any exceptions or special cases\\n- Consider if multiple rules might be working together\\n\\n6. Refine and Generalize the Pattern:\\n- Formulate a general description of the pattern(s) that applies to all images\\n- Ensure the description accounts for all observed variations\\n\\n7. Describe the Pattern in Detail:\\n- Provide a clear, step-by-step explanation of the pattern\\n- Use precise language to describe color relationships, positions, or transformations\\n- Include any conditions or exceptions to the main rule\\n\\n8. Verify the Description:\\n- Mentally apply your pattern description to the images. Does it accurately describe all of them?\\n- Consider if your description would work for hypothetical new images following the same logic\\n\\n9. Summarize the Pattern:\\n- Offer a concise yet comprehensive summary of the underlying logic\\n- Highlight the key aspects that define the puzzle's pattern\",\n","     'type': 'text'},\n","    {'image': None, 'text': 'Input Task 1', 'type': 'text'},\n","    {'image': 'data:image;base64,iVBORw0KGgoAAAANSUhEUgAAAcIAAAHCCAIAAADzel4SAAAC8ElEQVR4nO3coQ0CQRBAUY6coAuoi0bQNEMFFEQNKCQeteRnQ8i9pyebVT+jZjle7rthj8N5fPgry/U56WWAqfa//gDAf5NRgERGARIZBUhkFCCRUYBERgESGQVIZBQgkVGAREYBEhkFSGQUIJFRgGSdd/sOYAtsowCJjAIkMgqQyChAIqMAiYwCJDIKkMgoQCKjAImMAiQyCpDIKEAiowCJjAIk6+l1G592VQ/gg20UIJFRgERGARIZBUhkFCCRUYBERgESGQVIZBQgkVGAREYBEhkFSGQUIJFRAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACArXsDwtoIoCZxzBsAAAAASUVORK5CYII=',\n","     'text': None,\n","     'type': 'image'},\n","    {'image': None, 'text': 'Output Task 1', 'type': 'text'},\n","    {'image': 'data:image;base64,iVBORw0KGgoAAAANSUhEUgAAAcIAAAHCCAIAAADzel4SAAAD9klEQVR4nO3csU0DQRBAUR9y4IgWcF1uhBC5GVdAQa7BkTNIiIju9DndnngvXq0m+ppoprf3z8Pe3E+XlX4+P28jjDFdHyv9DPy5l60HANg3GQVIZBQgkVGAREYBEhkFSGQUIJFRgERGARIZBUhkFCCRUYBERgESGQVIjlsP8GO9o3OLDDIGsCO2UYBERgESGQVIZBQgkVGAREYBEhkFSGQUIJFRgERGARIZBUhkFCCRUYBERgGSUQ7lnZ+3+Y/XO2c3yBjAjthGARIZBUhkFCCRUYBERgESGQVIZBQgkVGAREYBEhkFSGQUIJFRgERGARIZBUiWHcob5EIdwDhsowCJjAIkMgqQyChAIqMAiYwCJDIKkMgoQCKjAImMAiQyCpDIKEAiowCJjAIk09fH69Yz8Nt0fWw9AjCXbRQgkVGAREYBEhkFSGQUIJFRgERGARIZBUhkFCCRUYBERgESGQVIZBQgkVGA5Hh+3ua/vp8uK81hDGCnbKMAiYwCJDIKkMgoQCKjAImMAiQyCpDIKEAiowCJjAIkMgqQyChAIqMAiYwCJMdFrxfdkQP4D2yjAImMAiQyCpDIKEAiowCJjAIkMgqQyChAIqMAiYwCJDIKkMgoQCKjAImMAiTLDuWt5366bD3C4TDMGMCO2EYBEhkFSGQUIJFRgERGARIZBUhkFCCRUYBERgESGQVIZBQgkVGAREYBEhkFSEY5lHd+3uY/Xu+c3SBjADtiGwVIZBQgkVGAREYBEhkFSGQUIJFRgERGARIZBUhkFCCRUYBERgESGQVIZBQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgL36BnDnKeHp7Be+AAAAAElFTkSuQmCC',\n","     'text': None,\n","     'type': 'image'},\n","    {'image': None, 'text': 'Input Task 2', 'type': 'text'},\n","    {'image': 'data:image;base64,iVBORw0KGgoAAAANSUhEUgAAAcIAAAHCCAIAAADzel4SAAAC/UlEQVR4nO3csQ3CMABFQUApUjEDAzFBlkEsw1zMQEUHKzh6igBxV39Zrp5cef+6HHfDTs/b+Pg+L+Pj/fUxPgb4HodPXwDgt8koQCKjAImMAiQyCpDIKEAiowCJjAIkMgqQyChAIqMAiYwCJDIKkMgoQDKt+vtulZUnnze6BsCmvEYBEhkFSGQUIJFRgERGARIZBUhkFCCRUYBERgESGQVIZBQgkVGAREYBEhkFSKZV6/u8jI+3+4IP4Ht4jQIkMgqQyChAIqMAiYwCJDIKkMgoQCKjAImMAiQyCpDIKEAiowCJjAIkMgoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPDv3sBjDSYTqaiaAAAAAElFTkSuQmCC',\n","     'text': None,\n","     'type': 'image'},\n","    {'image': None, 'text': 'Output Task 2', 'type': 'text'},\n","    {'image': 'data:image;base64,iVBORw0KGgoAAAANSUhEUgAAAcIAAAHCCAIAAADzel4SAAAD60lEQVR4nO3csU3DUBRAUQelcMUKeCAm8CKUiJJFMgEDmRWo0kFDgahsXVlOpHPqJ+vJxdWv3un79XFYbbpe1g8v47zTl/ez386bvnx6+1o/DBzr4egFAO6bjAIkMgqQyChAIqMAiYwCJDIKkMgoQCKjAImMAiQyCpDIKEAiowCJjAIkp6eXj6N34L/P9+ejVwDW8hoFSGQUIJFRgERGARIZBUhkFCCRUYBERgESGQVIZBQgkVGAREYBEhkFSGQUIDlvml7Gef3wdL1sXObO+BvA4DUKEMkoQCKjAImMAiQyCpDIKEAiowCJjAIkMgqQyChAIqMAiYwCJDIKkMgoQLLtUJ5rb3/5G8DgNQoQyShAIqMAiYwCJDIKkMgoQCKjAImMAiQyCpDIKEAiowCJjAIkMgqQyChAIqMAiYwCJDIKkMgoQCKjAImMAiQyCpDIKEAiowCJjAIkMgqQyChAIqMAiYwCJDIKkMgoQCKjAImMAiQyCpDIKEAiowCJjAIkMgqQyChAIqMAiYwCJDIKkMgoQCKjAImMAiTnoxf4tYzz+uHpermFLwMMXqMAkYwCJDIKkMgoQCKjAImMAiQyCpDIKEAiowCJjAIkMgqQyChAIqMAiYwCJLdyKG+/C3Vu3wG78hoFSGQUIJFRgERGARIZBUhkFCCRUYBERgESGQVIZBQgkVGAREYBEhkFSGQUILmVQ3mbLOO8ftihPGBXXqMAiYwCJDIKkMgoQCKjAImMAiQyCpDIKEAiowCJjAIkMgqQyChAIqMAiYwCAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHCvfgA1OCYP8crKZAAAAABJRU5ErkJggg==',\n","     'text': None,\n","     'type': 'image'},\n","    {'image': None, 'text': 'Input Task 3', 'type': 'text'},\n","    {'image': 'data:image;base64,iVBORw0KGgoAAAANSUhEUgAAAcIAAAHCCAIAAADzel4SAAAC80lEQVR4nO3cMQ0CQQBFQY5cgQsQhAKMoAEjKEAQuKDEAvCyOS7M1JvNr1622ml/vm1+wONyXHoCwDe2Sw8AWDcZBUhkFCCRUYBERgESGQVIZBQgkVGAREYBEhkFSGQUIJFRgERGARIZBUjmcVffd6f3D0/jdgCM5DUKkMgoQCKjAImMAiQyCpDIKEAiowCJjAIkMgqQyChAIqMAiYwCJDIKkMgoQDJ/9J3d4XkdNwVgjbxGARIZBUhkFCCRUYBERgESGQVIZBQgkVGAREYBEhkFSGQUIJFRgERGARIZBQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA+Hcv8xIHofzIfyAAAAAASUVORK5CYII=',\n","     'text': None,\n","     'type': 'image'},\n","    {'image': None, 'text': 'Output Task 3', 'type': 'text'},\n","    {'image': 'data:image;base64,iVBORw0KGgoAAAANSUhEUgAAAcIAAAHCCAIAAADzel4SAAADzklEQVR4nO3csW3CUBRA0TiiYAsyEBOwSGbIIpkgA5EtKGmhs3X9ZSzOqS3r6RdXr3rT6fvvgxfz/3PeegRgrs+tBwDYNxkFSGQUIJFRgERGARIZBUhkFCCRUYBERgESGQVIZBQgkVGAREYBEhkFSGQUIJFRgERGARIZBUhkFCCRUYBERgESGQVIZBQgkVGAREYBEhkFSGQUIJFRgERGARIZBUhkFCCRUYBERgESGQVIZBQgkVGAREYBEhkFSGQUIJFRgERGARIZBUhkFCCRUYBERgESGQVIZBQgkVGAREYBEhkFSGQUIJFRgERGARIZBUhkFCCRUYBERgESGQVIZBQgOWw9wLu4Hi/zP57GzQGszTYKkMgoQCKjAImMAiQyCpDIKEAiowCJjAIkMgqQyChAIqMAiYwCJDIKkMgoQOJQ3pNF5+y+br/jJgH2wjYKkMgoQCKjAImMAiQyCpDIKEAiowCJjAIkMgqQyChAIqMAiYwCJDIKkMgoQOJQ3pNxt+8W/vk8aAxgdbZRgERGARIZBUhkFCCRUYBERgESGQVIZBQgkVGAREYBEhkFSGQUIJFRgERGAZKBh/Kux8v8j8ddqHsRi15jGjcHsDbbKEAiowCJjAIkMgqQyChAIqMAiYwCJDIKkMgoQCKjAImMAiQyCpDIKEAiowDJwTm7R14DWMo2CpDIKEAiowCJjAIkMgqQyChAIqMAiYwCJDIKkMgoQCKjAImMAiQyCpDIKAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAe3UHfwUWZlWCxSsAAAAASUVORK5CYII=',\n","     'text': None,\n","     'type': 'image'},\n","    {'image': None, 'text': 'Input Task 4', 'type': 'text'},\n","    {'image': 'data:image;base64,iVBORw0KGgoAAAANSUhEUgAAAcIAAAHCCAIAAADzel4SAAAC9UlEQVR4nO3csQkCQRBAUU8uMLIGC7ICmzG2EevSLsy0BZfPInLvxcMy0WeiXd7X426O0+v+/fDzdp60BsBU+18vAPDfZBQgkVGAREYBEhkFSGQUIJFRgERGARIZBUhkFCCRUYBERgESGQVIZBQgWYemh/6+A9gC1yhAIqMAiYwCJDIKkMgoQCKjAImMAiQyCpDIKEAiowCJjAIkMgqQyChAIqMAyTr0993jcJm0xzLpXYDJXKMAiYwCJDIKkMgoQCKjAImMAiQyCpDIKEAiowCJjAIkMgqQyChAIqMAiYwCAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABs3QciKAodBlaASgAAAABJRU5ErkJggg==',\n","     'text': None,\n","     'type': 'image'},\n","    {'image': None, 'text': 'Output Task 4', 'type': 'text'},\n","    {'image': 'data:image;base64,iVBORw0KGgoAAAANSUhEUgAAAcIAAAHCCAIAAADzel4SAAAD50lEQVR4nO3csU3EQBBA0TvkwBE1UBAVuBliGqEu0wXZkRPd6svyGr0Xr1YTfU0098fH6+0Ybz9fzz/e122GMSbx/fl+9gjAs17OHgDg2mQUIJFRgERGARIZBUhkFCCRUYBERgESGQVIZBQgkVGAREYBEhkFSGQUIFmGXk9ydG6SMQButlGASEYBEhkFSGQUIJFRgERGARIZBUhkFCCRUYBERgESGQVIZBQgkVGAREYBkmXo6Ny+bgfNccUxAG62UYBIRgESGQVIZBQgkVGAREYBEhkFSGQUIJFRgERGARIZBUhkFCCRUYBERgGS5YpH55yzA+ZhGwVIZBQgkVGAREYBEhkFSGQUIJFRgERGARIZBUhkFCCRUYBERgESGQVIZBQgWYZeu1AH8IdtFCCRUYBERgESGQVIZBQgkVGAREYBEhkFSGQUIJFRgERGARIZBUhkFCCRUYBkGbp9t6/bQXM4wQdclG0UIJFRgERGARIZBUhkFCCRUYBERgESGQVIZBQgkVGAREYBEhkFSGQUIJFRgGQZen3cObtJTvBNMgZwIbZRgERGARIZBUhkFCCRUYBERgESGQVIZBQgkVGAREYBEhkFSGQUIJFRgERGAZKxQ3mTmOTo3CRjAOeyjQIkMgqQyChAIqMAiYwCJDIKkMgoQCKjAImMAiQyCpDIKEAiowCJjAIkMgqQzHIob+jo3L5u/3uM+0H/AgewjQIkMgqQyChAIqMAiYwCJDIKkMgoQCKjAImMAiQyCpDIKEAiowCJjAIkMgoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwFX9AlP6MM2woPM2AAAAAElFTkSuQmCC',\n","     'text': None,\n","     'type': 'image'},\n","    {'image': None, 'text': 'Input Task 5', 'type': 'text'},\n","    {'image': 'data:image;base64,iVBORw0KGgoAAAANSUhEUgAAAcIAAAHCCAIAAADzel4SAAAC80lEQVR4nO3WsQ3CQBQFQYwcEFEDBVEBzSCaoS5qICKDgArwyjoQM/Hp9KLVn57n/eYLTJf76AkAS2xHDwD4bTIKkMgoQCKjAImMAiQyCpDIKEAiowCJjAIkMgqQyChAIqMAiYwCJDIKkMyHx3X0hrfj6AEAS7hGARIZBUhkFCCRUYBERgESGQVIZBQgkVGAREYBEhkFSGQUIJFRgERGARIZBUjmj17fdqeVdkwr/QuwMtcoQCKjAImMAiQyCpDIKEAiowCJjAIkMgqQyChAIqMAiYwCJDIKkMgoQCKjAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/+4FnnEGq5PoUYAAAAAASUVORK5CYII=',\n","     'text': None,\n","     'type': 'image'},\n","    {'image': None, 'text': 'Output Task 5', 'type': 'text'},\n","    {'image': 'data:image;base64,iVBORw0KGgoAAAANSUhEUgAAAcIAAAHCCAIAAADzel4SAAAD6UlEQVR4nO3csU3DUBhGUYxSpGIFMlAmyCKUiJJFMgEDhRWo0kFDBY2tK+s50Tm1ZX3V1V+96fv16YGNmd6+Rk8A5nocPQDgtskoQCKjAImMAiQyCpDIKEAiowCJjAIkMgqQyChAIqMAiYwCJDIKkMgoQLI7XM+jN/DfcfQAYC7XKEAiowCJjAIkMgqQyChAIqMAiYwCJDIKkMgoQCKjAImMAiQyCpDIKEAiowDJbtHXl/1ppR2L3uu7+xnTSv8FVuAaBUhkFCCRUYBERgESGQVIZBQgkVGAREYBEhkFSGQUIJFRgERGARIZBUhkFCCZnl8+Rm/gr8/34+gJwFyuUYBERgESGQVIZBQgkVGAREYBEhkFSGQUIJFRgERGARIZBUhkFCCRUYBERgESGQVIZBQgkVGAREYBEhkFSGQUIJFRgERGARIZBUhkFCCRUYBERgESGQVIZBQgkVGAREYBEhkFSGQUIJFRgERGARIZBUhkFCCRUYBERgESGQVIZBQgkVGAREYBEhkFSGQUINmNHvDrsj+NnrAh0+gBwHyuUYBERgESGQVIZBQgkVGAREYBEhkFSGQUIJFRgERGARIZBUhkFCCRUYBERgGSrTyUd7ieR0/YlOPoAcBcrlGAREYBEhkFSGQUIJFRgERGARIZBUhkFCCRUYBERgESGQVIZBQgkVGAREYBkq08lLfIZX9a6c+L3utbb8a00n+BFbhGARIZBUhkFCCRUYBERgESGQVIZBQgkVGAREYBEhkFSGQUIJFRgERGARIZBQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgVv0AqDQX9NOlW9kAAAAASUVORK5CYII=',\n","     'text': None,\n","     'type': 'image'},\n","    {'image': None, 'text': 'Analyze the images', 'type': 'text'}],\n","   'role': 'user'}],\n"," 'train_text_messages': [{'content': \"You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet. \\nYour task is to analyze this task in details.\\nIn your analysis follow these steps:\\n1. Initial Observation:\\n   - How many matrices are in the set?\\n   - What is the general structure of each matrix (dimensions, range of numbers)?\\n   - What numbers are present across all matrices?\\n\\n2. Analyze Individual Matrices:\\n   - For each matrix, note:\\n     * The arrangement of numbers\\n     * The frequency of each number\\n     * Any patterns or structures formed by the numbers\\n\\n3. Compare Matrices:\\n   - Look for similarities and differences between matrices:\\n     * Are certain number combinations more common?\\n     * Do specific patterns of numbers appear in multiple matrices?\\n     * Are there consistent relationships between numbers across matrices?\\n\\n4. Identify Potential Rules or Patterns:\\n   - Consider various types of logical rules that might apply:\\n     * Number replacement rules (e.g., 1 always becomes 2)\\n     * Positional rules (e.g., corners always share a value)\\n     * Quantity rules (e.g., count of each number)\\n     * Shape or pattern rules (e.g., numbers forming specific shapes)\\n     * Relational rules (e.g., even numbers always adjacent to odd)\\n     * Mathematical operations (e.g., addition, multiplication, modulo)\\n\\n5. Test Pattern Hypotheses:\\n   - For each potential rule or pattern:\\n     * Check if it consistently applies across all matrices\\n     * Note any exceptions or special cases\\n   - Consider if multiple rules might be working together\\n\\n6. Refine and Generalize the Pattern:\\n   - Formulate a general description of the pattern(s) that applies to all matrices\\n   - Ensure the description accounts for all observed variations\\n\\n7. Describe the Pattern in Detail:\\n   - Provide a clear, step-by-step explanation of the pattern\\n   - Use precise language to describe number relationships, positions, or transformations\\n   - Include any conditions or exceptions to the main rule\\n\\n8. Verify the Description:\\n   - Mentally apply your pattern description to the matrices. Does it accurately describe all of them?\\n   - Consider if your description would work for hypothetical new matrices following the same logic\\n\\n9. Summarize the Pattern:\\n   - Offer a concise yet comprehensive summary of the underlying logic\\n   - Highlight the key aspects that define the puzzle's pattern\",\n","   'role': 'system'},\n","  {'content': \"You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet. \\nYour task is to analyze this task in details.\\nIn your analysis follow these steps:\\n1. Initial Observation:\\n   - How many matrices are in the set?\\n   - What is the general structure of each matrix (dimensions, range of numbers)?\\n   - What numbers are present across all matrices?\\n\\n2. Analyze Individual Matrices:\\n   - For each matrix, note:\\n     * The arrangement of numbers\\n     * The frequency of each number\\n     * Any patterns or structures formed by the numbers\\n\\n3. Compare Matrices:\\n   - Look for similarities and differences between matrices:\\n     * Are certain number combinations more common?\\n     * Do specific patterns of numbers appear in multiple matrices?\\n     * Are there consistent relationships between numbers across matrices?\\n\\n4. Identify Potential Rules or Patterns:\\n   - Consider various types of logical rules that might apply:\\n     * Number replacement rules (e.g., 1 always becomes 2)\\n     * Positional rules (e.g., corners always share a value)\\n     * Quantity rules (e.g., count of each number)\\n     * Shape or pattern rules (e.g., numbers forming specific shapes)\\n     * Relational rules (e.g., even numbers always adjacent to odd)\\n     * Mathematical operations (e.g., addition, multiplication, modulo)\\n\\n5. Test Pattern Hypotheses:\\n   - For each potential rule or pattern:\\n     * Check if it consistently applies across all matrices\\n     * Note any exceptions or special cases\\n   - Consider if multiple rules might be working together\\n\\n6. Refine and Generalize the Pattern:\\n   - Formulate a general description of the pattern(s) that applies to all matrices\\n   - Ensure the description accounts for all observed variations\\n\\n7. Describe the Pattern in Detail:\\n   - Provide a clear, step-by-step explanation of the pattern\\n   - Use precise language to describe number relationships, positions, or transformations\\n   - Include any conditions or exceptions to the main rule\\n\\n8. Verify the Description:\\n   - Mentally apply your pattern description to the matrices. Does it accurately describe all of them?\\n   - Consider if your description would work for hypothetical new matrices following the same logic\\n\\n9. Summarize the Pattern:\\n   - Offer a concise yet comprehensive summary of the underlying logic\\n   - Highlight the key aspects that define the puzzle's pattern\",\n","   'role': 'user'}]}"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["dataset['train'][0]['messages']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def filter_long_entries(dataset):\n","    def check_length(example):\n","        messages = example['messages']\n","        inputs = models['processor'](messages=messages, return_tensors=\"pt\", padding=True)\n","        pixel_values = inputs.pixel_values\n","        return pixel_values.numel() <= 16000\n","\n","    filtered_dataset = dataset.filter(check_length)\n","    return filtered_dataset\n","\n","# Apply the filter to all splits in the dataset\n","filtered_dataset = DatasetDict({\n","    split: filter_long_entries(dataset[split])\n","    for split in dataset.keys()\n","})\n","\n","print(\"Original dataset sizes:\")\n","for split, ds in dataset.items():\n","    print(f\"{split}: {len(ds)}\")\n","\n","print(\"\\nFiltered dataset sizes:\")\n","for split, ds in filtered_dataset.items():\n","    print(f\"{split}: {len(ds)}\")\n","\n","# Update the dataset variable with the filtered version\n","dataset = filtered_dataset\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:46:59.281974Z","iopub.status.busy":"2024-10-19T21:46:59.281555Z","iopub.status.idle":"2024-10-19T21:46:59.290075Z","shell.execute_reply":"2024-10-19T21:46:59.289169Z","shell.execute_reply.started":"2024-10-19T21:46:59.281923Z"},"trusted":true},"outputs":[],"source":["def eval(f):\n","    def wrapper(model, *args, **kwargs):\n","        if hasattr(model, \"to_inference\"):\n","            model.to_inference()\n","        else:\n","            model.eval()\n","        with torch.no_grad():\n","            return f(model, *args, **kwargs)\n","\n","    return wrapper\n","\n","\n","def train(f):\n","    def wrapper(model, *args, **kwargs):\n","        if hasattr(model, \"to_training\"):\n","            model.to_training()\n","        else:\n","            model.train()\n","        return f(model, *args, **kwargs)\n","\n","    return wrapper\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:46:59.291581Z","iopub.status.busy":"2024-10-19T21:46:59.291226Z","iopub.status.idle":"2024-10-19T21:46:59.302408Z","shell.execute_reply":"2024-10-19T21:46:59.301619Z","shell.execute_reply.started":"2024-10-19T21:46:59.291545Z"},"trusted":true},"outputs":[],"source":["@eval\n","def describe_puzzle(model, processor, image, prompt):\n","    # Create prompt\n","    messages = [\n","        {\n","            \"role\": \"user\",\n","            \"content\": [\n","                {\"type\": \"image\"},\n","                {\"type\": \"text\", \"text\": prompt},\n","            ],\n","        },\n","    ]\n","\n","    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","    inputs = processor(text=[text], images=[image], return_tensors=\"pt\")\n","    inputs = inputs.to(model.device)\n","\n","    # Run inference\n","    generated_ids = model.generate(**inputs, max_new_tokens=128)\n","    generated_ids = generated_ids[0, inputs.input_ids.shape[1] :]\n","    generated_text = processor.decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n","    return generated_text"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:46:59.303880Z","iopub.status.busy":"2024-10-19T21:46:59.303553Z","iopub.status.idle":"2024-10-19T21:46:59.310947Z","shell.execute_reply":"2024-10-19T21:46:59.310085Z","shell.execute_reply.started":"2024-10-19T21:46:59.303846Z"},"trusted":true},"outputs":[],"source":["# image = list_to_image(dataset[\"train\"][10][\"challenge\"][\"train\"][0][\"input\"])\n","# image"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:46:59.312922Z","iopub.status.busy":"2024-10-19T21:46:59.312213Z","iopub.status.idle":"2024-10-19T21:46:59.319317Z","shell.execute_reply":"2024-10-19T21:46:59.318462Z","shell.execute_reply.started":"2024-10-19T21:46:59.312835Z"},"trusted":true},"outputs":[],"source":["# describe_puzzle(models['vllm'], models['processor'], image, \"Describe the image\")"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:46:59.320730Z","iopub.status.busy":"2024-10-19T21:46:59.320408Z","iopub.status.idle":"2024-10-19T21:46:59.330883Z","shell.execute_reply":"2024-10-19T21:46:59.329870Z","shell.execute_reply.started":"2024-10-19T21:46:59.320696Z"},"trusted":true},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, condition_dim, latent_dim, hidden_dim):\n","        super(Encoder, self).__init__()\n","        self.condition_dim = condition_dim\n","\n","        self.query = nn.Linear(input_dim, hidden_dim, dtype=dtype)\n","        self.key = nn.Linear(input_dim, hidden_dim, dtype=dtype)\n","        self.value = nn.Linear(input_dim, hidden_dim, dtype=dtype)\n","\n","        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4, dtype=dtype)\n","\n","        self.fc1 = nn.Linear(hidden_dim, hidden_dim, dtype=dtype)\n","\n","        self.fc_mu = nn.Linear(hidden_dim, latent_dim, dtype=dtype)  # Mean of the latent space\n","        self.fc_var = nn.Linear(hidden_dim, latent_dim, dtype=dtype)  # Variance of the latent space\n","\n","    def forward(self, x, condition):\n","        # Add the condition to the input\n","        x_cond = torch.cat([x, condition], dim=1)\n","\n","        # Apply attention\n","        attn_output, _ = self.attention(self.query(x_cond), self.key(x_cond), self.value(x_cond))\n","        h = F.relu(self.fc1(attn_output.mean(dim=1)))  # Reduce to a single representation per sample\n","\n","        # Compute the mean and variance for the latent space\n","        mu = self.fc_mu(h)\n","        log_var = self.fc_var(h)\n","\n","        return mu, log_var"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:46:59.332321Z","iopub.status.busy":"2024-10-19T21:46:59.331979Z","iopub.status.idle":"2024-10-19T21:46:59.341125Z","shell.execute_reply":"2024-10-19T21:46:59.340249Z","shell.execute_reply.started":"2024-10-19T21:46:59.332286Z"},"trusted":true},"outputs":[],"source":["def reparameterize(mu, log_var):\n","    std = torch.exp(0.5 * log_var)\n","    eps = torch.randn_like(std)\n","    return mu + eps * std"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:46:59.342635Z","iopub.status.busy":"2024-10-19T21:46:59.342269Z","iopub.status.idle":"2024-10-19T21:46:59.352531Z","shell.execute_reply":"2024-10-19T21:46:59.351423Z","shell.execute_reply.started":"2024-10-19T21:46:59.342589Z"},"trusted":true},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, latent_dim, condition_dim, output_dim, hidden_dim):\n","        super(Decoder, self).__init__()\n","        self.condition_dim = condition_dim\n","        self.fc1 = nn.Linear(latent_dim + condition_dim, hidden_dim, dtype=dtype)\n","\n","        self.query = nn.Linear(hidden_dim, hidden_dim, dtype=dtype)\n","        self.key = nn.Linear(hidden_dim, hidden_dim, dtype=dtype)\n","        self.value = nn.Linear(hidden_dim, hidden_dim, dtype=dtype)\n","\n","        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4, dtype=dtype)\n","        self.fc_output = nn.Linear(\n","            hidden_dim, output_dim * output_dim * 10, dtype=dtype\n","        )  # output is the 30x30 image with each pixel being a vector of logits\n","\n","    def forward(self, z, condition, output_len):\n","        # Combine latent variable z and condition\n","        z_cond = torch.cat([z.unsqueeze(1).repeat(1, condition.shape[1], 1), condition], dim=-1)\n","\n","        h = F.relu(self.fc1(z_cond))\n","\n","        # Apply attention to guide the generation process\n","        attn_output, _ = self.attention(self.query(h), self.key(h), self.value(h))\n","\n","        # Generate output\n","        output = torch.softmax(self.fc_output(attn_output), dim=-1)\n","\n","        return output"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:46:59.353868Z","iopub.status.busy":"2024-10-19T21:46:59.353575Z","iopub.status.idle":"2024-10-19T21:46:59.363004Z","shell.execute_reply":"2024-10-19T21:46:59.362159Z","shell.execute_reply.started":"2024-10-19T21:46:59.353824Z"},"trusted":true},"outputs":[],"source":["class CVAE(nn.Module):\n","    def __init__(self, input_dim, condition_dim, latent_dim, output_dim, hidden_dim):\n","        super(CVAE, self).__init__()\n","        self.encoder = Encoder(input_dim, condition_dim, latent_dim, hidden_dim)\n","        self.decoder = Decoder(latent_dim, condition_dim, output_dim, hidden_dim)\n","\n","    def forward(self, x, condition, output_len):\n","        # Encode\n","        mu, log_var = self.encoder(x, condition)\n","\n","        # Reparameterization trick\n","        z = reparameterize(mu, log_var)  # (B, latent_dim)\n","\n","        # Decode\n","        output = self.decoder(z, condition, output_len)  # (B, output_len, output_dim * output_dim * 10)\n","\n","        return output, mu, log_var"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:46:59.364799Z","iopub.status.busy":"2024-10-19T21:46:59.364344Z","iopub.status.idle":"2024-10-19T21:46:59.383018Z","shell.execute_reply":"2024-10-19T21:46:59.382166Z","shell.execute_reply.started":"2024-10-19T21:46:59.364754Z"},"trusted":true},"outputs":[],"source":["class ARCModel(torch.nn.Module):\n","    def __init__(self, llm_model, vllm_model):\n","        super().__init__()\n","        self.llm_model = llm_model\n","        self.vllm_model = vllm_model\n","\n","        self.text_proj = nn.Linear(3072, 2304, dtype=dtype)\n","        self.image_proj = nn.Linear(1536, 2304, dtype=dtype)\n","\n","        self.output_dim = 30\n","\n","        self.cvae = CVAE(input_dim=2304, condition_dim=2304, latent_dim=512, output_dim=self.output_dim, hidden_dim=1024)\n","\n","    def to(self, device):\n","        self.device = device\n","        self.cvae.to(device)\n","        self.text_proj.to(device)\n","        self.image_proj.to(device)\n","        return self\n","\n","    def to_inference(self):\n","        self.llm_model.eval()\n","        self.vllm_model.eval()\n","\n","    def to_training(self):\n","        self.llm_model.train()\n","        self.vllm_model.train()\n","\n","    def cvae_loss(self, recon_x, x, mu, log_var):\n","        recon_loss = F.binary_cross_entropy_with_logits(recon_x, x, reduction=\"sum\")  # TODO: try BCELoss\n","        # KL Divergence loss\n","        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n","        return recon_loss + kl_loss\n","\n","    def encode(self, text_inputs, image_inputs):\n","        with torch.no_grad():\n","            text_features = self.llm_model(**text_inputs.to(self.llm_model.device)).hidden_states[-1]  # (batch_size, seq_len, 3072)\n","            image_features = self.vllm_model(**image_inputs.to(self.vllm_model.device)).hidden_states[-1]  # (batch_size, vid_len, 3584)\n","\n","        # -- todo: cleanup\n","        text_inputs.to(\"cpu\")\n","        image_inputs.to(\"cpu\")\n","\n","        torch.cuda.empty_cache()\n","        # -- todo: cleanup\n","\n","        text_features = self.text_proj(text_features.to(self.device))\n","        image_features = self.image_proj(image_features.to(self.device))\n","\n","        features = torch.cat([text_features, image_features], dim=1)  # (batch_size, seq_len + vid_len, 2304)\n","        return features\n","\n","    def forward(self, train_inputs, test_inputs, targets=None):\n","        train_features = self.encode(text_inputs=train_inputs[\"text\"], image_inputs=train_inputs[\"image\"])  # (B, seq_len + vid_len, 2304)\n","        test_features = self.encode(text_inputs=test_inputs[\"text\"], image_inputs=test_inputs[\"image\"])  # (B, seq_len + vid_len, 2304)\n","\n","        outputs, mu, log_var = self.cvae(train_features, test_features, output_len=30)  # (B, cond_seq_len, 30)\n","        \n","        B = outputs.shape[0]\n","        outputs = outputs[:, 0, :].reshape(B, self.output_dim * self.output_dim, 10).cpu().float()\n","        labels = F.one_hot(torch.tensor(targets).reshape(B, self.output_dim * self.output_dim), num_classes=10).float()\n","\n","        if targets is not None:\n","            loss = self.cvae_loss(outputs, labels, mu, log_var)\n","            return {\"loss\": loss, \"outputs\": outputs, \"mu\": mu, \"log_var\": log_var}\n","\n","        # we will only take (B, 30, 30) for the loss calculation\n","        return {\"loss\": None, \"outputs\": outputs, \"mu\": mu, \"log_var\": log_var}\n","\n","    def from_pretrained(self, path):\n","        # self.space_model.load_state_dict(torch.load(f\"{path}/space_model.pth\"))\n","        # self.classifier.load_state_dict(torch.load(f\"{path}/classifier.pth\"))\n","        # return self\n","        ...\n","\n","    def save_pretrained(self, path):\n","        # self.base_model.save_pretrained(f\"{path}/base\")\n","        # torch.save(self.space_model.state_dict(), f\"{path}/space_model.pth\")\n","        # torch.save(self.classifier.state_dict(), f\"{path}/classifier.pth\")\n","        ..."]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:46:59.384565Z","iopub.status.busy":"2024-10-19T21:46:59.384239Z","iopub.status.idle":"2024-10-19T21:46:59.782468Z","shell.execute_reply":"2024-10-19T21:46:59.781443Z","shell.execute_reply.started":"2024-10-19T21:46:59.384527Z"},"scrolled":true,"trusted":true},"outputs":[{"data":{"text/plain":["ARCModel(\n","  (llm_model): LlamaForCausalLM(\n","    (model): LlamaModel(\n","      (embed_tokens): Embedding(128256, 3072)\n","      (layers): ModuleList(\n","        (0-27): 28 x LlamaDecoderLayer(\n","          (self_attn): LlamaSdpaAttention(\n","            (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n","            (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n","            (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n","            (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n","            (rotary_emb): LlamaRotaryEmbedding()\n","          )\n","          (mlp): LlamaMLP(\n","            (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n","            (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n","            (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n","            (act_fn): SiLU()\n","          )\n","          (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n","          (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n","        )\n","      )\n","      (norm): LlamaRMSNorm((3072,), eps=1e-05)\n","      (rotary_emb): LlamaRotaryEmbedding()\n","    )\n","    (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n","  )\n","  (vllm_model): Qwen2VLForConditionalGeneration(\n","    (visual): Qwen2VisionTransformerPretrainedModel(\n","      (patch_embed): PatchEmbed(\n","        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n","      )\n","      (rotary_pos_emb): VisionRotaryEmbedding()\n","      (blocks): ModuleList(\n","        (0-31): 32 x Qwen2VLVisionBlock(\n","          (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","          (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","          (attn): VisionSdpaAttention(\n","            (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n","            (proj): Linear(in_features=1280, out_features=1280, bias=True)\n","          )\n","          (mlp): VisionMlp(\n","            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","            (act): QuickGELUActivation()\n","            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","          )\n","        )\n","      )\n","      (merger): PatchMerger(\n","        (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (0): Linear(in_features=5120, out_features=5120, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Linear(in_features=5120, out_features=1536, bias=True)\n","        )\n","      )\n","    )\n","    (model): Qwen2VLModel(\n","      (embed_tokens): Embedding(151936, 1536)\n","      (layers): ModuleList(\n","        (0-27): 28 x Qwen2VLDecoderLayer(\n","          (self_attn): Qwen2VLSdpaAttention(\n","            (rotary_emb): Qwen2VLRotaryEmbedding()\n","            (k_proj): QuantLinear()\n","            (o_proj): QuantLinear()\n","            (q_proj): QuantLinear()\n","            (v_proj): QuantLinear()\n","          )\n","          (mlp): Qwen2MLP(\n","            (act_fn): SiLU()\n","            (down_proj): QuantLinear()\n","            (gate_proj): QuantLinear()\n","            (up_proj): QuantLinear()\n","          )\n","          (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n","          (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n","        )\n","      )\n","      (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n","      (rotary_emb): Qwen2VLRotaryEmbedding()\n","    )\n","    (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n","  )\n","  (text_proj): Linear(in_features=3072, out_features=2304, bias=True)\n","  (image_proj): Linear(in_features=1536, out_features=2304, bias=True)\n","  (cvae): CVAE(\n","    (encoder): Encoder(\n","      (query): Linear(in_features=2304, out_features=1024, bias=True)\n","      (key): Linear(in_features=2304, out_features=1024, bias=True)\n","      (value): Linear(in_features=2304, out_features=1024, bias=True)\n","      (attention): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","      )\n","      (fc1): Linear(in_features=1024, out_features=1024, bias=True)\n","      (fc_mu): Linear(in_features=1024, out_features=512, bias=True)\n","      (fc_var): Linear(in_features=1024, out_features=512, bias=True)\n","    )\n","    (decoder): Decoder(\n","      (fc1): Linear(in_features=2816, out_features=1024, bias=True)\n","      (query): Linear(in_features=1024, out_features=1024, bias=True)\n","      (key): Linear(in_features=1024, out_features=1024, bias=True)\n","      (value): Linear(in_features=1024, out_features=1024, bias=True)\n","      (attention): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","      )\n","      (fc_output): Linear(in_features=1024, out_features=9000, bias=True)\n","    )\n","  )\n",")"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["arc_model = ARCModel(models[\"llm\"], models[\"vllm\"])\n","arc_model.to(\"cuda:0\")"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:46:59.784233Z","iopub.status.busy":"2024-10-19T21:46:59.783805Z","iopub.status.idle":"2024-10-19T21:46:59.791544Z","shell.execute_reply":"2024-10-19T21:46:59.790523Z","shell.execute_reply.started":"2024-10-19T21:46:59.784186Z"},"trusted":true},"outputs":[],"source":["def pad_matrix(matrix, target_rows, target_cols, pad_value=0):\n","    # Pad existing rows to target column length\n","    padded_matrix = [row + [pad_value] * (target_cols - len(row)) for row in matrix]\n","\n","    # Add new rows if necessary\n","    while len(padded_matrix) < target_rows:\n","        padded_matrix.append([pad_value] * target_cols)\n","\n","    return padded_matrix"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:46:59.793562Z","iopub.status.busy":"2024-10-19T21:46:59.793032Z","iopub.status.idle":"2024-10-19T21:46:59.807222Z","shell.execute_reply":"2024-10-19T21:46:59.806264Z","shell.execute_reply.started":"2024-10-19T21:46:59.793514Z"},"trusted":true},"outputs":[],"source":["def collate(mode, tokenizer, processor):\n","    def convert_to_pil_image(image_dict):\n","        if isinstance(image_dict, dict) and \"bytes\" in image_dict:\n","            return Image.open(io.BytesIO(image_dict[\"bytes\"]))\n","        return image_dict\n","\n","    def prepare_inputs(text_messages, image_messages):\n","        \n","        def clean_none_values(messages):\n","            return [{k: v for k, v in message.items() if v is not None} for message in messages]\n","        \n","        image_messages = [[{**msg, 'content': clean_none_values(msg['content'])} for msg in msgs] for msgs in image_messages]\n","        \n","        text_encodings = tokenizer.apply_chat_template(\n","            text_messages,\n","            tokenize=True,\n","            add_generation_prompt=(mode not in [\"train\", \"val\"]),\n","            return_tensors=\"pt\",\n","            return_dict=True,\n","            padding=True,\n","        )\n","\n","        image_text = processor.apply_chat_template(\n","            image_messages, tokenize=False, add_generation_prompt=True\n","        )\n","        image_inputs, _ = process_vision_info(image_messages)\n","\n","        image_encodings = processor(\n","            text=image_text,\n","            images=image_inputs,\n","            padding=True,\n","            return_tensors=\"pt\",\n","        )\n","\n","        return text_encodings, image_encodings\n","\n","    def collate_fn(batch):\n","        # Separate the different components of the batch\n","        # For 'test' mode, remove the last assistant message from each entry\n","        train_text_messages = [item[\"messages\"][\"train_text_messages\"] for item in batch]\n","        train_image_messages = [item[\"messages\"][\"train_image_messages\"] for item in batch]\n","\n","        test_text_messages = [item[\"messages\"][\"test_text_messages\"] for item in batch]\n","        test_image_messages = [item[\"messages\"][\"test_image_messages\"] for item in batch]\n","\n","        # Tokenize the texts\n","        train_text_encodings, train_image_encodings = prepare_inputs(train_text_messages, train_image_messages)\n","        test_text_encodings, test_image_encodings = prepare_inputs(test_text_messages, test_image_messages)\n","\n","        # If 'solution' is present (for training/validation data)\n","        if \"solution\" in batch[0]:\n","            solutions = [pad_matrix(item[\"solution\"], target_rows=30, target_cols=30) for item in batch]\n","            return {\n","                \"train_inputs\": {\"text\": train_text_encodings, \"image\": train_image_encodings},\n","                \"test_inputs\": {\"text\": test_text_encodings, \"image\": test_image_encodings},\n","                \"targets\": solutions,\n","            }\n","        else:\n","            return {\n","                \"train_inputs\": {\"text\": train_text_encodings, \"image\": train_image_encodings},\n","                \"test_inputs\": {\"text\": test_text_encodings, \"image\": test_image_encodings},\n","            }\n","\n","    return collate_fn"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:46:59.808646Z","iopub.status.busy":"2024-10-19T21:46:59.808286Z","iopub.status.idle":"2024-10-19T21:47:00.079846Z","shell.execute_reply":"2024-10-19T21:47:00.078832Z","shell.execute_reply.started":"2024-10-19T21:46:59.808604Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["train_inputs:\n","  text:\n","    input_ids:\n","      torch.Size([1, 1023])\n","    attention_mask:\n","      torch.Size([1, 1023])\n","  image:\n","    input_ids:\n","      torch.Size([1, 3095])\n","    attention_mask:\n","      torch.Size([1, 3095])\n","    pixel_values:\n","      torch.Size([10240, 1176])\n","    image_grid_thw:\n","      torch.Size([10, 3])\n","test_inputs:\n","  text:\n","    input_ids:\n","      torch.Size([1, 1234])\n","    attention_mask:\n","      torch.Size([1, 1234])\n","  image:\n","    input_ids:\n","      torch.Size([1, 769])\n","    attention_mask:\n","      torch.Size([1, 769])\n","    pixel_values:\n","      torch.Size([1024, 1176])\n","    image_grid_thw:\n","      torch.Size([1, 3])\n","targets:\n","  List of length: 1, 30, 30\n"]}],"source":["dataloader = torch.utils.data.DataLoader(\n","    dataset[\"train\"], batch_size=1, collate_fn=collate(mode=\"train\", tokenizer=models[\"tokenizer\"], processor=models[\"processor\"])\n",")\n","\n","\n","def print_recursive(obj, indent=0):\n","    if isinstance(obj, torch.Tensor):\n","        print(\"  \" * indent + str(obj.shape))\n","    elif (\n","        isinstance(obj, dict)\n","        or isinstance(obj, transformers.tokenization_utils_base.BatchEncoding)\n","        or isinstance(obj, transformers.feature_extraction_utils.BatchFeature)\n","    ):\n","        for key, value in obj.items():\n","            print(\"  \" * indent + str(key) + \":\")\n","            print_recursive(value, indent + 1)\n","    elif isinstance(obj, list):\n","        print(\"  \" * indent + f\"List of length: {len(obj)}, {len(obj[0])}, {len(obj[0][0])}\")\n","    else:\n","        print(\"  \" * indent + str(obj))\n","\n","\n","for batch in dataloader:\n","    print_recursive(batch)\n","#     outputs = arc_model(**batch)\n","#     print('-'* 30)\n","#     print_recursive(outputs)\n","    break"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:47:00.081562Z","iopub.status.busy":"2024-10-19T21:47:00.081232Z","iopub.status.idle":"2024-10-19T21:47:00.088702Z","shell.execute_reply":"2024-10-19T21:47:00.087748Z","shell.execute_reply.started":"2024-10-19T21:47:00.081526Z"},"trusted":true},"outputs":[],"source":["def calculate_partial_match(pred, label):\n","    if not isinstance(pred, list) or not isinstance(label, list):\n","        return 0  # No match if either is not a list\n","\n","    if len(pred) != len(label):\n","        return 0  # No match if outer dimensions differ\n","\n","    total_elements = 0\n","    correct_elements = 0\n","\n","    for p_row, l_row in zip(pred, label):\n","        if not isinstance(p_row, list) or not isinstance(l_row, list) or len(p_row) != len(l_row):\n","            return 0  # No match if any row is not a list or dimensions differ\n","\n","        total_elements += len(l_row)\n","        correct_elements += sum(p == l for p, l in zip(p_row, l_row))\n","\n","    return correct_elements / total_elements if total_elements > 0 else 0"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:47:00.090186Z","iopub.status.busy":"2024-10-19T21:47:00.089831Z","iopub.status.idle":"2024-10-19T21:47:00.101595Z","shell.execute_reply":"2024-10-19T21:47:00.100714Z","shell.execute_reply.started":"2024-10-19T21:47:00.090148Z"},"trusted":true},"outputs":[],"source":["def calculate_accuracy(pred, label):\n","    return (pred == label).mean()"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:47:00.102965Z","iopub.status.busy":"2024-10-19T21:47:00.102663Z","iopub.status.idle":"2024-10-19T21:47:00.111059Z","shell.execute_reply":"2024-10-19T21:47:00.110157Z","shell.execute_reply.started":"2024-10-19T21:47:00.102932Z"},"trusted":true},"outputs":[],"source":["def compute_metrics(outputs, labels):\n","    return {\n","        \"accuracy\": calculate_accuracy(outputs, labels),\n","        \"partial_match\": np.array([calculate_partial_match(pred, label) for pred, label in zip(outputs, labels)]).mean(),\n","    }\n"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:47:00.112788Z","iopub.status.busy":"2024-10-19T21:47:00.112496Z","iopub.status.idle":"2024-10-19T21:47:00.129812Z","shell.execute_reply":"2024-10-19T21:47:00.128820Z","shell.execute_reply.started":"2024-10-19T21:47:00.112755Z"},"trusted":true},"outputs":[],"source":["@train\n","def training(model, tokenizer, processor, dataset, config):\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"])\n","    \n","    train_dataloader = torch.utils.data.DataLoader(\n","        dataset[\"train\"], batch_size=config[\"batch_size\"], collate_fn=collate(mode=\"train\", tokenizer=tokenizer, processor=processor)\n","    )\n","    \n","    val_dataloader = torch.utils.data.DataLoader(\n","        dataset[\"val\"], batch_size=config[\"batch_size\"], collate_fn=collate(mode=\"val\", tokenizer=tokenizer, processor=processor)\n","    )\n","    \n","    model.train()\n","    \n","    train_loss = 0\n","    \n","    history = {'train_loss': [], 'val_loss': [], 'accuracy': [], 'partial_match': []}\n","    # Calculate total number of training steps\n","    total_steps = len(train_dataloader) * config[\"epochs\"]\n","    \n","    print(f\"Total steps: {total_steps}\")\n","    \n","    # Create the learning rate scheduler\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=config[\"warmup_steps\"],\n","        num_training_steps=total_steps\n","    )\n","    \n","    for epoch in tqdm(range(config[\"epochs\"]), desc=\"Epochs\", total=config[\"epochs\"]):\n","        train_loss = 0\n","        steps = 0\n","        for batch in tqdm(train_dataloader, desc=\"Train Batches\", total=len(train_dataloader)):\n","            outputs = model(**batch)\n","            \n","            loss = outputs[\"loss\"] / config[\"gradient_accumulation_steps\"]\n","            loss.backward()\n","            \n","            if (steps + 1) % config[\"gradient_accumulation_steps\"] == 0:\n","                optimizer.step()\n","                scheduler.step()  # Update learning rate\n","                optimizer.zero_grad()\n","            \n","            train_loss += loss.item() * config[\"gradient_accumulation_steps\"]\n","            steps += 1\n","\n","        print(f\"Epoch {epoch + 1}, Loss: {train_loss / len(train_dataloader)}\")\n","        \n","        val_loss = 0\n","        with torch.no_grad():\n","            for batch in tqdm(val_dataloader, desc=\"Val Batches\", total=len(val_dataloader)):\n","                outputs = model(**batch)\n","                loss = outputs[\"loss\"]\n","                val_loss += loss.item()\n","                \n","                # outputs (B, 900, 10)\n","                B = outputs[\"outputs\"].shape[0]\n","                pred = outputs[\"outputs\"].reshape(B, 30, 30, 10).argmax(dim=-1).numpy() # (B, 30, 30)\n","                labels = np.array(batch[\"targets\"])\n","                \n","                metrics = compute_metrics(pred, labels)\n","                \n","                history['accuracy'].append(metrics['accuracy'])\n","                history['partial_match'].append(metrics['partial_match'])\n","                \n","            \n","        print(f\"Epoch {epoch + 1}, Val Loss: {val_loss / len(val_dataloader)}\")\n","        \n","        history['train_loss'].append(train_loss / len(train_dataloader))\n","        history['val_loss'].append(val_loss / len(val_dataloader))\n","        \n","    return history"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:47:00.135502Z","iopub.status.busy":"2024-10-19T21:47:00.135105Z","iopub.status.idle":"2024-10-19T21:47:00.140408Z","shell.execute_reply":"2024-10-19T21:47:00.139486Z","shell.execute_reply.started":"2024-10-19T21:47:00.135451Z"},"trusted":true},"outputs":[],"source":["config = {\n","    'epochs': 5,\n","    'batch_size': 1,\n","    'lr': 2e-4,\n","    'gradient_accumulation_steps': 4,\n","    'warmup_steps': 200\n","}"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T21:47:00.141790Z","iopub.status.busy":"2024-10-19T21:47:00.141504Z","iopub.status.idle":"2024-10-19T22:03:52.189435Z","shell.execute_reply":"2024-10-19T22:03:52.187819Z","shell.execute_reply.started":"2024-10-19T21:47:00.141759Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total steps: 2080\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad57a9fb31764decae9667dd528d271b","version_major":2,"version_minor":0},"text/plain":["Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b8ee173b41df4ab0a9810f3c5cf06910","version_major":2,"version_minor":0},"text/plain":["Train Batches:   0%|          | 0/416 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"]},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 14.74 GiB of which 2.96 GiB is free. Process 51723 has 11.78 GiB memory in use. Of the allocated memory 11.51 GiB is allocated by PyTorch, and 142.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43marc_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokenizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprocessor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[14], line 19\u001b[0m, in \u001b[0;36mtrain.<locals>.wrapper\u001b[0;34m(model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[30], line 34\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(model, tokenizer, processor, dataset, config)\u001b[0m\n\u001b[1;32m     32\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Batches\u001b[39m\u001b[38;5;124m\"\u001b[39m, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader)):\n\u001b[0;32m---> 34\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradient_accumulation_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     37\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[22], line 54\u001b[0m, in \u001b[0;36mARCModel.forward\u001b[0;34m(self, train_inputs, test_inputs, targets)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_inputs, test_inputs, targets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 54\u001b[0m     train_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, seq_len + vid_len, 2304)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     test_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(text_inputs\u001b[38;5;241m=\u001b[39mtest_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m], image_inputs\u001b[38;5;241m=\u001b[39mtest_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# (B, seq_len + vid_len, 2304)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     outputs, mu, log_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcvae(train_features, test_features, output_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)  \u001b[38;5;66;03m# (B, cond_seq_len, 30)\u001b[39;00m\n","Cell \u001b[0;32mIn[22], line 38\u001b[0m, in \u001b[0;36mARCModel.encode\u001b[0;34m(self, text_inputs, image_inputs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     37\u001b[0m     text_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtext_inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_model\u001b[38;5;241m.\u001b[39mdevice))\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# (batch_size, seq_len, 3072)\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mimage_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# (batch_size, vid_len, 3584)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# -- todo: cleanup\u001b[39;00m\n\u001b[1;32m     41\u001b[0m text_inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:1691\u001b[0m, in \u001b[0;36mQwen2VLForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas)\u001b[0m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1690\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39mget_dtype())\n\u001b[0;32m-> 1691\u001b[0m     image_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_thw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1692\u001b[0m     image_mask \u001b[38;5;241m=\u001b[39m (input_ids \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mimage_token_id)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand_as(inputs_embeds)\n\u001b[1;32m   1693\u001b[0m     image_embeds \u001b[38;5;241m=\u001b[39m image_embeds\u001b[38;5;241m.\u001b[39mto(inputs_embeds\u001b[38;5;241m.\u001b[39mdevice, inputs_embeds\u001b[38;5;241m.\u001b[39mdtype)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:1128\u001b[0m, in \u001b[0;36mQwen2VisionTransformerPretrainedModel.forward\u001b[0;34m(self, hidden_states, grid_thw)\u001b[0m\n\u001b[1;32m   1125\u001b[0m cu_seqlens \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(cu_seqlens, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m-> 1128\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerger(hidden_states)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:431\u001b[0m, in \u001b[0;36mQwen2VLVisionBlock.forward\u001b[0;34m(self, hidden_states, cu_seqlens, rotary_pos_emb)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, cu_seqlens, rotary_pos_emb) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 431\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrotary_pos_emb\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(hidden_states))\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:404\u001b[0m, in \u001b[0;36mVisionSdpaAttention.forward\u001b[0;34m(self, hidden_states, cu_seqlens, rotary_pos_emb)\u001b[0m\n\u001b[1;32m    402\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    403\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 404\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    406\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(seq_length, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 14.74 GiB of which 2.96 GiB is free. Process 51723 has 11.78 GiB memory in use. Of the allocated memory 11.51 GiB is allocated by PyTorch, and 142.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}],"source":["history = training(arc_model, models['tokenizer'], models['processor'], dataset, config)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["history"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8951125,"sourceId":67357,"sourceType":"competition"},{"datasetId":5754327,"sourceId":9635165,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
