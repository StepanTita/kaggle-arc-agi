{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["# ! pip install torch==2.4.1 torchvision==0.19.0\n","# ! pip install accelerate==0.34.2\n","# ! pip install transformers==4.45.1\n","# ! pip install unsloth==2024.9.post3\n","# ! pip install bitsandbytes==0.44.0\n","# ! pip install qwen-vl-utils"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T23:34:46.046013Z","iopub.status.busy":"2024-10-15T23:34:46.045601Z","iopub.status.idle":"2024-10-15T23:34:46.058629Z","shell.execute_reply":"2024-10-15T23:34:46.057514Z","shell.execute_reply.started":"2024-10-15T23:34:46.045970Z"},"trusted":true},"outputs":[],"source":["%env CUDA_VISIBLE_DEVICES=0,1\n","%env TOKENIZERS_PARALLELISM=false"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T23:34:46.291042Z","iopub.status.busy":"2024-10-15T23:34:46.290735Z","iopub.status.idle":"2024-10-15T23:34:46.295435Z","shell.execute_reply":"2024-10-15T23:34:46.294460Z","shell.execute_reply.started":"2024-10-15T23:34:46.291008Z"},"trusted":true},"outputs":[],"source":["BASE_PATH = \"/home/stepan/kaggle-arc-agi\"\n","# MODEL_ID = f\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"\n","MODEL_ID = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n","# VLLM_MODEL_ID = \"unsloth/Llama-3.2-11B-Vision-Instruct\"\n","VLLM_MODEL_ID = \"4bit/Qwen2-VL-7B-Instruct\"\n","MAX_NEW_TOKENS = 2048\n","MAX_SEQ_LENGTH = 32768 - MAX_NEW_TOKENS"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T23:34:46.515089Z","iopub.status.busy":"2024-10-15T23:34:46.514783Z","iopub.status.idle":"2024-10-15T23:34:46.519224Z","shell.execute_reply":"2024-10-15T23:34:46.518270Z","shell.execute_reply.started":"2024-10-15T23:34:46.515055Z"},"trusted":true},"outputs":[],"source":["import sys\n","\n","sys.path.append(BASE_PATH)\n","sys.path.append(f\"{BASE_PATH}/scripts\")"]},{"cell_type":"code","execution_count":188,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T23:34:46.709073Z","iopub.status.busy":"2024-10-15T23:34:46.708811Z","iopub.status.idle":"2024-10-15T23:34:57.504914Z","shell.execute_reply":"2024-10-15T23:34:57.503903Z","shell.execute_reply.started":"2024-10-15T23:34:46.709043Z"},"trusted":true},"outputs":[],"source":["import io\n","import json\n","import base64\n","from PIL import Image\n","\n","import numpy as np\n","from tqdm.auto import tqdm\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.distributions import Normal\n","\n","import transformers\n","from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig  # type: ignore\n","from transformers import MllamaForConditionalGeneration, Qwen2VLForConditionalGeneration, AutoProcessor\n","from transformers import Trainer, TrainingArguments\n","\n","from datasets import Dataset, DatasetDict  # type: ignore\n","from datasets import concatenate_datasets  # type: ignore\n","\n","from qwen_vl_utils import process_vision_info # type: ignore\n","\n","import data_utils  # type: ignore"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T23:34:57.507485Z","iopub.status.busy":"2024-10-15T23:34:57.506879Z","iopub.status.idle":"2024-10-15T23:34:57.512076Z","shell.execute_reply":"2024-10-15T23:34:57.511195Z","shell.execute_reply.started":"2024-10-15T23:34:57.507440Z"},"trusted":true},"outputs":[],"source":["dtype = torch.bfloat16"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T23:34:57.513343Z","iopub.status.busy":"2024-10-15T23:34:57.513040Z","iopub.status.idle":"2024-10-15T23:34:57.522996Z","shell.execute_reply":"2024-10-15T23:34:57.522013Z","shell.execute_reply.started":"2024-10-15T23:34:57.513310Z"},"trusted":true},"outputs":[],"source":["def get_models():\n","    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n","    llm_model = AutoModelForCausalLM.from_pretrained(\n","        MODEL_ID,\n","        torch_dtype=dtype,\n","        device_map=\"auto\",\n","        max_memory={0: \"15GiB\", \"cpu\": \"16GiB\"},\n","        attn_implementation=\"flash_attention_2\",\n","        output_hidden_states=True,\n","        return_dict_in_generate=True,\n","        quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n","    )\n","\n","    processor = AutoProcessor.from_pretrained(VLLM_MODEL_ID)\n","    vllm_model = Qwen2VLForConditionalGeneration.from_pretrained(\n","        VLLM_MODEL_ID,\n","        torch_dtype=dtype,\n","        device_map=\"auto\",\n","        max_memory={1: \"15GiB\", \"cpu\": \"16GiB\"},\n","        attn_implementation=\"flash_attention_2\",\n","        output_hidden_states=True,\n","        return_dict_in_generate=True,\n","        quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n","    )\n","\n","    return {\"llm\": llm_model, \"tokenizer\": tokenizer, \"vllm\": vllm_model, \"processor\": processor}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T23:34:57.525212Z","iopub.status.busy":"2024-10-15T23:34:57.524868Z","iopub.status.idle":"2024-10-15T23:36:30.516010Z","shell.execute_reply":"2024-10-15T23:36:30.515241Z","shell.execute_reply.started":"2024-10-15T23:34:57.525146Z"},"trusted":true},"outputs":[],"source":["models = get_models()"]},{"cell_type":"code","execution_count":161,"metadata":{},"outputs":[],"source":["TRAIN_SYSTEM_PROMPT = (\n","    \"\"\"You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.\"\"\"\n",")\n","TEST_SYSTEM_PROMPT = (\n","    \"\"\"You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.\"\"\"\n",")\n","\n","TRAIN_PROMPT = \"\"\"Here are the example input and output pairs from which you should learn the underlying rule to later predict the output for the given test input:\n","-----------------\n","{training_data}\"\"\"\n","\n","TEST_PROMPT = \"\"\"Now, solve the following puzzle based on its input grid by applying the rules you have learned from the training data.:\n","-----------------\n","{input_test_data}\n","-----------------\n","What is the output grid? Only provide the output grid in the form as in the example input and output pairs. Do not provide any additional information:\"\"\"\n","\n","TRAIN_IMAGE_PROMPT = \"Describe the images\"\n","TEST_IMAGE_PROMPT = \"Describe the image\""]},{"cell_type":"code","execution_count":194,"metadata":{},"outputs":[],"source":["def list_to_image(integer_list_2d, target_size=30):\n","    # Convert the 2D list to a NumPy array\n","    array = np.array(integer_list_2d)\n","\n","    # Get the unique values in the array\n","    unique_values = np.unique(array)\n","\n","    # Create a colormap\n","    cmap = plt.get_cmap(\"tab10\")\n","\n","    # Create a color lookup dictionary\n","    color_lookup = {value: cmap(i % 10)[:3] for i, value in enumerate(unique_values)}\n","\n","    # Create an RGB array\n","    rgb_array = np.array([[color_lookup[val] for val in row] for row in array])\n","\n","    # Convert to 8-bit color values\n","    rgb_array = (rgb_array * 255).astype(np.uint8)\n","\n","    # Create an image from the colored array\n","    image = Image.fromarray(rgb_array, mode=\"RGB\")\n","\n","    # Create a new blank image with the target size\n","    new_image = Image.new(\"RGB\", (target_size, target_size), color=(0, 0, 0))\n","\n","    # Paste the original image onto the new image\n","    new_image.paste(image, (0, 0))\n","\n","    new_image = new_image.resize((target_size * 15, target_size * 15), Image.NEAREST)\n","\n","    return new_image\n","\n","def pil_image_to_base64(image):\n","    buffered = io.BytesIO()\n","    image.save(buffered, format=\"PNG\")\n","    return 'data:image;base64,' + base64.b64encode(buffered.getvalue()).decode('utf-8')\n"]},{"cell_type":"code","execution_count":195,"metadata":{},"outputs":[],"source":["def prepare_inputs(dct, prepare_solution=False):\n","    if prepare_solution:\n","        return \"<output>\\n\" + \"\\n\".join(\" \".join(map(str, row)) for row in dct) + \"\\n</output>\"\n","    else:\n","        input_str = \"\\n\".join(\" \".join(map(str, row)) for row in dct[\"input\"])\n","        output_str = \"\\n\".join(\" \".join(map(str, row)) for row in dct[\"output\"]) if \"output\" in dct else \"\"\n","        text = f\"<input>\\n{input_str}\\n</input>\"\n","        if output_str:\n","            text += f\"\\n\\n<output>\\n{output_str}\\n</output>\"\n","        return text"]},{"cell_type":"code","execution_count":196,"metadata":{},"outputs":[],"source":["def to_dataset(data, solutions=None):\n","    restructured_data = {\n","        \"id\": [],\n","        \"challenge\": [],\n","    }\n","    if solutions is not None:\n","        restructured_data[\"solution\"] = []\n","\n","    for challenge_id, challenge_data in data.items():  # for all challenges\n","        for test_id, task in enumerate(\n","            challenge_data[\"test\"]\n","        ):  # for all test tasks in this challenge we want to expand dataset so that each test task is separate dataset record\n","            restructured_data[\"id\"].append(challenge_id)\n","            restructured_data[\"challenge\"].append({\"train\": challenge_data[\"train\"], \"test\": task, \"order\": test_id})\n","            if solutions is not None:\n","                restructured_data[\"solution\"].append(solutions[challenge_id][test_id])\n","\n","    return Dataset.from_dict(restructured_data)\n","\n","\n","def prepare_inputs(dct, prepare_solution=False):\n","    if prepare_solution:\n","        return \"<output>\\n\" + \"\\n\".join(\" \".join(map(str, row)) for row in dct) + \"\\n</output>\"\n","    else:\n","        input_str = \"\\n\".join(\" \".join(map(str, row)) for row in dct[\"input\"])\n","        output_str = \"\\n\".join(\" \".join(map(str, row)) for row in dct[\"output\"]) if \"output\" in dct else \"\"\n","        text = f\"<input>\\n{input_str}\\n</input>\"\n","        if output_str:\n","            text += f\"\\n\\n<output>\\n{output_str}\\n</output>\"\n","        return text\n","\n","\n","def prepare_dataset(tokenizer, base_path=None, final_training=False):\n","    # Load all datasets\n","    training_challenges = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_training_challenges.json\")\n","    training_solutions = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_training_solutions.json\")\n","    evaluation_challenges = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_evaluation_challenges.json\")\n","    evaluation_solutions = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_evaluation_solutions.json\")\n","    test_challenges = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_test_challenges.json\")\n","\n","    train_dataset = to_dataset(training_challenges, training_solutions)\n","    eval_dataset = to_dataset(evaluation_challenges, evaluation_solutions)\n","    pred_dataset = to_dataset(test_challenges)\n","\n","    def create_chat(challenge, solution=None):\n","        train_input = TRAIN_PROMPT.format(\n","            training_data=\"\\n\\n\".join([prepare_inputs(ex) for ex in challenge[\"train\"]]),\n","        )\n","        test_input = TEST_PROMPT.format(\n","            input_test_data=prepare_inputs(challenge[\"test\"]),\n","        )\n","\n","        train_text_messages = [\n","            {\"role\": \"system\", \"content\": TRAIN_SYSTEM_PROMPT},\n","            {\"role\": \"user\", \"content\": train_input},\n","        ]\n","\n","        test_text_messages = [\n","            {\"role\": \"system\", \"content\": TEST_SYSTEM_PROMPT},\n","            {\"role\": \"user\", \"content\": test_input},\n","        ]\n","\n","        train_image_messages = [\n","            {\n","                \"role\": \"user\",\n","                \"content\": [\n","                    *[{\"type\": \"image\", \"image\": pil_image_to_base64(list_to_image(example[\"input\"]))} for example in challenge[\"train\"]],\n","                    {\"type\": \"text\", \"text\": TRAIN_IMAGE_PROMPT},\n","                ],\n","            },\n","        ]\n","\n","        test_image_messages = [\n","            {\n","                \"role\": \"user\",\n","                \"content\": [\n","                    {\"type\": \"image\", \"image\": pil_image_to_base64(list_to_image(challenge[\"test\"][\"input\"]))},\n","                    {\"type\": \"text\", \"text\": TEST_IMAGE_PROMPT},\n","                ],\n","            },\n","        ]\n","\n","        if solution:\n","            test_text_messages.append(\n","                {\n","                    \"role\": \"assistant\",\n","                    \"content\": prepare_inputs(solution, prepare_solution=True),\n","                }\n","            )\n","\n","        return {\n","            \"train_text_messages\": train_text_messages,\n","            \"test_text_messages\": test_text_messages,\n","            \"train_image_messages\": train_image_messages,\n","            \"test_image_messages\": test_image_messages,\n","        }\n","\n","    def process_dataset(examples, solutions=None):\n","        # Create messages for each challenge-solution pair\n","        chats = []\n","        for challenge, solution in zip(examples[\"challenge\"], solutions or [None] * len(examples[\"challenge\"])):\n","            chat = create_chat(challenge, solution)\n","            chats.append(chat)\n","\n","        return {\"messages\": chats}\n","\n","    pred_dataset = pred_dataset.map(lambda x: process_dataset(x), batched=True)\n","    train_dataset = train_dataset.map(lambda x: process_dataset(x, train_dataset[\"solution\"]), batched=True)\n","    eval_dataset = eval_dataset.map(lambda x: process_dataset(x, eval_dataset[\"solution\"]), batched=True)\n","\n","    if final_training:  # if final training, we need to add the validation dataset to the training dataset\n","        train_dataset = concatenate_datasets([train_dataset, eval_dataset]).shuffle(seed=42)\n","\n","        return DatasetDict(\n","            {\n","                \"train\": train_dataset,\n","                \"predict\": pred_dataset,\n","            }\n","        )\n","\n","    test_dataset = eval_dataset.train_test_split(test_size=0.3)\n","\n","    dataset = DatasetDict(\n","        {\n","            \"train\": train_dataset,\n","            \"test\": test_dataset[\"train\"],\n","            \"val\": test_dataset[\"test\"],\n","            \"predict\": pred_dataset,\n","        }\n","    )\n","\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T23:36:30.518005Z","iopub.status.busy":"2024-10-15T23:36:30.517342Z","iopub.status.idle":"2024-10-15T23:36:34.019086Z","shell.execute_reply":"2024-10-15T23:36:34.018151Z","shell.execute_reply.started":"2024-10-15T23:36:30.517955Z"},"trusted":true},"outputs":[],"source":["dataset = prepare_dataset(models[\"tokenizer\"], base_path=BASE_PATH, final_training=False)\n","dataset"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T23:36:34.030567Z","iopub.status.busy":"2024-10-15T23:36:34.030136Z","iopub.status.idle":"2024-10-15T23:36:34.038586Z","shell.execute_reply":"2024-10-15T23:36:34.037630Z","shell.execute_reply.started":"2024-10-15T23:36:34.030520Z"},"trusted":true},"outputs":[],"source":["def eval(f):\n","    def wrapper(model, *args, **kwargs):\n","        if hasattr(model, \"to_inference\"):\n","            model.to_inference()\n","        else:\n","            model.eval()\n","        with torch.no_grad():\n","            return f(model, *args, **kwargs)\n","\n","    return wrapper\n","\n","\n","def train(f):\n","    def wrapper(model, *args, **kwargs):\n","        if hasattr(model, \"to_training\"):\n","            model.to_training()\n","        else:\n","            model.train()\n","        return f(model, *args, **kwargs)\n","\n","    return wrapper\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T23:36:34.039844Z","iopub.status.busy":"2024-10-15T23:36:34.039569Z","iopub.status.idle":"2024-10-15T23:36:34.052442Z","shell.execute_reply":"2024-10-15T23:36:34.051528Z","shell.execute_reply.started":"2024-10-15T23:36:34.039813Z"},"trusted":true},"outputs":[],"source":["@eval\n","def describe_puzzle(model, processor, image, prompt):\n","    # Create prompt\n","    messages = [\n","        {\n","            \"role\": \"user\",\n","            \"content\": [\n","                {\"type\": \"image\"},\n","                {\"type\": \"text\", \"text\": prompt},\n","            ],\n","        },\n","    ]\n","\n","    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","    inputs = processor(text=[text], images=[image], return_tensors=\"pt\")\n","    inputs = inputs.to(model.device)\n","\n","    # Run inference\n","    generated_ids = model.generate(**inputs, max_new_tokens=128)\n","    generated_ids = generated_ids[0, inputs.input_ids.shape[1] :]\n","    generated_text = processor.decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n","    return generated_text"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T23:36:34.066339Z","iopub.status.busy":"2024-10-15T23:36:34.066027Z","iopub.status.idle":"2024-10-15T23:36:34.119657Z","shell.execute_reply":"2024-10-15T23:36:34.118106Z","shell.execute_reply.started":"2024-10-15T23:36:34.066306Z"},"trusted":true},"outputs":[],"source":["image = list_to_image(dataset[\"train\"][10][\"challenge\"][\"train\"][0][\"input\"])\n","image"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T23:40:58.353193Z","iopub.status.busy":"2024-10-15T23:40:58.352743Z","iopub.status.idle":"2024-10-15T23:40:58.357795Z","shell.execute_reply":"2024-10-15T23:40:58.356776Z","shell.execute_reply.started":"2024-10-15T23:40:58.353142Z"},"trusted":true},"outputs":[],"source":["# describe_puzzle(models['vllm'], models['processor'], image, \"Describe the image\")"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T23:54:43.452960Z","iopub.status.busy":"2024-10-15T23:54:43.451669Z","iopub.status.idle":"2024-10-15T23:54:43.462891Z","shell.execute_reply":"2024-10-15T23:54:43.461890Z","shell.execute_reply.started":"2024-10-15T23:54:43.452899Z"},"trusted":true},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, condition_dim, latent_dim, hidden_dim):\n","        super(Encoder, self).__init__()\n","        self.condition_dim = condition_dim\n","\n","        self.query = nn.Linear(input_dim, hidden_dim, dtype=dtype)\n","        self.key = nn.Linear(input_dim, hidden_dim, dtype=dtype)\n","        self.value = nn.Linear(input_dim, hidden_dim, dtype=dtype)\n","\n","        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4, dtype=dtype)\n","\n","        self.fc1 = nn.Linear(hidden_dim, hidden_dim, dtype=dtype)\n","\n","        self.fc_mu = nn.Linear(hidden_dim, latent_dim, dtype=dtype)  # Mean of the latent space\n","        self.fc_var = nn.Linear(hidden_dim, latent_dim, dtype=dtype)  # Variance of the latent space\n","\n","    def forward(self, x, condition):\n","        # Add the condition to the input\n","        x_cond = torch.cat([x, condition], dim=1)\n","\n","        # Apply attention\n","        attn_output, _ = self.attention(self.query(x_cond), self.key(x_cond), self.value(x_cond))\n","        h = F.relu(self.fc1(attn_output.mean(dim=1)))  # Reduce to a single representation per sample\n","\n","        # Compute the mean and variance for the latent space\n","        mu = self.fc_mu(h)\n","        log_var = self.fc_var(h)\n","\n","        return mu, log_var"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T23:54:44.101193Z","iopub.status.busy":"2024-10-15T23:54:44.099700Z","iopub.status.idle":"2024-10-15T23:54:44.106974Z","shell.execute_reply":"2024-10-15T23:54:44.105707Z","shell.execute_reply.started":"2024-10-15T23:54:44.101138Z"},"trusted":true},"outputs":[],"source":["def reparameterize(mu, log_var):\n","    std = torch.exp(0.5 * log_var)\n","    eps = torch.randn_like(std)\n","    return mu + eps * std"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T00:14:06.398682Z","iopub.status.busy":"2024-10-16T00:14:06.397833Z","iopub.status.idle":"2024-10-16T00:14:06.409375Z","shell.execute_reply":"2024-10-16T00:14:06.408272Z","shell.execute_reply.started":"2024-10-16T00:14:06.398628Z"},"trusted":true},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, latent_dim, condition_dim, output_dim, hidden_dim):\n","        super(Decoder, self).__init__()\n","        self.condition_dim = condition_dim\n","        self.fc1 = nn.Linear(latent_dim + condition_dim, hidden_dim, dtype=dtype)\n","\n","        self.query = nn.Linear(hidden_dim, hidden_dim, dtype=dtype)\n","        self.key = nn.Linear(hidden_dim, hidden_dim, dtype=dtype)\n","        self.value = nn.Linear(hidden_dim, hidden_dim, dtype=dtype)\n","\n","        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4, dtype=dtype)\n","        self.fc_output = nn.Linear(\n","            hidden_dim, output_dim * output_dim * 10, dtype=dtype\n","        )  # output is the 30x30 image with each pixel being a vector of logits\n","\n","    def forward(self, z, condition, output_len):\n","        # Combine latent variable z and condition\n","        z_cond = torch.cat([z.unsqueeze(1).repeat(1, condition.shape[1], 1), condition], dim=-1)\n","\n","        h = F.relu(self.fc1(z_cond))\n","\n","        # Apply attention to guide the generation process\n","        attn_output, _ = self.attention(self.query(h), self.key(h), self.value(h))\n","\n","        # Generate output\n","        output = torch.softmax(self.fc_output(attn_output), dim=-1)\n","\n","        return output"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T00:14:06.710683Z","iopub.status.busy":"2024-10-16T00:14:06.710040Z","iopub.status.idle":"2024-10-16T00:14:06.717345Z","shell.execute_reply":"2024-10-16T00:14:06.716302Z","shell.execute_reply.started":"2024-10-16T00:14:06.710645Z"},"trusted":true},"outputs":[],"source":["class CVAE(nn.Module):\n","    def __init__(self, input_dim, condition_dim, latent_dim, output_dim, hidden_dim):\n","        super(CVAE, self).__init__()\n","        self.encoder = Encoder(input_dim, condition_dim, latent_dim, hidden_dim)\n","        self.decoder = Decoder(latent_dim, condition_dim, output_dim, hidden_dim)\n","\n","    def forward(self, x, condition, output_len):\n","        # Encode\n","        mu, log_var = self.encoder(x, condition)\n","\n","        # Reparameterization trick\n","        z = reparameterize(mu, log_var)  # (B, latent_dim)\n","\n","        # Decode\n","        output = self.decoder(z, condition, output_len)  # (B, output_len, output_dim * output_dim * 10)\n","\n","        return output, mu, log_var"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T00:24:14.336109Z","iopub.status.busy":"2024-10-16T00:24:14.335146Z","iopub.status.idle":"2024-10-16T00:24:14.352403Z","shell.execute_reply":"2024-10-16T00:24:14.351468Z","shell.execute_reply.started":"2024-10-16T00:24:14.336066Z"},"trusted":true},"outputs":[],"source":["class ARCModel(torch.nn.Module):\n","    def __init__(self, llm_model, vllm_model):\n","        super().__init__()\n","        self.llm_model = llm_model\n","        self.vllm_model = vllm_model\n","\n","        self.text_proj = nn.Linear(3072, 2304, dtype=dtype)\n","        self.image_proj = nn.Linear(3584, 2304, dtype=dtype)\n","\n","        self.output_dim = 30\n","\n","        self.cvae = CVAE(input_dim=2304, condition_dim=2304, latent_dim=512, output_dim=self.output_dim, hidden_dim=1024)\n","\n","    def to(self, device):\n","        self.device = device\n","        self.cvae.to(device)\n","        self.text_proj.to(device)\n","        self.image_proj.to(device)\n","        return self\n","\n","    def to_inference(self):\n","        self.llm_model.eval()\n","        self.vllm_model.eval()\n","\n","    def to_training(self):\n","        self.llm_model.train()\n","        self.vllm_model.train()\n","\n","    def cvae_loss(self, recon_x, x, mu, log_var):\n","        recon_loss = F.binary_cross_entropy(recon_x, x, reduction=\"sum\")  # TODO: try BCELoss\n","        # KL Divergence loss\n","        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n","        return recon_loss + kl_loss\n","\n","    def encode(self, text_inputs, image_inputs):\n","        with torch.no_grad():\n","            text_features = self.llm_model(**text_inputs.to(self.llm_model.device)).hidden_states[-1]  # (batch_size, seq_len, 3072)\n","            image_features = self.vllm_model(**image_inputs.to(self.vllm_model.device)).hidden_states[-1]  # (batch_size, vid_len, 3584)\n","\n","        # -- todo: cleanup\n","        text_inputs.to(\"cpu\")\n","        image_inputs.to(\"cpu\")\n","\n","        torch.cuda.empty_cache()\n","        # -- todo: cleanup\n","\n","        text_features = self.text_proj(text_features.to(self.device))\n","        image_features = self.image_proj(image_features.to(self.device))\n","\n","        features = torch.cat([text_features, image_features], dim=1)  # (batch_size, seq_len + vid_len, 2304)\n","        return features\n","\n","    def forward(self, train_inputs, test_inputs, targets=None):\n","        train_features = self.encode(text_inputs=train_inputs[\"text\"], image_inputs=train_inputs[\"image\"])  # (B, seq_len + vid_len, 2304)\n","        test_features = self.encode(text_inputs=test_inputs[\"text\"], image_inputs=test_inputs[\"image\"])  # (B, seq_len + vid_len, 2304)\n","\n","        outputs, mu, log_var = self.cvae(train_features, test_features, output_len=30)  # (B, cond_seq_len, 30)\n","\n","        outputs = outputs[:, 0, :].reshape(outputs.shape[0], self.output_dim, self.output_dim, 10)\n","\n","        if targets is not None:\n","            loss = self.cvae_loss(outputs, targets, mu, log_var)\n","            return {\"loss\": loss, \"outputs\": outputs, \"mu\": mu, \"log_var\": log_var}\n","\n","        # we will only take (B, 30, 30) for the loss calculation\n","        return {\"loss\": None, \"outputs\": outputs, \"mu\": mu, \"log_var\": log_var}\n","\n","    def from_pretrained(self, path):\n","        # self.space_model.load_state_dict(torch.load(f\"{path}/space_model.pth\"))\n","        # self.classifier.load_state_dict(torch.load(f\"{path}/classifier.pth\"))\n","        # return self\n","        ...\n","\n","    def save_pretrained(self, path):\n","        # self.base_model.save_pretrained(f\"{path}/base\")\n","        # torch.save(self.space_model.state_dict(), f\"{path}/space_model.pth\")\n","        # torch.save(self.classifier.state_dict(), f\"{path}/classifier.pth\")\n","        ..."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T00:24:14.694529Z","iopub.status.busy":"2024-10-16T00:24:14.693675Z","iopub.status.idle":"2024-10-16T00:24:15.084218Z","shell.execute_reply":"2024-10-16T00:24:15.083264Z","shell.execute_reply.started":"2024-10-16T00:24:14.694491Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["arc_model = ARCModel(models[\"llm\"], models[\"vllm\"])\n","arc_model.to(\"cuda:0\")"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T00:24:15.768514Z","iopub.status.busy":"2024-10-16T00:24:15.767849Z","iopub.status.idle":"2024-10-16T00:24:15.779362Z","shell.execute_reply":"2024-10-16T00:24:15.778344Z","shell.execute_reply.started":"2024-10-16T00:24:15.768469Z"},"trusted":true},"outputs":[],"source":["arc_model.to_inference()"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T23:38:27.717666Z","iopub.status.busy":"2024-10-15T23:38:27.717051Z","iopub.status.idle":"2024-10-15T23:38:27.745960Z","shell.execute_reply":"2024-10-15T23:38:27.744849Z","shell.execute_reply.started":"2024-10-15T23:38:27.717626Z"},"trusted":true},"outputs":[],"source":["text_messages = [\n","    {\n","        \"role\": \"user\",\n","        \"content\": [\n","            {\"type\": \"image\"},\n","            {\"type\": \"text\", \"text\": \"Describe the image\"},\n","        ],\n","    },\n","]\n","\n","image_features = models[\"processor\"].apply_chat_template(text_messages, tokenize=False, add_generation_prompt=True)\n","image_features = models[\"processor\"](text=[image_features], images=[image], return_tensors=\"pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T00:24:18.815004Z","iopub.status.busy":"2024-10-16T00:24:18.814331Z","iopub.status.idle":"2024-10-16T00:24:28.160374Z","shell.execute_reply":"2024-10-16T00:24:28.159330Z","shell.execute_reply.started":"2024-10-16T00:24:18.814959Z"},"trusted":true},"outputs":[],"source":["outputs = arc_model(\n","    train_inputs={\"text\": models[\"tokenizer\"](dataset[\"train\"][0][\"texts\"], return_tensors=\"pt\"), \"image\": image_features},\n","    test_inputs={\"text\": models[\"tokenizer\"](dataset[\"train\"][0][\"texts\"], return_tensors=\"pt\"), \"image\": image_features},\n","    targets=None,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# {'messages': {'train_messages': [], 'test_message': []}, 'train_images': [], 'test_input': [], 'test_output': []}"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["def pad_matrix(matrix, target_rows, target_cols, pad_value=0):\n","    # Pad existing rows to target column length\n","    padded_matrix = [row + [pad_value] * (target_cols - len(row)) for row in matrix]\n","\n","    # Add new rows if necessary\n","    while len(padded_matrix) < target_rows:\n","        padded_matrix.append([pad_value] * target_cols)\n","\n","    return padded_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset[\"train\"][0]"]},{"cell_type":"code","execution_count":224,"metadata":{},"outputs":[],"source":["def collate(mode, tokenizer, processor):\n","    def convert_to_pil_image(image_dict):\n","        if isinstance(image_dict, dict) and \"bytes\" in image_dict:\n","            return Image.open(io.BytesIO(image_dict[\"bytes\"]))\n","        return image_dict\n","\n","    def prepare_inputs(text_messages, image_messages):\n","        \n","        def clean_none_values(messages):\n","            return [{k: v for k, v in message.items() if v is not None} for message in messages]\n","        \n","        image_messages = [[{**msg, 'content': clean_none_values(msg['content'])} for msg in msgs] for msgs in image_messages]\n","        \n","        text_encodings = tokenizer.apply_chat_template(\n","            text_messages,\n","            tokenize=True,\n","            add_generation_prompt=(mode not in [\"train\", \"val\"]),\n","            return_tensors=\"pt\",\n","            return_dict=True,\n","            padding=True,\n","        )\n","\n","        image_text = processor.apply_chat_template(\n","            image_messages, tokenize=False, add_generation_prompt=True\n","        )\n","        image_inputs, _ = process_vision_info(image_messages)\n","\n","        image_encodings = processor(\n","            text=image_text,\n","            images=image_inputs,\n","            padding=True,\n","            return_tensors=\"pt\",\n","        )\n","\n","        return text_encodings, image_encodings\n","\n","    def collate_fn(batch):\n","        # Separate the different components of the batch\n","        # For 'test' mode, remove the last assistant message from each entry\n","        train_text_messages = [item[\"messages\"][\"train_text_messages\"] for item in batch]\n","        train_image_messages = [item[\"messages\"][\"train_image_messages\"] for item in batch]\n","\n","        test_text_messages = [item[\"messages\"][\"test_text_messages\"] for item in batch]\n","        test_image_messages = [item[\"messages\"][\"test_image_messages\"] for item in batch]\n","\n","        # Tokenize the texts\n","        train_text_encodings, train_image_encodings = prepare_inputs(train_text_messages, train_image_messages)\n","        test_text_encodings, test_image_encodings = prepare_inputs(test_text_messages, test_image_messages)\n","\n","        # If 'solution' is present (for training/validation data)\n","        if \"solution\" in batch[0]:\n","            solutions = [pad_matrix(item[\"solution\"], target_rows=30, target_cols=30) for item in batch]\n","            return {\n","                \"train_inputs\": {\"text\": train_text_encodings, \"image\": train_image_encodings},\n","                \"test_inputs\": {\"text\": test_text_encodings, \"image\": test_image_encodings},\n","                \"targets\": solutions,\n","            }\n","        else:\n","            return {\n","                \"train_inputs\": {\"text\": train_text_encodings, \"image\": train_image_encodings},\n","                \"test_inputs\": {\"text\": test_text_encodings, \"image\": test_image_encodings},\n","            }\n","\n","    return collate_fn"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataloader = torch.utils.data.DataLoader(\n","    dataset[\"train\"], batch_size=8, collate_fn=collate(mode=\"train\", tokenizer=models[\"tokenizer\"], processor=models[\"processor\"])\n",")\n","\n","\n","def print_recursive(obj, indent=0):\n","    if isinstance(obj, torch.Tensor):\n","        print(\"  \" * indent + str(obj.shape))\n","    elif (\n","        isinstance(obj, dict)\n","        or isinstance(obj, transformers.tokenization_utils_base.BatchEncoding)\n","        or isinstance(obj, transformers.feature_extraction_utils.BatchFeature)\n","    ):\n","        for key, value in obj.items():\n","            print(\"  \" * indent + str(key) + \":\")\n","            print_recursive(value, indent + 1)\n","    elif isinstance(obj, list):\n","        print(\"  \" * indent + f\"List of length: {len(obj)}, {len(obj[0])}, {len(obj[0][0])}\")\n","    else:\n","        print(\"  \" * indent + str(obj))\n","\n","\n","for batch in dataloader:\n","    print_recursive(batch)\n","    break"]},{"cell_type":"code","execution_count":226,"metadata":{},"outputs":[],"source":["def compute_metrics(pred):\n","    raise ValueError(pred)\n","    return {\n","        \"accuracy\": accuracy,\n","        \"f1\": f1,\n","        \"precision\": precision,\n","        \"recall\": recall,\n","    }"]},{"cell_type":"code","execution_count":229,"metadata":{},"outputs":[],"source":["@train\n","def training(model, tokenizer, processor, dataset):\n","    model = torch.nn.DataParallel(model, device_ids=[0, 1])\n","    \n","    trainer = Trainer(\n","        model=model,\n","        tokenizer=tokenizer,\n","        train_dataset=dataset[\"train\"],\n","        eval_dataset=dataset[\"val\"],\n","        data_collator=collate(mode=\"train\", tokenizer=tokenizer, processor=processor),\n","        args=TrainingArguments(\n","            per_device_train_batch_size=2,\n","            gradient_accumulation_steps=2,\n","            eval_steps=500,\n","            warmup_steps=500,\n","            num_train_epochs=2,  # Set this for 1 full training run.\n","            # max_steps=60,\n","            evaluation_strategy=\"steps\",\n","            learning_rate=2e-5,\n","            fp16=dtype == torch.float16,\n","            bf16=dtype == torch.bfloat16,\n","            logging_steps=250,\n","            optim=\"adamw_8bit\",\n","            weight_decay=0.01,\n","            lr_scheduler_type=\"linear\",\n","            seed=3407,\n","            output_dir=\"models/gemma-2-2b\",\n","            save_strategy=\"no\",\n","            remove_unused_columns=False,\n","        ),\n","        compute_metrics=compute_metrics,\n","    )\n","    stats = trainer.train()\n","    return trainer, stats"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer, stats = training(arc_model, models[\"tokenizer\"], models[\"processor\"], dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@train\n","def training(model, dataset, optimizer, epochs):\n","    model.train()\n","    for epoch in range(epochs):\n","        total_loss = 0\n","        for json_object in dataset:\n","            optimizer.zero_grad()\n","\n","            # Prepare inputs for the CVAE\n","            train_features, test_features = prepare_inputs(json_object)\n","\n","            # Forward pass through the CVAE\n","            generated_output, mu, log_var = model(\n","                train_inputs={\"text\": ..., \"image\": ...},\n","                test_inputs={\"text\": ..., \"image\": ...},\n","            )\n","\n","            # Compute loss (reconstruction + KL divergence)\n","            test_output = extract_llm_features([json_object[\"test_example\"][\"output\"]])\n","            loss = cvae_loss(generated_output, test_output, mu, log_var)\n","\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","\n","        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataset)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# llm - 4096\n","# vllm - 3584"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8951125,"sourceId":67357,"sourceType":"competition"},{"datasetId":5754327,"sourceId":9635165,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
