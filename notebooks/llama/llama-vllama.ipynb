{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":67357,"databundleVersionId":8951125,"sourceType":"competition"},{"sourceId":9635165,"sourceType":"datasetVersion","datasetId":5754327}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ! pip install torch==2.4.1 torchvision==0.19.0\n# ! pip install accelerate==0.34.2\n# ! pip install transformers==4.45.1\n# ! pip install unsloth==2024.9.post3\n! pip install bitsandbytes==0.44.0","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%env CUDA_VISIBLE_DEVICES=0,1\n%env TOKENIZERS_PARALLELISM=false","metadata":{"execution":{"iopub.status.busy":"2024-10-15T23:34:46.045601Z","iopub.execute_input":"2024-10-15T23:34:46.046013Z","iopub.status.idle":"2024-10-15T23:34:46.058629Z","shell.execute_reply.started":"2024-10-15T23:34:46.045970Z","shell.execute_reply":"2024-10-15T23:34:46.057514Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"env: CUDA_VISIBLE_DEVICES=0,1\nenv: TOKENIZERS_PARALLELISM=false\n","output_type":"stream"}]},{"cell_type":"code","source":"BASE_PATH = \"/kaggle/input\"\n# MODEL_ID = f\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"\nMODEL_ID = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n# VLLM_MODEL_ID = \"unsloth/Llama-3.2-11B-Vision-Instruct\"\nVLLM_MODEL_ID = \"4bit/Qwen2-VL-7B-Instruct\"\nMAX_NEW_TOKENS = 2048\nMAX_SEQ_LENGTH = 32768 - MAX_NEW_TOKENS","metadata":{"execution":{"iopub.status.busy":"2024-10-15T23:34:46.290735Z","iopub.execute_input":"2024-10-15T23:34:46.291042Z","iopub.status.idle":"2024-10-15T23:34:46.295435Z","shell.execute_reply.started":"2024-10-15T23:34:46.291008Z","shell.execute_reply":"2024-10-15T23:34:46.294460Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import sys\n\n# sys.path.append(BASE_PATH)\n# sys.path.append(f\"{BASE_PATH}/scripts\")\nsys.path.append('/kaggle/input/arc-agi-python-utilities')","metadata":{"execution":{"iopub.status.busy":"2024-10-15T23:34:46.514783Z","iopub.execute_input":"2024-10-15T23:34:46.515089Z","iopub.status.idle":"2024-10-15T23:34:46.519224Z","shell.execute_reply.started":"2024-10-15T23:34:46.515055Z","shell.execute_reply":"2024-10-15T23:34:46.518270Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import json\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions import Normal\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig  # type: ignore\nfrom transformers import MllamaForConditionalGeneration, Qwen2VLForConditionalGeneration, AutoProcessor\n\nfrom tqdm.auto import tqdm  # type: ignore\n\nimport data_utils # type: ignore","metadata":{"execution":{"iopub.status.busy":"2024-10-15T23:34:46.708811Z","iopub.execute_input":"2024-10-15T23:34:46.709073Z","iopub.status.idle":"2024-10-15T23:34:57.504914Z","shell.execute_reply.started":"2024-10-15T23:34:46.709043Z","shell.execute_reply":"2024-10-15T23:34:57.503903Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"dtype = torch.bfloat16","metadata":{"execution":{"iopub.status.busy":"2024-10-15T23:34:57.506879Z","iopub.execute_input":"2024-10-15T23:34:57.507485Z","iopub.status.idle":"2024-10-15T23:34:57.512076Z","shell.execute_reply.started":"2024-10-15T23:34:57.507440Z","shell.execute_reply":"2024-10-15T23:34:57.511195Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_models():    \n    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n    llm_model = AutoModelForCausalLM.from_pretrained(\n        MODEL_ID,\n        torch_dtype=dtype,\n        device_map=\"auto\",\n        max_memory = {0: \"15GiB\", \"cpu\": \"16GiB\"},\n#         flash_attention_2=True,\n        output_hidden_states=True,\n        return_dict_in_generate=True,\n        quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n    )\n    \n    processor = AutoProcessor.from_pretrained(VLLM_MODEL_ID)\n    vllm_model = Qwen2VLForConditionalGeneration.from_pretrained(\n        VLLM_MODEL_ID,\n        torch_dtype=dtype,\n        device_map=\"auto\",\n        max_memory = {1: \"15GiB\", \"cpu\": \"16GiB\"},\n#         flash_attention_2=True,\n        output_hidden_states=True,\n        return_dict_in_generate=True,\n        quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n    )\n\n    return {\n        'llm': llm_model,\n        'tokenizer': tokenizer,\n        'vllm': vllm_model,\n        'processor': processor\n    }\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T23:34:57.513040Z","iopub.execute_input":"2024-10-15T23:34:57.513343Z","iopub.status.idle":"2024-10-15T23:34:57.522996Z","shell.execute_reply.started":"2024-10-15T23:34:57.513310Z","shell.execute_reply":"2024-10-15T23:34:57.522013Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"models = get_models()","metadata":{"execution":{"iopub.status.busy":"2024-10-15T23:34:57.524868Z","iopub.execute_input":"2024-10-15T23:34:57.525212Z","iopub.status.idle":"2024-10-15T23:36:30.516010Z","shell.execute_reply.started":"2024-10-15T23:34:57.525146Z","shell.execute_reply":"2024-10-15T23:36:30.515241Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n/opt/conda/lib/python3.10/site-packages/transformers/quantizers/auto.py:182: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\nUnrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e6cd5fb7a0c4493ac8a8969eb46f2eb"}},"metadata":{}}]},{"cell_type":"code","source":"dataset = data_utils.prepare_dataset(models['tokenizer'], fit_dataset=False, base_path=BASE_PATH, final_training=False, prepare_inputs_func=data_utils.prepare_inputs)\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-10-15T23:36:30.517342Z","iopub.execute_input":"2024-10-15T23:36:30.518005Z","iopub.status.idle":"2024-10-15T23:36:34.019086Z","shell.execute_reply.started":"2024-10-15T23:36:30.517955Z","shell.execute_reply":"2024-10-15T23:36:34.018151Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/105 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"032d592912034c6ab77c95256f112a6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/416 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32a702b144b741cb85d14c226bf49cc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/419 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb672d6d760b4c6ca2f5a92243273b99"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'challenge', 'solution', 'texts', 'messages'],\n        num_rows: 416\n    })\n    test: Dataset({\n        features: ['id', 'challenge', 'solution', 'texts', 'messages'],\n        num_rows: 293\n    })\n    val: Dataset({\n        features: ['id', 'challenge', 'solution', 'texts', 'messages'],\n        num_rows: 126\n    })\n    predict: Dataset({\n        features: ['id', 'challenge', 'texts', 'messages'],\n        num_rows: 105\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset['train'][0]['challenge']['test']['input']","metadata":{"execution":{"iopub.status.busy":"2024-10-15T23:36:34.020162Z","iopub.execute_input":"2024-10-15T23:36:34.020508Z","iopub.status.idle":"2024-10-15T23:36:34.028756Z","shell.execute_reply.started":"2024-10-15T23:36:34.020472Z","shell.execute_reply":"2024-10-15T23:36:34.027744Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[[7, 0, 7], [7, 0, 7], [7, 7, 0]]"},"metadata":{}}]},{"cell_type":"code","source":"def eval(f):\n    def wrapper(model, *args, **kwargs):\n        if hasattr(model, 'to_inference'):\n            model.to_inference()\n        else:\n            model.eval()\n        with torch.no_grad():\n            return f(model, *args, **kwargs)\n\n    return wrapper\n\n\ndef train(f):\n    def wrapper(model, *args, **kwargs):\n        if hasattr(model, 'to_training'):\n            model.to_training()\n        else:\n            model.train()\n        return f(model, *args, **kwargs)\n\n    return wrapper\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T23:36:34.030136Z","iopub.execute_input":"2024-10-15T23:36:34.030567Z","iopub.status.idle":"2024-10-15T23:36:34.038586Z","shell.execute_reply.started":"2024-10-15T23:36:34.030520Z","shell.execute_reply":"2024-10-15T23:36:34.037630Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"@eval\ndef describe_puzzle(model, processor, image, prompt):\n    # Create prompt\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\"},\n                {\"type\": \"text\", \"text\": prompt},\n            ],\n        },\n    ]\n\n    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = processor(text=[text], images=[image], return_tensors=\"pt\")\n    inputs = inputs.to(model.device)\n\n    # Run inference\n    generated_ids = model.generate(**inputs, max_new_tokens=128)\n    generated_ids = generated_ids[0, inputs.input_ids.shape[1]:]\n    generated_text = processor.decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n    return generated_text","metadata":{"execution":{"iopub.status.busy":"2024-10-15T23:36:34.039569Z","iopub.execute_input":"2024-10-15T23:36:34.039844Z","iopub.status.idle":"2024-10-15T23:36:34.052442Z","shell.execute_reply.started":"2024-10-15T23:36:34.039813Z","shell.execute_reply":"2024-10-15T23:36:34.051528Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef list_to_image(integer_list_2d, target_size=30):\n    # Convert the 2D list to a NumPy array\n    array = np.array(integer_list_2d)\n    \n    # Get the unique values in the array\n    unique_values = np.unique(array)\n    \n    # Create a colormap\n    cmap = plt.get_cmap('tab10')\n    \n    # Create a color lookup dictionary\n    color_lookup = {value: cmap(i % 10)[:3] for i, value in enumerate(unique_values)}\n    \n    # Create an RGB array\n    rgb_array = np.array([[color_lookup[val] for val in row] for row in array])\n    \n    # Convert to 8-bit color values\n    rgb_array = (rgb_array * 255).astype(np.uint8)\n    \n    # Create an image from the colored array\n    image = Image.fromarray(rgb_array, mode='RGB')\n    \n    # Create a new blank image with the target size\n    new_image = Image.new('RGB', (target_size, target_size), color=(0, 0, 0))\n    \n    # Paste the original image onto the new image\n    new_image.paste(image, (0, 0))\n    \n    new_image = new_image.resize((target_size * 15, target_size * 15), Image.NEAREST)\n    \n    return new_image","metadata":{"execution":{"iopub.status.busy":"2024-10-15T23:36:34.053616Z","iopub.execute_input":"2024-10-15T23:36:34.054033Z","iopub.status.idle":"2024-10-15T23:36:34.063349Z","shell.execute_reply.started":"2024-10-15T23:36:34.053999Z","shell.execute_reply":"2024-10-15T23:36:34.062480Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"image = list_to_image(dataset['train'][10]['challenge']['train'][0]['input'])\nimage","metadata":{"execution":{"iopub.status.busy":"2024-10-15T23:36:34.066027Z","iopub.execute_input":"2024-10-15T23:36:34.066339Z","iopub.status.idle":"2024-10-15T23:36:34.119657Z","shell.execute_reply.started":"2024-10-15T23:36:34.066306Z","shell.execute_reply":"2024-10-15T23:36:34.118106Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<PIL.Image.Image image mode=RGB size=450x450>","image/png":"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\n","image/jpeg":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"# describe_puzzle(models['vllm'], models['processor'], image, \"Describe the image\")","metadata":{"execution":{"iopub.status.busy":"2024-10-15T23:40:58.352743Z","iopub.execute_input":"2024-10-15T23:40:58.353193Z","iopub.status.idle":"2024-10-15T23:40:58.357795Z","shell.execute_reply.started":"2024-10-15T23:40:58.353142Z","shell.execute_reply":"2024-10-15T23:40:58.356776Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_dim, condition_dim, latent_dim, hidden_dim):\n        super(Encoder, self).__init__()\n        self.condition_dim = condition_dim\n        \n        self.query = nn.Linear(input_dim, hidden_dim, dtype=dtype)\n        self.key = nn.Linear(input_dim, hidden_dim, dtype=dtype)\n        self.value = nn.Linear(input_dim, hidden_dim, dtype=dtype)\n        \n        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4, dtype=dtype)\n        \n        self.fc1 = nn.Linear(hidden_dim, hidden_dim, dtype=dtype)\n\n        self.fc_mu = nn.Linear(hidden_dim, latent_dim, dtype=dtype)      # Mean of the latent space\n        self.fc_var = nn.Linear(hidden_dim, latent_dim, dtype=dtype)     # Variance of the latent space\n\n    def forward(self, x, condition):\n        # Add the condition to the input\n        x_cond = torch.cat([x, condition], dim=1)\n\n        # Apply attention\n        attn_output, _ = self.attention(self.query(x_cond), self.key(x_cond), self.value(x_cond))\n        h = F.relu(self.fc1(attn_output.mean(dim=1)))  # Reduce to a single representation per sample\n\n        # Compute the mean and variance for the latent space\n        mu = self.fc_mu(h)\n        log_var = self.fc_var(h)\n\n        return mu, log_var","metadata":{"execution":{"iopub.status.busy":"2024-10-15T23:54:43.451669Z","iopub.execute_input":"2024-10-15T23:54:43.452960Z","iopub.status.idle":"2024-10-15T23:54:43.462891Z","shell.execute_reply.started":"2024-10-15T23:54:43.452899Z","shell.execute_reply":"2024-10-15T23:54:43.461890Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"def reparameterize(mu, log_var):\n    std = torch.exp(0.5 * log_var)\n    eps = torch.randn_like(std)\n    return mu + eps * std","metadata":{"execution":{"iopub.status.busy":"2024-10-15T23:54:44.099700Z","iopub.execute_input":"2024-10-15T23:54:44.101193Z","iopub.status.idle":"2024-10-15T23:54:44.106974Z","shell.execute_reply.started":"2024-10-15T23:54:44.101138Z","shell.execute_reply":"2024-10-15T23:54:44.105707Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, latent_dim, condition_dim, output_dim, hidden_dim):\n        super(Decoder, self).__init__()\n        self.condition_dim = condition_dim\n        self.fc1 = nn.Linear(latent_dim + condition_dim, hidden_dim, dtype=dtype)\n        \n        self.query = nn.Linear(hidden_dim, hidden_dim, dtype=dtype)\n        self.key = nn.Linear(hidden_dim, hidden_dim, dtype=dtype)\n        self.value = nn.Linear(hidden_dim, hidden_dim, dtype=dtype)\n\n        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4, dtype=dtype)\n        self.fc_output = nn.Linear(hidden_dim, output_dim, dtype=dtype)\n\n    def forward(self, z, condition, output_len):\n        # Combine latent variable z and condition\n        z_cond = torch.cat([z.unsqueeze(1).repeat(1, condition.shape[1], 1), condition], dim=-1)\n\n        h = F.relu(self.fc1(z_cond))\n        \n        # Apply attention to guide the generation process\n        attn_output, _ = self.attention(self.query(h), self.key(h), self.value(h))\n\n        # Generate output\n        output = torch.sigmoid(self.fc_output(attn_output))\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-10-16T00:14:06.397833Z","iopub.execute_input":"2024-10-16T00:14:06.398682Z","iopub.status.idle":"2024-10-16T00:14:06.409375Z","shell.execute_reply.started":"2024-10-16T00:14:06.398628Z","shell.execute_reply":"2024-10-16T00:14:06.408272Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"class CVAE(nn.Module):\n    def __init__(self, input_dim, condition_dim, latent_dim, output_dim, hidden_dim):\n        super(CVAE, self).__init__()\n        self.encoder = Encoder(input_dim, condition_dim, latent_dim, hidden_dim)\n        self.decoder = Decoder(latent_dim, condition_dim, output_dim, hidden_dim)\n\n    def forward(self, x, condition, output_len):\n        # Encode\n        mu, log_var = self.encoder(x, condition)\n        \n        # Reparameterization trick\n        z = reparameterize(mu, log_var) # (B, latent_dim)\n\n        # Decode\n        output = self.decoder(z, condition, output_len)\n\n        return output, mu, log_var","metadata":{"execution":{"iopub.status.busy":"2024-10-16T00:14:06.710040Z","iopub.execute_input":"2024-10-16T00:14:06.710683Z","iopub.status.idle":"2024-10-16T00:14:06.717345Z","shell.execute_reply.started":"2024-10-16T00:14:06.710645Z","shell.execute_reply":"2024-10-16T00:14:06.716302Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"class ARCModel(torch.nn.Module):\n    def __init__(self, llm_model, vllm_model):\n        super().__init__()\n        self.llm_model = llm_model\n        self.vllm_model = vllm_model\n        \n        self.text_proj = nn.Linear(3072, 2304, dtype=dtype)\n        self.image_proj = nn.Linear(3584, 2304, dtype=dtype)\n        \n        self.output_dim = 30\n        \n        self.cvae = CVAE(input_dim=2304, condition_dim=2304, latent_dim=512, output_dim=self.output_dim, hidden_dim=1024)\n\n    def to(self, device):\n        self.device = device\n        self.cvae.to(device)\n        self.text_proj.to(device)\n        self.image_proj.to(device)\n        return self\n\n    def to_inference(self):\n        self.llm_model.eval()\n        self.vllm_model.eval()\n\n    def to_training(self):\n        self.llm_model.train()\n        self.vllm_model.train()\n        \n    def cvae_loss(self,recon_x, x, mu, log_var):\n        recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum') # TODO: try BCELoss\n        # KL Divergence loss\n        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n        return recon_loss + kl_loss\n    \n    def encode(self, text_inputs, image_inputs):\n        with torch.no_grad():\n            text_features = self.llm_model(**text_inputs.to(self.llm_model.device)).hidden_states[-1] # (batch_size, seq_len, 3072)\n            image_features = self.vllm_model(**image_inputs.to(self.vllm_model.device)).hidden_states[-1] # (batch_size, vid_len, 3584)\n        \n        # -- todo: cleanup\n        text_inputs.to('cpu')\n        image_inputs.to('cpu')\n        \n        torch.cuda.empty_cache()\n        # -- todo: cleanup\n        \n        text_features = self.text_proj(text_features.to(self.device))\n        image_features = self.image_proj(image_features.to(self.device))\n        \n        features = torch.cat([text_features, image_features], dim=1) # (batch_size, seq_len + vid_len, 2304)\n        return features\n\n    def forward(self, train_inputs, test_inputs, targets=None):\n        train_features = self.encode(text_inputs=train_inputs['text'], image_inputs=train_inputs['image']) # (B, seq_len + vid_len, 2304)\n        test_features = self.encode(text_inputs=test_inputs['text'], image_inputs=test_inputs['image']) # (B, seq_len + vid_len, 2304)\n        \n        outputs, _, _ = self.cvae(train_features, test_features, output_len=30) # (B, cond_seq_len, 30)\n        \n        # we will only take (B, 30, 30) for the loss calculation\n        return outputs[:, :self.output_dim, :]\n\n    def from_pretrained(self, path):\n        # self.space_model.load_state_dict(torch.load(f\"{path}/space_model.pth\"))\n        # self.classifier.load_state_dict(torch.load(f\"{path}/classifier.pth\"))\n        # return self\n        ...\n\n    def save_pretrained(self, path):\n        # self.base_model.save_pretrained(f\"{path}/base\")\n        # torch.save(self.space_model.state_dict(), f\"{path}/space_model.pth\")\n        # torch.save(self.classifier.state_dict(), f\"{path}/classifier.pth\")\n        ...","metadata":{"execution":{"iopub.status.busy":"2024-10-16T00:24:14.335146Z","iopub.execute_input":"2024-10-16T00:24:14.336109Z","iopub.status.idle":"2024-10-16T00:24:14.352403Z","shell.execute_reply.started":"2024-10-16T00:24:14.336066Z","shell.execute_reply":"2024-10-16T00:24:14.351468Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"arc_model = ARCModel(models['llm'], models['vllm'])\narc_model.to('cuda:0')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-16T00:24:14.693675Z","iopub.execute_input":"2024-10-16T00:24:14.694529Z","iopub.status.idle":"2024-10-16T00:24:15.084218Z","shell.execute_reply.started":"2024-10-16T00:24:14.694491Z","shell.execute_reply":"2024-10-16T00:24:15.083264Z"},"trusted":true},"execution_count":84,"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"ARCModel(\n  (llm_model): LlamaForCausalLM(\n    (model): LlamaModel(\n      (embed_tokens): Embedding(128256, 3072)\n      (layers): ModuleList(\n        (0-27): 28 x LlamaDecoderLayer(\n          (self_attn): LlamaSdpaAttention(\n            (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n            (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n            (rotary_emb): LlamaRotaryEmbedding()\n          )\n          (mlp): LlamaMLP(\n            (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n            (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n            (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n            (act_fn): SiLU()\n          )\n          (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n          (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n        )\n      )\n      (norm): LlamaRMSNorm((3072,), eps=1e-05)\n      (rotary_emb): LlamaRotaryEmbedding()\n    )\n    (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n  )\n  (vllm_model): Qwen2VLForConditionalGeneration(\n    (visual): Qwen2VisionTransformerPretrainedModel(\n      (patch_embed): PatchEmbed(\n        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n      )\n      (rotary_pos_emb): VisionRotaryEmbedding()\n      (blocks): ModuleList(\n        (0-31): 32 x Qwen2VLVisionBlock(\n          (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (attn): VisionSdpaAttention(\n            (qkv): Linear4bit(in_features=1280, out_features=3840, bias=True)\n            (proj): Linear4bit(in_features=1280, out_features=1280, bias=True)\n          )\n          (mlp): VisionMlp(\n            (fc1): Linear4bit(in_features=1280, out_features=5120, bias=True)\n            (act): QuickGELUActivation()\n            (fc2): Linear4bit(in_features=5120, out_features=1280, bias=True)\n          )\n        )\n      )\n      (merger): PatchMerger(\n        (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        (mlp): Sequential(\n          (0): Linear4bit(in_features=5120, out_features=5120, bias=True)\n          (1): GELU(approximate='none')\n          (2): Linear4bit(in_features=5120, out_features=3584, bias=True)\n        )\n      )\n    )\n    (model): Qwen2VLModel(\n      (embed_tokens): Embedding(152064, 3584)\n      (layers): ModuleList(\n        (0-27): 28 x Qwen2VLDecoderLayer(\n          (self_attn): Qwen2VLSdpaAttention(\n            (q_proj): Linear4bit(in_features=3584, out_features=3584, bias=True)\n            (k_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n            (v_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n            (o_proj): Linear4bit(in_features=3584, out_features=3584, bias=False)\n            (rotary_emb): Qwen2VLRotaryEmbedding()\n          )\n          (mlp): Qwen2MLP(\n            (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n            (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n            (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n            (act_fn): SiLU()\n          )\n          (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n          (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n        )\n      )\n      (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n      (rotary_emb): Qwen2VLRotaryEmbedding()\n    )\n    (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n  )\n  (text_proj): Linear(in_features=3072, out_features=2304, bias=True)\n  (image_proj): Linear(in_features=3584, out_features=2304, bias=True)\n  (cvae): CVAE(\n    (encoder): Encoder(\n      (query): Linear(in_features=2304, out_features=1024, bias=True)\n      (key): Linear(in_features=2304, out_features=1024, bias=True)\n      (value): Linear(in_features=2304, out_features=1024, bias=True)\n      (attention): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n      )\n      (fc1): Linear(in_features=1024, out_features=1024, bias=True)\n      (fc_mu): Linear(in_features=1024, out_features=512, bias=True)\n      (fc_var): Linear(in_features=1024, out_features=512, bias=True)\n    )\n    (decoder): Decoder(\n      (fc1): Linear(in_features=2816, out_features=1024, bias=True)\n      (query): Linear(in_features=1024, out_features=1024, bias=True)\n      (key): Linear(in_features=1024, out_features=1024, bias=True)\n      (value): Linear(in_features=1024, out_features=1024, bias=True)\n      (attention): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n      )\n      (fc_output): Linear(in_features=1024, out_features=30, bias=True)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"arc_model.to_inference()","metadata":{"execution":{"iopub.status.busy":"2024-10-16T00:24:15.767849Z","iopub.execute_input":"2024-10-16T00:24:15.768514Z","iopub.status.idle":"2024-10-16T00:24:15.779362Z","shell.execute_reply.started":"2024-10-16T00:24:15.768469Z","shell.execute_reply":"2024-10-16T00:24:15.778344Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\"},\n                {\"type\": \"text\", \"text\": \"Describe the image\"},\n            ],\n        },\n    ]\n\nimage_features = models['processor'].apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nimage_features = models['processor'](text=[image_features], images=[image], return_tensors=\"pt\")","metadata":{"execution":{"iopub.status.busy":"2024-10-15T23:38:27.717051Z","iopub.execute_input":"2024-10-15T23:38:27.717666Z","iopub.status.idle":"2024-10-15T23:38:27.745960Z","shell.execute_reply.started":"2024-10-15T23:38:27.717626Z","shell.execute_reply":"2024-10-15T23:38:27.744849Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"outputs = arc_model(\n    train_inputs={\n    'text': models['tokenizer'](dataset['train'][0]['texts'], return_tensors='pt'),\n    'image': image_features\n}, \ntest_inputs={\n    'text': models['tokenizer'](dataset['train'][0]['texts'], return_tensors='pt'),\n    'image': image_features\n},\ntargets=None)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T00:24:18.814331Z","iopub.execute_input":"2024-10-16T00:24:18.815004Z","iopub.status.idle":"2024-10-16T00:24:28.160374Z","shell.execute_reply.started":"2024-10-16T00:24:18.814959Z","shell.execute_reply":"2024-10-16T00:24:28.159330Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"def train_cvae(model, dataset, optimizer, epochs):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for json_object in dataset:\n            optimizer.zero_grad()\n            \n            # Prepare inputs for the CVAE\n            train_features, test_features = prepare_inputs(json_object)\n            \n            # Forward pass through the CVAE\n            generated_output, mu, log_var = model(train_features, test_features)\n            \n            # Compute loss (reconstruction + KL divergence)\n            test_output = extract_llm_features([json_object['test_example']['output']])\n            loss = cvae_loss(generated_output, test_output, mu, log_var)\n            \n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        \n        print(f'Epoch {epoch + 1}, Loss: {total_loss / len(dataset)}')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# llm - 4096\n# vllm - 3584","metadata":{},"execution_count":null,"outputs":[]}]}