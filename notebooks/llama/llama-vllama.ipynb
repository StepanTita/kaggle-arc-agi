{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["# ! pip install torch==2.4.1 torchvision==0.19.0\n","# ! pip install accelerate==0.34.2\n","# ! pip install transformers==4.45.1\n","# ! pip install unsloth==2024.9.post3\n","! pip install bitsandbytes==0.44.0\n","! pip install qwen-vl-utils"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%env CUDA_VISIBLE_DEVICES=0,1\n","%env TOKENIZERS_PARALLELISM=false"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["BASE_PATH = \"/kaggle/input\"\n","# MODEL_ID = f\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"\n","MODEL_ID = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n","# VLLM_MODEL_ID = \"unsloth/Llama-3.2-11B-Vision-Instruct\"\n","VLLM_MODEL_ID = \"4bit/Qwen2-VL-7B-Instruct\"\n","MAX_NEW_TOKENS = 2048\n","MAX_SEQ_LENGTH = 32768 - MAX_NEW_TOKENS"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import sys\n","\n","# sys.path.append(BASE_PATH)\n","# sys.path.append(f\"{BASE_PATH}/scripts\")\n","sys.path.append('/kaggle/input/arc-agi-python-utilities')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import io\n","import json\n","import base64\n","from PIL import Image\n","\n","import numpy as np\n","from tqdm.auto import tqdm\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.distributions import Normal\n","\n","import transformers\n","from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig  # type: ignore\n","from transformers import MllamaForConditionalGeneration, Qwen2VLForConditionalGeneration, AutoProcessor\n","from transformers import get_linear_schedule_with_warmup\n","\n","from datasets import Dataset, DatasetDict  # type: ignore\n","from datasets import concatenate_datasets  # type: ignore\n","\n","from qwen_vl_utils import process_vision_info # type: ignore\n","\n","import data_utils  # type: ignore"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dtype = torch.bfloat16"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_models():\n","    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n","    llm_model = AutoModelForCausalLM.from_pretrained(\n","        MODEL_ID,\n","        torch_dtype=dtype,\n","        device_map=\"auto\",\n","        max_memory={0: \"15GiB\", \"cpu\": \"16GiB\"},\n","        attn_implementation=\"sdpa\",\n","        output_hidden_states=True,\n","        return_dict_in_generate=True,\n","        quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n","    )\n","\n","    processor = AutoProcessor.from_pretrained(VLLM_MODEL_ID)\n","    vllm_model = Qwen2VLForConditionalGeneration.from_pretrained(\n","        VLLM_MODEL_ID,\n","        torch_dtype=dtype,\n","        device_map=\"auto\",\n","        max_memory={1: \"15GiB\", \"cpu\": \"16GiB\"},\n","        attn_implementation=\"sdpa\",\n","        output_hidden_states=True,\n","        return_dict_in_generate=True,\n","        quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n","    )\n","\n","    return {\"llm\": llm_model, \"tokenizer\": tokenizer, \"vllm\": vllm_model, \"processor\": processor}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["models = get_models()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["TRAIN_IMAGE_SYSTEM_PROMPT = (\n","    \"\"\"You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet. \n","Your task is to analyze this task in details.\n","In your analysis follow these steps:\n","1. Initial Observation:\n","- How many images are in the set?\n","- What is the general structure of each image (grid size, number of colored squares)?\n","- What colors are present across all images?\n","\n","2. Analyze Individual Images:\n","For each image, note:\n","- The arrangement of colored squares\n","- The frequency of each color\n","- Any patterns or structures formed by the colored squares\n","\n","3. Compare Images:\n","Look for similarities and differences between images:\n","- Are certain color combinations more common?\n","- Do specific patterns of colored squares appear in multiple images?\n","- Are there consistent relationships between colors across images?\n","\n","4. Identify Potential Rules or Patterns:\n","- Consider various types of logical rules that might apply:\n","- Color replacement rules (e.g., blue always becomes red)\n","- Positional rules (e.g., corners always share a color)\n","- Quantity rules (e.g., number of squares of each color)\n","- Shape or pattern rules (e.g., colors forming specific shapes)\n","- Relational rules (e.g., red squares always adjacent to blue)\n","\n","5. Test Pattern Hypotheses:\n","For each potential rule or pattern:\n","- Check if it consistently applies across all images\n","- Note any exceptions or special cases\n","- Consider if multiple rules might be working together\n","\n","6. Refine and Generalize the Pattern:\n","- Formulate a general description of the pattern(s) that applies to all images\n","- Ensure the description accounts for all observed variations\n","\n","7. Describe the Pattern in Detail:\n","- Provide a clear, step-by-step explanation of the pattern\n","- Use precise language to describe color relationships, positions, or transformations\n","- Include any conditions or exceptions to the main rule\n","\n","8. Verify the Description:\n","- Mentally apply your pattern description to the images. Does it accurately describe all of them?\n","- Consider if your description would work for hypothetical new images following the same logic\n","\n","9. Summarize the Pattern:\n","- Offer a concise yet comprehensive summary of the underlying logic\n","- Highlight the key aspects that define the puzzle's pattern\"\"\"\n",")\n","TEST_IMAGE_SYSTEM_PROMPT = (\n","    \"\"\"You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.\n","Your task is to analyze this task in details. Follow these steps:\n","1. Initial Observation:\n","   - What is the size of the grid (e.g., 3x3, 5x5)?\n","   - How many colored squares are present?\n","   - What colors are used in this image?\n","\n","2. Color Distribution:\n","   - Count the number of squares for each color\n","   - Note if any colors are dominant or rare\n","\n","3. Spatial Analysis:\n","   - Describe the position of each colored square:\n","     * Use precise coordinates (e.g., top-left is (0,0))\n","     * Note any patterns in color placement (e.g., diagonal, clustered)\n","\n","4. Edge and Corner Analysis:\n","   - Describe the colors present on the edges of the grid\n","   - Note the colors in each corner\n","   - Identify any patterns specific to edges or corners\n","\n","5. Symmetry and Balance:\n","   - Is the image symmetrical? If so, in what way? (horizontal, vertical, rotational)\n","   - Is there a balance in color distribution across the grid?\n","\n","6. Patterns and Structures:\n","   - Identify any repeating patterns of colors\n","   - Note any shapes or structures formed by same-colored squares\n","   - Describe any linear sequences or progressions of colors\n","\n","7. Color Relationships:\n","   - Analyze how different colors relate to each other:\n","     * Are certain colors always adjacent?\n","     * Do some colors appear in specific arrangements relative to others?\n","\n","8. Unique Features:\n","   - Highlight any standout characteristics that seem unusual or significant\n","   - Note any squares that appear to break an otherwise consistent pattern\n","\n","9. Quantitative Aspects:\n","   - Calculate ratios or percentages of different colors\n","   - Note any numerical patterns in color distribution\n","\n","10. Comparative Elements:\n","    - If there are elements that might be compared (e.g., left vs right, top vs bottom), describe these comparisons\n","\n","11. Abstraction:\n","    - Try to describe the image in more abstract terms, as if explaining a rule\n","    - Consider how this image might be part of a larger pattern or rule set\n","\n","12. Summary:\n","    - Provide a concise yet comprehensive summary of the image's key features\n","    - Highlight the most notable or potentially significant aspects of the image\"\"\"\n",")\n","\n","TRAIN_TEXT_SYSTEM_PROMPT = \"\"\"1. Initial Observation:\n","   - How many matrices are in the set?\n","   - What is the general structure of each matrix (dimensions, range of numbers)?\n","   - What numbers are present across all matrices?\n","\n","2. Analyze Individual Matrices:\n","   - For each matrix, note:\n","     * The arrangement of numbers\n","     * The frequency of each number\n","     * Any patterns or structures formed by the numbers\n","\n","3. Compare Matrices:\n","   - Look for similarities and differences between matrices:\n","     * Are certain number combinations more common?\n","     * Do specific patterns of numbers appear in multiple matrices?\n","     * Are there consistent relationships between numbers across matrices?\n","\n","4. Identify Potential Rules or Patterns:\n","   - Consider various types of logical rules that might apply:\n","     * Number replacement rules (e.g., 1 always becomes 2)\n","     * Positional rules (e.g., corners always share a value)\n","     * Quantity rules (e.g., count of each number)\n","     * Shape or pattern rules (e.g., numbers forming specific shapes)\n","     * Relational rules (e.g., even numbers always adjacent to odd)\n","     * Mathematical operations (e.g., addition, multiplication, modulo)\n","\n","5. Test Pattern Hypotheses:\n","   - For each potential rule or pattern:\n","     * Check if it consistently applies across all matrices\n","     * Note any exceptions or special cases\n","   - Consider if multiple rules might be working together\n","\n","6. Refine and Generalize the Pattern:\n","   - Formulate a general description of the pattern(s) that applies to all matrices\n","   - Ensure the description accounts for all observed variations\n","\n","7. Describe the Pattern in Detail:\n","   - Provide a clear, step-by-step explanation of the pattern\n","   - Use precise language to describe number relationships, positions, or transformations\n","   - Include any conditions or exceptions to the main rule\n","\n","8. Verify the Description:\n","   - Mentally apply your pattern description to the matrices. Does it accurately describe all of them?\n","   - Consider if your description would work for hypothetical new matrices following the same logic\n","\n","9. Summarize the Pattern:\n","   - Offer a concise yet comprehensive summary of the underlying logic\n","   - Highlight the key aspects that define the puzzle's pattern\n","Here are the example input and output pairs from which you should learn the underlying rule to later predict the output for the given test input:\n","-----------------\n","{training_data}\"\"\"\n","\n","TEST_TEXT_SYSTEM_PROMPT = \"\"\"1. Initial Observation:\n","   - What are the dimensions of the matrix (e.g., 3x3, 5x5)?\n","   - What is the range of numbers present?\n","   - Are there any immediately noticeable patterns or repetitions?\n","\n","2. Number Distribution:\n","   - Count the frequency of each number\n","   - Note if any numbers are particularly common or rare\n","\n","3. Spatial Analysis:\n","   - Describe the position of key numbers:\n","     * Use precise coordinates (e.g., top-left is (0,0))\n","     * Note any patterns in number placement (e.g., diagonal, clustered)\n","\n","4. Edge and Corner Analysis:\n","   - Describe the numbers present on the edges of the matrix\n","   - Note the numbers in each corner\n","   - Identify any patterns specific to edges or corners\n","\n","5. Symmetry and Balance:\n","   - Is the matrix symmetrical? If so, in what way? (horizontal, vertical, rotational)\n","   - Is there a balance in number distribution across the matrix?\n","\n","6. Patterns and Structures:\n","   - Identify any repeating patterns of numbers\n","   - Note any shapes or structures formed by identical or related numbers\n","   - Describe any sequences or progressions of numbers (e.g., arithmetic, geometric)\n","\n","7. Number Relationships:\n","   - Analyze how different numbers relate to each other:\n","     * Are certain numbers always adjacent?\n","     * Do some numbers appear in specific arrangements relative to others?\n","   - Look for mathematical relationships (e.g., sums, products, differences)\n","\n","8. Unique Features:\n","   - Highlight any standout characteristics that seem unusual or significant\n","   - Note any numbers that appear to break an otherwise consistent pattern\n","\n","9. Quantitative Aspects:\n","   - Calculate any relevant statistics (e.g., mean, median, mode)\n","   - Note any numerical patterns in the overall distribution\n","\n","10. Comparative Elements:\n","    - If there are elements that might be compared (e.g., rows vs columns, quadrants), describe these comparisons\n","\n","11. Abstraction:\n","    - Try to describe the matrix in more abstract terms, as if explaining a rule\n","    - Consider how this matrix might be part of a larger pattern or rule set\n","\n","12. Summary:\n","    - Provide a concise yet comprehensive summary of the matrix's key features\n","    - Highlight the most notable or potentially significant aspects of the matrix\n","Here is the input test data:\n","-----------------\n","{input_test_data}\n","-----------------\"\"\"\n","\n","TRAIN_IMAGE_PROMPT = \"Analyze the images\"\n","TEST_IMAGE_PROMPT = \"Analyze the image\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def list_to_image(integer_list_2d, target_size=30):\n","    # Convert the 2D list to a NumPy array\n","    array = np.array(integer_list_2d)\n","\n","    # Get the unique values in the array\n","    unique_values = np.unique(array)\n","\n","    # Create a colormap\n","    cmap = plt.get_cmap(\"tab10\")\n","\n","    # Create a color lookup dictionary\n","    color_lookup = {value: cmap(i % 10)[:3] for i, value in enumerate(unique_values)}\n","\n","    # Create an RGB array\n","    rgb_array = np.array([[color_lookup[val] for val in row] for row in array])\n","\n","    # Convert to 8-bit color values\n","    rgb_array = (rgb_array * 255).astype(np.uint8)\n","\n","    # Create an image from the colored array\n","    image = Image.fromarray(rgb_array, mode=\"RGB\")\n","\n","    # Create a new blank image with the target size\n","    new_image = Image.new(\"RGB\", (target_size, target_size), color=(0, 0, 0))\n","\n","    # Paste the original image onto the new image\n","    new_image.paste(image, (0, 0))\n","\n","    new_image = new_image.resize((target_size * 15, target_size * 15), Image.NEAREST)\n","\n","    return new_image\n","\n","def pil_image_to_base64(image):\n","    buffered = io.BytesIO()\n","    image.save(buffered, format=\"PNG\")\n","    return 'data:image;base64,' + base64.b64encode(buffered.getvalue()).decode('utf-8')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def prepare_inputs(dct, prepare_solution=False):\n","    if prepare_solution:\n","        return \"<output>\\n\" + \"\\n\".join(\" \".join(map(str, row)) for row in dct) + \"\\n</output>\"\n","    else:\n","        input_str = \"\\n\".join(\" \".join(map(str, row)) for row in dct[\"input\"])\n","        output_str = \"\\n\".join(\" \".join(map(str, row)) for row in dct[\"output\"]) if \"output\" in dct else \"\"\n","        text = f\"<input>\\n{input_str}\\n</input>\"\n","        if output_str:\n","            text += f\"\\n\\n<output>\\n{output_str}\\n</output>\"\n","        return text"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def to_dataset(data, solutions=None):\n","    restructured_data = {\n","        \"id\": [],\n","        \"challenge\": [],\n","    }\n","    if solutions is not None:\n","        restructured_data[\"solution\"] = []\n","\n","    for challenge_id, challenge_data in data.items():  # for all challenges\n","        for test_id, task in enumerate(\n","            challenge_data[\"test\"]\n","        ):  # for all test tasks in this challenge we want to expand dataset so that each test task is separate dataset record\n","            restructured_data[\"id\"].append(challenge_id)\n","            restructured_data[\"challenge\"].append({\"train\": challenge_data[\"train\"], \"test\": task, \"order\": test_id})\n","            if solutions is not None:\n","                restructured_data[\"solution\"].append(solutions[challenge_id][test_id])\n","\n","    return Dataset.from_dict(restructured_data)\n","\n","\n","def prepare_inputs(dct, prepare_solution=False):\n","    if prepare_solution:\n","        return \"<output>\\n\" + \"\\n\".join(\" \".join(map(str, row)) for row in dct) + \"\\n</output>\"\n","    else:\n","        input_str = \"\\n\".join(\" \".join(map(str, row)) for row in dct[\"input\"])\n","        output_str = \"\\n\".join(\" \".join(map(str, row)) for row in dct[\"output\"]) if \"output\" in dct else \"\"\n","        text = f\"<input>\\n{input_str}\\n</input>\"\n","        if output_str:\n","            text += f\"\\n\\n<output>\\n{output_str}\\n</output>\"\n","        return text\n","\n","\n","def prepare_dataset(tokenizer, base_path=None, final_training=False):\n","    # Load all datasets\n","    training_challenges = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_training_challenges.json\")\n","    training_solutions = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_training_solutions.json\")\n","    evaluation_challenges = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_evaluation_challenges.json\")\n","    evaluation_solutions = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_evaluation_solutions.json\")\n","    test_challenges = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_test_challenges.json\")\n","\n","    train_dataset = to_dataset(training_challenges, training_solutions)\n","    eval_dataset = to_dataset(evaluation_challenges, evaluation_solutions)\n","    pred_dataset = to_dataset(test_challenges)\n","\n","    def create_chat(challenge, solution=None):\n","        train_input = TRAIN_TEXT_SYSTEM_PROMPT.format(\n","            training_data=\"\\n\\n\".join([prepare_inputs(ex) for ex in challenge[\"train\"]]),\n","        )\n","        test_input = TEST_TEXT_SYSTEM_PROMPT.format(\n","            input_test_data=prepare_inputs(challenge[\"test\"]),\n","        )\n","\n","        train_text_messages = [\n","            {\"role\": \"system\", \"content\": TRAIN_TEXT_SYSTEM_PROMPT},\n","            {\"role\": \"user\", \"content\": train_input},\n","        ]\n","\n","        test_text_messages = [\n","            {\"role\": \"system\", \"content\": TEST_TEXT_SYSTEM_PROMPT},\n","            {\"role\": \"user\", \"content\": test_input},\n","        ]\n","\n","        train_image_messages = [\n","            {\n","                \"role\": \"user\",\n","                \"content\": [\n","                    {\"type\": \"text\", \"text\": TRAIN_IMAGE_SYSTEM_PROMPT}\n","                ] + [\n","                    item for example in challenge[\"train\"] for item in [\n","                        {\"type\": \"text\", \"text\": f\"Input Task {i+1}\"},\n","                        {\"type\": \"image\", \"image\": pil_image_to_base64(list_to_image(example[\"input\"]))},\n","                        {\"type\": \"text\", \"text\": f\"Output Task {i+1}\"},\n","                        {\"type\": \"image\", \"image\": pil_image_to_base64(list_to_image(example[\"output\"]))}\n","                    ] for i in range(len(challenge[\"train\"]))\n","                ] + [{\"type\": \"text\", \"text\": TRAIN_IMAGE_PROMPT}],\n","            },\n","        ]\n","\n","        test_image_messages = [\n","            {\n","                \"role\": \"user\",\n","                \"content\": [\n","                    {\"type\": \"text\", \"text\": TEST_IMAGE_SYSTEM_PROMPT},\n","                    {\"type\": \"text\", \"text\": f\"Input Test Task\"},\n","                    {\"type\": \"image\", \"image\": pil_image_to_base64(list_to_image(challenge[\"test\"][\"input\"]))},\n","                    {\"type\": \"text\", \"text\": TEST_IMAGE_PROMPT},\n","                ],\n","            },\n","        ]\n","\n","        if solution:\n","            test_text_messages.append(\n","                {\n","                    \"role\": \"assistant\",\n","                    \"content\": prepare_inputs(solution, prepare_solution=True),\n","                }\n","            )\n","\n","        return {\n","            \"train_text_messages\": train_text_messages,\n","            \"test_text_messages\": test_text_messages,\n","            \"train_image_messages\": train_image_messages,\n","            \"test_image_messages\": test_image_messages,\n","        }\n","\n","    def process_dataset(examples, solutions=None):\n","        # Create messages for each challenge-solution pair\n","        chats = []\n","        for challenge, solution in zip(examples[\"challenge\"], solutions or [None] * len(examples[\"challenge\"])):\n","            chat = create_chat(challenge, solution)\n","            chats.append(chat)\n","\n","        return {\"messages\": chats}\n","\n","    pred_dataset = pred_dataset.map(lambda x: process_dataset(x), batched=True)\n","    train_dataset = train_dataset.map(lambda x: process_dataset(x, train_dataset[\"solution\"]), batched=True)\n","    eval_dataset = eval_dataset.map(lambda x: process_dataset(x, eval_dataset[\"solution\"]), batched=True)\n","\n","    if final_training:  # if final training, we need to add the validation dataset to the training dataset\n","        train_dataset = concatenate_datasets([train_dataset, eval_dataset]).shuffle(seed=42)\n","\n","        return DatasetDict(\n","            {\n","                \"train\": train_dataset,\n","                \"predict\": pred_dataset,\n","            }\n","        )\n","\n","    test_dataset = eval_dataset.train_test_split(test_size=0.3)\n","\n","    dataset = DatasetDict(\n","        {\n","            \"train\": train_dataset,\n","            \"test\": test_dataset[\"train\"],\n","            \"val\": test_dataset[\"test\"],\n","            \"predict\": pred_dataset,\n","        }\n","    )\n","\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dataset = prepare_dataset(models[\"tokenizer\"], base_path=BASE_PATH, final_training=False)\n","dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def eval(f):\n","    def wrapper(model, *args, **kwargs):\n","        if hasattr(model, \"to_inference\"):\n","            model.to_inference()\n","        else:\n","            model.eval()\n","        with torch.no_grad():\n","            return f(model, *args, **kwargs)\n","\n","    return wrapper\n","\n","\n","def train(f):\n","    def wrapper(model, *args, **kwargs):\n","        if hasattr(model, \"to_training\"):\n","            model.to_training()\n","        else:\n","            model.train()\n","        return f(model, *args, **kwargs)\n","\n","    return wrapper\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["@eval\n","def describe_puzzle(model, processor, image, prompt):\n","    # Create prompt\n","    messages = [\n","        {\n","            \"role\": \"user\",\n","            \"content\": [\n","                {\"type\": \"image\"},\n","                {\"type\": \"text\", \"text\": prompt},\n","            ],\n","        },\n","    ]\n","\n","    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","    inputs = processor(text=[text], images=[image], return_tensors=\"pt\")\n","    inputs = inputs.to(model.device)\n","\n","    # Run inference\n","    generated_ids = model.generate(**inputs, max_new_tokens=128)\n","    generated_ids = generated_ids[0, inputs.input_ids.shape[1] :]\n","    generated_text = processor.decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n","    return generated_text"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# image = list_to_image(dataset[\"train\"][10][\"challenge\"][\"train\"][0][\"input\"])\n","# image"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# describe_puzzle(models['vllm'], models['processor'], image, \"Describe the image\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, condition_dim, latent_dim, hidden_dim):\n","        super(Encoder, self).__init__()\n","        self.condition_dim = condition_dim\n","\n","        self.query = nn.Linear(input_dim, hidden_dim, dtype=dtype)\n","        self.key = nn.Linear(input_dim, hidden_dim, dtype=dtype)\n","        self.value = nn.Linear(input_dim, hidden_dim, dtype=dtype)\n","\n","        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4, dtype=dtype)\n","\n","        self.fc1 = nn.Linear(hidden_dim, hidden_dim, dtype=dtype)\n","\n","        self.fc_mu = nn.Linear(hidden_dim, latent_dim, dtype=dtype)  # Mean of the latent space\n","        self.fc_var = nn.Linear(hidden_dim, latent_dim, dtype=dtype)  # Variance of the latent space\n","\n","    def forward(self, x, condition):\n","        # Add the condition to the input\n","        x_cond = torch.cat([x, condition], dim=1)\n","\n","        # Apply attention\n","        attn_output, _ = self.attention(self.query(x_cond), self.key(x_cond), self.value(x_cond))\n","        h = F.relu(self.fc1(attn_output.mean(dim=1)))  # Reduce to a single representation per sample\n","\n","        # Compute the mean and variance for the latent space\n","        mu = self.fc_mu(h)\n","        log_var = self.fc_var(h)\n","\n","        return mu, log_var"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def reparameterize(mu, log_var):\n","    std = torch.exp(0.5 * log_var)\n","    eps = torch.randn_like(std)\n","    return mu + eps * std"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, latent_dim, condition_dim, output_dim, hidden_dim):\n","        super(Decoder, self).__init__()\n","        self.condition_dim = condition_dim\n","        self.fc1 = nn.Linear(latent_dim + condition_dim, hidden_dim, dtype=dtype)\n","\n","        self.query = nn.Linear(hidden_dim, hidden_dim, dtype=dtype)\n","        self.key = nn.Linear(hidden_dim, hidden_dim, dtype=dtype)\n","        self.value = nn.Linear(hidden_dim, hidden_dim, dtype=dtype)\n","\n","        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4, dtype=dtype)\n","        self.fc_output = nn.Linear(\n","            hidden_dim, output_dim * output_dim * 10, dtype=dtype\n","        )  # output is the 30x30 image with each pixel being a vector of logits\n","\n","    def forward(self, z, condition, output_len):\n","        # Combine latent variable z and condition\n","        z_cond = torch.cat([z.unsqueeze(1).repeat(1, condition.shape[1], 1), condition], dim=-1)\n","\n","        h = F.relu(self.fc1(z_cond))\n","\n","        # Apply attention to guide the generation process\n","        attn_output, _ = self.attention(self.query(h), self.key(h), self.value(h))\n","\n","        # Generate output\n","        output = torch.softmax(self.fc_output(attn_output), dim=-1)\n","\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class CVAE(nn.Module):\n","    def __init__(self, input_dim, condition_dim, latent_dim, output_dim, hidden_dim):\n","        super(CVAE, self).__init__()\n","        self.encoder = Encoder(input_dim, condition_dim, latent_dim, hidden_dim)\n","        self.decoder = Decoder(latent_dim, condition_dim, output_dim, hidden_dim)\n","\n","    def forward(self, x, condition, output_len):\n","        # Encode\n","        mu, log_var = self.encoder(x, condition)\n","\n","        # Reparameterization trick\n","        z = reparameterize(mu, log_var)  # (B, latent_dim)\n","\n","        # Decode\n","        output = self.decoder(z, condition, output_len)  # (B, output_len, output_dim * output_dim * 10)\n","\n","        return output, mu, log_var"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class ARCModel(torch.nn.Module):\n","    def __init__(self, llm_model, vllm_model):\n","        super().__init__()\n","        self.llm_model = llm_model\n","        self.vllm_model = vllm_model\n","\n","        self.text_proj = nn.Linear(3072, 2304, dtype=dtype)\n","        self.image_proj = nn.Linear(3584, 2304, dtype=dtype)\n","\n","        self.output_dim = 30\n","\n","        self.cvae = CVAE(input_dim=2304, condition_dim=2304, latent_dim=512, output_dim=self.output_dim, hidden_dim=1024)\n","\n","    def to(self, device):\n","        self.device = device\n","        self.cvae.to(device)\n","        self.text_proj.to(device)\n","        self.image_proj.to(device)\n","        return self\n","\n","    def to_inference(self):\n","        self.llm_model.eval()\n","        self.vllm_model.eval()\n","\n","    def to_training(self):\n","        self.llm_model.train()\n","        self.vllm_model.train()\n","\n","    def cvae_loss(self, recon_x, x, mu, log_var):\n","        recon_loss = F.binary_cross_entropy_with_logits(recon_x, x, reduction=\"sum\")  # TODO: try BCELoss\n","        # KL Divergence loss\n","        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n","        return recon_loss + kl_loss\n","\n","    def encode(self, text_inputs, image_inputs):\n","        with torch.no_grad():\n","            text_features = self.llm_model(**text_inputs.to(self.llm_model.device)).hidden_states[-1]  # (batch_size, seq_len, 3072)\n","            image_features = self.vllm_model(**image_inputs.to(self.vllm_model.device)).hidden_states[-1]  # (batch_size, vid_len, 3584)\n","\n","        # -- todo: cleanup\n","        text_inputs.to(\"cpu\")\n","        image_inputs.to(\"cpu\")\n","\n","        torch.cuda.empty_cache()\n","        # -- todo: cleanup\n","\n","        text_features = self.text_proj(text_features.to(self.device))\n","        image_features = self.image_proj(image_features.to(self.device))\n","\n","        features = torch.cat([text_features, image_features], dim=1)  # (batch_size, seq_len + vid_len, 2304)\n","        return features\n","\n","    def forward(self, train_inputs, test_inputs, targets=None):\n","        train_features = self.encode(text_inputs=train_inputs[\"text\"], image_inputs=train_inputs[\"image\"])  # (B, seq_len + vid_len, 2304)\n","        test_features = self.encode(text_inputs=test_inputs[\"text\"], image_inputs=test_inputs[\"image\"])  # (B, seq_len + vid_len, 2304)\n","\n","        outputs, mu, log_var = self.cvae(train_features, test_features, output_len=30)  # (B, cond_seq_len, 30)\n","        \n","        B = outputs.shape[0]\n","        outputs = outputs[:, 0, :].reshape(B, self.output_dim * self.output_dim, 10).cpu().float()\n","        labels = F.one_hot(torch.tensor(targets).reshape(B, self.output_dim * self.output_dim), num_classes=10).float()\n","\n","        if targets is not None:\n","            loss = self.cvae_loss(outputs, labels, mu, log_var)\n","            return {\"loss\": loss, \"outputs\": outputs, \"mu\": mu, \"log_var\": log_var}\n","\n","        # we will only take (B, 30, 30) for the loss calculation\n","        return {\"loss\": None, \"outputs\": outputs, \"mu\": mu, \"log_var\": log_var}\n","\n","    def from_pretrained(self, path):\n","        # self.space_model.load_state_dict(torch.load(f\"{path}/space_model.pth\"))\n","        # self.classifier.load_state_dict(torch.load(f\"{path}/classifier.pth\"))\n","        # return self\n","        ...\n","\n","    def save_pretrained(self, path):\n","        # self.base_model.save_pretrained(f\"{path}/base\")\n","        # torch.save(self.space_model.state_dict(), f\"{path}/space_model.pth\")\n","        # torch.save(self.classifier.state_dict(), f\"{path}/classifier.pth\")\n","        ..."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["arc_model = ARCModel(models[\"llm\"], models[\"vllm\"])\n","arc_model.to(\"cuda:0\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def pad_matrix(matrix, target_rows, target_cols, pad_value=0):\n","    # Pad existing rows to target column length\n","    padded_matrix = [row + [pad_value] * (target_cols - len(row)) for row in matrix]\n","\n","    # Add new rows if necessary\n","    while len(padded_matrix) < target_rows:\n","        padded_matrix.append([pad_value] * target_cols)\n","\n","    return padded_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def collate(mode, tokenizer, processor):\n","    def convert_to_pil_image(image_dict):\n","        if isinstance(image_dict, dict) and \"bytes\" in image_dict:\n","            return Image.open(io.BytesIO(image_dict[\"bytes\"]))\n","        return image_dict\n","\n","    def prepare_inputs(text_messages, image_messages):\n","        \n","        def clean_none_values(messages):\n","            return [{k: v for k, v in message.items() if v is not None} for message in messages]\n","        \n","        image_messages = [[{**msg, 'content': clean_none_values(msg['content'])} for msg in msgs] for msgs in image_messages]\n","        \n","        text_encodings = tokenizer.apply_chat_template(\n","            text_messages,\n","            tokenize=True,\n","            add_generation_prompt=(mode not in [\"train\", \"val\"]),\n","            return_tensors=\"pt\",\n","            return_dict=True,\n","            padding=True,\n","        )\n","\n","        image_text = processor.apply_chat_template(\n","            image_messages, tokenize=False, add_generation_prompt=True\n","        )\n","        image_inputs, _ = process_vision_info(image_messages)\n","\n","        image_encodings = processor(\n","            text=image_text,\n","            images=image_inputs,\n","            padding=True,\n","            return_tensors=\"pt\",\n","        )\n","\n","        return text_encodings, image_encodings\n","\n","    def collate_fn(batch):\n","        # Separate the different components of the batch\n","        # For 'test' mode, remove the last assistant message from each entry\n","        train_text_messages = [item[\"messages\"][\"train_text_messages\"] for item in batch]\n","        train_image_messages = [item[\"messages\"][\"train_image_messages\"] for item in batch]\n","\n","        test_text_messages = [item[\"messages\"][\"test_text_messages\"] for item in batch]\n","        test_image_messages = [item[\"messages\"][\"test_image_messages\"] for item in batch]\n","\n","        # Tokenize the texts\n","        train_text_encodings, train_image_encodings = prepare_inputs(train_text_messages, train_image_messages)\n","        test_text_encodings, test_image_encodings = prepare_inputs(test_text_messages, test_image_messages)\n","\n","        # If 'solution' is present (for training/validation data)\n","        if \"solution\" in batch[0]:\n","            solutions = [pad_matrix(item[\"solution\"], target_rows=30, target_cols=30) for item in batch]\n","            return {\n","                \"train_inputs\": {\"text\": train_text_encodings, \"image\": train_image_encodings},\n","                \"test_inputs\": {\"text\": test_text_encodings, \"image\": test_image_encodings},\n","                \"targets\": solutions,\n","            }\n","        else:\n","            return {\n","                \"train_inputs\": {\"text\": train_text_encodings, \"image\": train_image_encodings},\n","                \"test_inputs\": {\"text\": test_text_encodings, \"image\": test_image_encodings},\n","            }\n","\n","    return collate_fn"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dataloader = torch.utils.data.DataLoader(\n","    dataset[\"train\"], batch_size=1, collate_fn=collate(mode=\"train\", tokenizer=models[\"tokenizer\"], processor=models[\"processor\"])\n",")\n","\n","\n","def print_recursive(obj, indent=0):\n","    if isinstance(obj, torch.Tensor):\n","        print(\"  \" * indent + str(obj.shape))\n","    elif (\n","        isinstance(obj, dict)\n","        or isinstance(obj, transformers.tokenization_utils_base.BatchEncoding)\n","        or isinstance(obj, transformers.feature_extraction_utils.BatchFeature)\n","    ):\n","        for key, value in obj.items():\n","            print(\"  \" * indent + str(key) + \":\")\n","            print_recursive(value, indent + 1)\n","    elif isinstance(obj, list):\n","        print(\"  \" * indent + f\"List of length: {len(obj)}, {len(obj[0])}, {len(obj[0][0])}\")\n","    else:\n","        print(\"  \" * indent + str(obj))\n","\n","\n","for batch in dataloader:\n","    print_recursive(batch)\n","#     outputs = arc_model(**batch)\n","#     print('-'* 30)\n","#     print_recursive(outputs)\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def calculate_partial_match(pred, label):\n","    if not isinstance(pred, list) or not isinstance(label, list):\n","        return 0  # No match if either is not a list\n","\n","    if len(pred) != len(label):\n","        return 0  # No match if outer dimensions differ\n","\n","    total_elements = 0\n","    correct_elements = 0\n","\n","    for p_row, l_row in zip(pred, label):\n","        if not isinstance(p_row, list) or not isinstance(l_row, list) or len(p_row) != len(l_row):\n","            return 0  # No match if any row is not a list or dimensions differ\n","\n","        total_elements += len(l_row)\n","        correct_elements += sum(p == l for p, l in zip(p_row, l_row))\n","\n","    return correct_elements / total_elements if total_elements > 0 else 0"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def calculate_accuracy(pred, label):\n","    return (pred == label).mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def compute_metrics(outputs, labels):\n","    return {\n","        \"accuracy\": calculate_accuracy(outputs, labels),\n","        \"partial_match\": np.array([calculate_partial_match(pred, label) for pred, label in zip(outputs, labels)]).mean(),\n","    }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["@train\n","def training(model, tokenizer, processor, dataset, config):\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"])\n","    \n","    train_dataloader = torch.utils.data.DataLoader(\n","        dataset[\"train\"], batch_size=config[\"batch_size\"], collate_fn=collate(mode=\"train\", tokenizer=tokenizer, processor=processor)\n","    )\n","    \n","    val_dataloader = torch.utils.data.DataLoader(\n","        dataset[\"val\"], batch_size=config[\"batch_size\"], collate_fn=collate(mode=\"val\", tokenizer=tokenizer, processor=processor)\n","    )\n","    \n","    model.train()\n","    \n","    train_loss = 0\n","    \n","    history = {'train_loss': [], 'val_loss': [], 'accuracy': [], 'partial_match': []}\n","    # Calculate total number of training steps\n","    total_steps = len(train_dataloader) * config[\"epochs\"]\n","    \n","    print(f\"Total steps: {total_steps}\")\n","    \n","    # Create the learning rate scheduler\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=config[\"warmup_steps\"],\n","        num_training_steps=total_steps\n","    )\n","    \n","    for epoch in tqdm(range(config[\"epochs\"]), desc=\"Epochs\", total=config[\"epochs\"]):\n","        train_loss = 0\n","        steps = 0\n","        for batch in tqdm(train_dataloader, desc=\"Train Batches\", total=len(train_dataloader)):\n","            outputs = model(**batch)\n","            \n","            loss = outputs[\"loss\"] / config[\"gradient_accumulation_steps\"]\n","            loss.backward()\n","            \n","            if (steps + 1) % config[\"gradient_accumulation_steps\"] == 0:\n","                optimizer.step()\n","                scheduler.step()  # Update learning rate\n","                optimizer.zero_grad()\n","            \n","            train_loss += loss.item() * config[\"gradient_accumulation_steps\"]\n","            steps += 1\n","\n","        print(f\"Epoch {epoch + 1}, Loss: {train_loss / len(train_dataloader)}\")\n","        \n","        val_loss = 0\n","        with torch.no_grad():\n","            for batch in tqdm(val_dataloader, desc=\"Val Batches\", total=len(val_dataloader)):\n","                outputs = model(**batch)\n","                loss = outputs[\"loss\"]\n","                val_loss += loss.item()\n","                \n","                # outputs (B, 900, 10)\n","                B = outputs[\"outputs\"].shape[0]\n","                pred = outputs[\"outputs\"].reshape(B, 30, 30, 10).argmax(dim=-1).numpy() # (B, 30, 30)\n","                labels = np.array(batch[\"targets\"])\n","                \n","                metrics = compute_metrics(pred, labels)\n","                \n","                history['accuracy'].append(metrics['accuracy'])\n","                history['partial_match'].append(metrics['partial_match'])\n","                \n","            \n","        print(f\"Epoch {epoch + 1}, Val Loss: {val_loss / len(val_dataloader)}\")\n","        \n","        history['train_loss'].append(train_loss / len(train_dataloader))\n","        history['val_loss'].append(val_loss / len(val_dataloader))\n","        \n","    return history"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["config = {\n","    'epochs': 1,\n","    'batch_size': 2,\n","    'lr': 2e-3,\n","    'gradient_accumulation_steps': 2,\n","    'warmup_steps': 200\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["history = training(arc_model, models['tokenizer'], models['processor'], dataset, config)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["history"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8951125,"sourceId":67357,"sourceType":"competition"},{"datasetId":5754327,"sourceId":9635165,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
