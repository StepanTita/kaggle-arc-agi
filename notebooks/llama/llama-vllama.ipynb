{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":67357,"databundleVersionId":8951125,"sourceType":"competition"},{"sourceId":9635165,"sourceType":"datasetVersion","datasetId":5754327}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ! pip install torch==2.4.1 torchvision==0.19.0\n# ! pip install accelerate==0.34.2\n# ! pip install transformers==4.45.1\n# ! pip install unsloth==2024.9.post3\n! pip install bitsandbytes==0.44.0\n! pip install qwen-vl-utils","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%env CUDA_VISIBLE_DEVICES=0,1\n%env TOKENIZERS_PARALLELISM=false","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE_PATH = \"/kaggle/input\"\n# MODEL_ID = f\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"\nMODEL_ID = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n# VLLM_MODEL_ID = \"unsloth/Llama-3.2-11B-Vision-Instruct\"\nVLLM_MODEL_ID = \"4bit/Qwen2-VL-7B-Instruct\"\nMAX_NEW_TOKENS = 2048\nMAX_SEQ_LENGTH = 32768 - MAX_NEW_TOKENS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\n\n# sys.path.append(BASE_PATH)\n# sys.path.append(f\"{BASE_PATH}/scripts\")\nsys.path.append('/kaggle/input/arc-agi-python-utilities')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import io\nimport json\nimport base64\nfrom PIL import Image\n\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions import Normal\n\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig  # type: ignore\nfrom transformers import MllamaForConditionalGeneration, Qwen2VLForConditionalGeneration, AutoProcessor\nfrom transformers import Trainer, TrainingArguments\n\nfrom datasets import Dataset, DatasetDict  # type: ignore\nfrom datasets import concatenate_datasets  # type: ignore\n\nfrom qwen_vl_utils import process_vision_info # type: ignore\n\nimport data_utils  # type: ignore","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtype = torch.bfloat16","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_models():\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n    llm_model = AutoModelForCausalLM.from_pretrained(\n        MODEL_ID,\n        torch_dtype=dtype,\n        device_map=\"auto\",\n        max_memory={0: \"15GiB\", \"cpu\": \"16GiB\"},\n        attn_implementation=\"sdpa\",\n        output_hidden_states=True,\n        return_dict_in_generate=True,\n        quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n    )\n\n    processor = AutoProcessor.from_pretrained(VLLM_MODEL_ID)\n    vllm_model = Qwen2VLForConditionalGeneration.from_pretrained(\n        VLLM_MODEL_ID,\n        torch_dtype=dtype,\n        device_map=\"auto\",\n        max_memory={1: \"15GiB\", \"cpu\": \"16GiB\"},\n        attn_implementation=\"sdpa\",\n        output_hidden_states=True,\n        return_dict_in_generate=True,\n        quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n    )\n\n    return {\"llm\": llm_model, \"tokenizer\": tokenizer, \"vllm\": vllm_model, \"processor\": processor}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = get_models()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_SYSTEM_PROMPT = (\n    \"\"\"You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.\"\"\"\n)\nTEST_SYSTEM_PROMPT = (\n    \"\"\"You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.\"\"\"\n)\n\nTRAIN_PROMPT = \"\"\"Here are the example input and output pairs from which you should learn the underlying rule to later predict the output for the given test input:\n-----------------\n{training_data}\"\"\"\n\nTEST_PROMPT = \"\"\"Now, solve the following puzzle based on its input grid by applying the rules you have learned from the training data.:\n-----------------\n{input_test_data}\n-----------------\nWhat is the output grid? Only provide the output grid in the form as in the example input and output pairs. Do not provide any additional information:\"\"\"\n\nTRAIN_IMAGE_PROMPT = \"Describe the images\"\nTEST_IMAGE_PROMPT = \"Describe the image\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def list_to_image(integer_list_2d, target_size=30):\n    # Convert the 2D list to a NumPy array\n    array = np.array(integer_list_2d)\n\n    # Get the unique values in the array\n    unique_values = np.unique(array)\n\n    # Create a colormap\n    cmap = plt.get_cmap(\"tab10\")\n\n    # Create a color lookup dictionary\n    color_lookup = {value: cmap(i % 10)[:3] for i, value in enumerate(unique_values)}\n\n    # Create an RGB array\n    rgb_array = np.array([[color_lookup[val] for val in row] for row in array])\n\n    # Convert to 8-bit color values\n    rgb_array = (rgb_array * 255).astype(np.uint8)\n\n    # Create an image from the colored array\n    image = Image.fromarray(rgb_array, mode=\"RGB\")\n\n    # Create a new blank image with the target size\n    new_image = Image.new(\"RGB\", (target_size, target_size), color=(0, 0, 0))\n\n    # Paste the original image onto the new image\n    new_image.paste(image, (0, 0))\n\n    new_image = new_image.resize((target_size * 15, target_size * 15), Image.NEAREST)\n\n    return new_image\n\ndef pil_image_to_base64(image):\n    buffered = io.BytesIO()\n    image.save(buffered, format=\"PNG\")\n    return 'data:image;base64,' + base64.b64encode(buffered.getvalue()).decode('utf-8')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_inputs(dct, prepare_solution=False):\n    if prepare_solution:\n        return \"<output>\\n\" + \"\\n\".join(\" \".join(map(str, row)) for row in dct) + \"\\n</output>\"\n    else:\n        input_str = \"\\n\".join(\" \".join(map(str, row)) for row in dct[\"input\"])\n        output_str = \"\\n\".join(\" \".join(map(str, row)) for row in dct[\"output\"]) if \"output\" in dct else \"\"\n        text = f\"<input>\\n{input_str}\\n</input>\"\n        if output_str:\n            text += f\"\\n\\n<output>\\n{output_str}\\n</output>\"\n        return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_dataset(data, solutions=None):\n    restructured_data = {\n        \"id\": [],\n        \"challenge\": [],\n    }\n    if solutions is not None:\n        restructured_data[\"solution\"] = []\n\n    for challenge_id, challenge_data in data.items():  # for all challenges\n        for test_id, task in enumerate(\n            challenge_data[\"test\"]\n        ):  # for all test tasks in this challenge we want to expand dataset so that each test task is separate dataset record\n            restructured_data[\"id\"].append(challenge_id)\n            restructured_data[\"challenge\"].append({\"train\": challenge_data[\"train\"], \"test\": task, \"order\": test_id})\n            if solutions is not None:\n                restructured_data[\"solution\"].append(solutions[challenge_id][test_id])\n\n    return Dataset.from_dict(restructured_data)\n\n\ndef prepare_inputs(dct, prepare_solution=False):\n    if prepare_solution:\n        return \"<output>\\n\" + \"\\n\".join(\" \".join(map(str, row)) for row in dct) + \"\\n</output>\"\n    else:\n        input_str = \"\\n\".join(\" \".join(map(str, row)) for row in dct[\"input\"])\n        output_str = \"\\n\".join(\" \".join(map(str, row)) for row in dct[\"output\"]) if \"output\" in dct else \"\"\n        text = f\"<input>\\n{input_str}\\n</input>\"\n        if output_str:\n            text += f\"\\n\\n<output>\\n{output_str}\\n</output>\"\n        return text\n\n\ndef prepare_dataset(tokenizer, base_path=None, final_training=False):\n    # Load all datasets\n    training_challenges = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_training_challenges.json\")\n    training_solutions = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_training_solutions.json\")\n    evaluation_challenges = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_evaluation_challenges.json\")\n    evaluation_solutions = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_evaluation_solutions.json\")\n    test_challenges = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_test_challenges.json\")\n\n    train_dataset = to_dataset(training_challenges, training_solutions)\n    eval_dataset = to_dataset(evaluation_challenges, evaluation_solutions)\n    pred_dataset = to_dataset(test_challenges)\n\n    def create_chat(challenge, solution=None):\n        train_input = TRAIN_PROMPT.format(\n            training_data=\"\\n\\n\".join([prepare_inputs(ex) for ex in challenge[\"train\"]]),\n        )\n        test_input = TEST_PROMPT.format(\n            input_test_data=prepare_inputs(challenge[\"test\"]),\n        )\n\n        train_text_messages = [\n            {\"role\": \"system\", \"content\": TRAIN_SYSTEM_PROMPT},\n            {\"role\": \"user\", \"content\": train_input},\n        ]\n\n        test_text_messages = [\n            {\"role\": \"system\", \"content\": TEST_SYSTEM_PROMPT},\n            {\"role\": \"user\", \"content\": test_input},\n        ]\n\n        train_image_messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    *[{\"type\": \"image\", \"image\": pil_image_to_base64(list_to_image(example[\"input\"]))} for example in challenge[\"train\"]],\n                    {\"type\": \"text\", \"text\": TRAIN_IMAGE_PROMPT},\n                ],\n            },\n        ]\n\n        test_image_messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image\", \"image\": pil_image_to_base64(list_to_image(challenge[\"test\"][\"input\"]))},\n                    {\"type\": \"text\", \"text\": TEST_IMAGE_PROMPT},\n                ],\n            },\n        ]\n\n        if solution:\n            test_text_messages.append(\n                {\n                    \"role\": \"assistant\",\n                    \"content\": prepare_inputs(solution, prepare_solution=True),\n                }\n            )\n\n        return {\n            \"train_text_messages\": train_text_messages,\n            \"test_text_messages\": test_text_messages,\n            \"train_image_messages\": train_image_messages,\n            \"test_image_messages\": test_image_messages,\n        }\n\n    def process_dataset(examples, solutions=None):\n        # Create messages for each challenge-solution pair\n        chats = []\n        for challenge, solution in zip(examples[\"challenge\"], solutions or [None] * len(examples[\"challenge\"])):\n            chat = create_chat(challenge, solution)\n            chats.append(chat)\n\n        return {\"messages\": chats}\n\n    pred_dataset = pred_dataset.map(lambda x: process_dataset(x), batched=True)\n    train_dataset = train_dataset.map(lambda x: process_dataset(x, train_dataset[\"solution\"]), batched=True)\n    eval_dataset = eval_dataset.map(lambda x: process_dataset(x, eval_dataset[\"solution\"]), batched=True)\n\n    if final_training:  # if final training, we need to add the validation dataset to the training dataset\n        train_dataset = concatenate_datasets([train_dataset, eval_dataset]).shuffle(seed=42)\n\n        return DatasetDict(\n            {\n                \"train\": train_dataset,\n                \"predict\": pred_dataset,\n            }\n        )\n\n    test_dataset = eval_dataset.train_test_split(test_size=0.3)\n\n    dataset = DatasetDict(\n        {\n            \"train\": train_dataset,\n            \"test\": test_dataset[\"train\"],\n            \"val\": test_dataset[\"test\"],\n            \"predict\": pred_dataset,\n        }\n    )\n\n    return dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = prepare_dataset(models[\"tokenizer\"], base_path=BASE_PATH, final_training=False)\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval(f):\n    def wrapper(model, *args, **kwargs):\n        if hasattr(model, \"to_inference\"):\n            model.to_inference()\n        else:\n            model.eval()\n        with torch.no_grad():\n            return f(model, *args, **kwargs)\n\n    return wrapper\n\n\ndef train(f):\n    def wrapper(model, *args, **kwargs):\n        if hasattr(model, \"to_training\"):\n            model.to_training()\n        else:\n            model.train()\n        return f(model, *args, **kwargs)\n\n    return wrapper\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@eval\ndef describe_puzzle(model, processor, image, prompt):\n    # Create prompt\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\"},\n                {\"type\": \"text\", \"text\": prompt},\n            ],\n        },\n    ]\n\n    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = processor(text=[text], images=[image], return_tensors=\"pt\")\n    inputs = inputs.to(model.device)\n\n    # Run inference\n    generated_ids = model.generate(**inputs, max_new_tokens=128)\n    generated_ids = generated_ids[0, inputs.input_ids.shape[1] :]\n    generated_text = processor.decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n    return generated_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image = list_to_image(dataset[\"train\"][10][\"challenge\"][\"train\"][0][\"input\"])\n# image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# describe_puzzle(models['vllm'], models['processor'], image, \"Describe the image\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_dim, condition_dim, latent_dim, hidden_dim):\n        super(Encoder, self).__init__()\n        self.condition_dim = condition_dim\n\n        self.query = nn.Linear(input_dim, hidden_dim, dtype=dtype)\n        self.key = nn.Linear(input_dim, hidden_dim, dtype=dtype)\n        self.value = nn.Linear(input_dim, hidden_dim, dtype=dtype)\n\n        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4, dtype=dtype)\n\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim, dtype=dtype)\n\n        self.fc_mu = nn.Linear(hidden_dim, latent_dim, dtype=dtype)  # Mean of the latent space\n        self.fc_var = nn.Linear(hidden_dim, latent_dim, dtype=dtype)  # Variance of the latent space\n\n    def forward(self, x, condition):\n        # Add the condition to the input\n        x_cond = torch.cat([x, condition], dim=1)\n\n        # Apply attention\n        attn_output, _ = self.attention(self.query(x_cond), self.key(x_cond), self.value(x_cond))\n        h = F.relu(self.fc1(attn_output.mean(dim=1)))  # Reduce to a single representation per sample\n\n        # Compute the mean and variance for the latent space\n        mu = self.fc_mu(h)\n        log_var = self.fc_var(h)\n\n        return mu, log_var","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reparameterize(mu, log_var):\n    std = torch.exp(0.5 * log_var)\n    eps = torch.randn_like(std)\n    return mu + eps * std","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, latent_dim, condition_dim, output_dim, hidden_dim):\n        super(Decoder, self).__init__()\n        self.condition_dim = condition_dim\n        self.fc1 = nn.Linear(latent_dim + condition_dim, hidden_dim, dtype=dtype)\n\n        self.query = nn.Linear(hidden_dim, hidden_dim, dtype=dtype)\n        self.key = nn.Linear(hidden_dim, hidden_dim, dtype=dtype)\n        self.value = nn.Linear(hidden_dim, hidden_dim, dtype=dtype)\n\n        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4, dtype=dtype)\n        self.fc_output = nn.Linear(\n            hidden_dim, output_dim * output_dim * 10, dtype=dtype\n        )  # output is the 30x30 image with each pixel being a vector of logits\n\n    def forward(self, z, condition, output_len):\n        # Combine latent variable z and condition\n        z_cond = torch.cat([z.unsqueeze(1).repeat(1, condition.shape[1], 1), condition], dim=-1)\n\n        h = F.relu(self.fc1(z_cond))\n\n        # Apply attention to guide the generation process\n        attn_output, _ = self.attention(self.query(h), self.key(h), self.value(h))\n\n        # Generate output\n        output = torch.softmax(self.fc_output(attn_output), dim=-1)\n\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CVAE(nn.Module):\n    def __init__(self, input_dim, condition_dim, latent_dim, output_dim, hidden_dim):\n        super(CVAE, self).__init__()\n        self.encoder = Encoder(input_dim, condition_dim, latent_dim, hidden_dim)\n        self.decoder = Decoder(latent_dim, condition_dim, output_dim, hidden_dim)\n\n    def forward(self, x, condition, output_len):\n        # Encode\n        mu, log_var = self.encoder(x, condition)\n\n        # Reparameterization trick\n        z = reparameterize(mu, log_var)  # (B, latent_dim)\n\n        # Decode\n        output = self.decoder(z, condition, output_len)  # (B, output_len, output_dim * output_dim * 10)\n\n        return output, mu, log_var","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ARCModel(torch.nn.Module):\n    def __init__(self, llm_model, vllm_model):\n        super().__init__()\n        self.llm_model = llm_model\n        self.vllm_model = vllm_model\n\n        self.text_proj = nn.Linear(3072, 2304, dtype=dtype)\n        self.image_proj = nn.Linear(3584, 2304, dtype=dtype)\n\n        self.output_dim = 30\n\n        self.cvae = CVAE(input_dim=2304, condition_dim=2304, latent_dim=512, output_dim=self.output_dim, hidden_dim=1024)\n\n    def to(self, device):\n        self.device = device\n        self.cvae.to(device)\n        self.text_proj.to(device)\n        self.image_proj.to(device)\n        return self\n\n    def to_inference(self):\n        self.llm_model.eval()\n        self.vllm_model.eval()\n\n    def to_training(self):\n        self.llm_model.train()\n        self.vllm_model.train()\n\n    def cvae_loss(self, recon_x, x, mu, log_var):\n        recon_loss = F.binary_cross_entropy_with_logits(recon_x, x, reduction=\"sum\")  # TODO: try BCELoss\n        # KL Divergence loss\n        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n        return recon_loss + kl_loss\n\n    def encode(self, text_inputs, image_inputs):\n        with torch.no_grad():\n            text_features = self.llm_model(**text_inputs.to(self.llm_model.device)).hidden_states[-1]  # (batch_size, seq_len, 3072)\n            image_features = self.vllm_model(**image_inputs.to(self.vllm_model.device)).hidden_states[-1]  # (batch_size, vid_len, 3584)\n\n        # -- todo: cleanup\n        text_inputs.to(\"cpu\")\n        image_inputs.to(\"cpu\")\n\n        torch.cuda.empty_cache()\n        # -- todo: cleanup\n\n        text_features = self.text_proj(text_features.to(self.device))\n        image_features = self.image_proj(image_features.to(self.device))\n\n        features = torch.cat([text_features, image_features], dim=1)  # (batch_size, seq_len + vid_len, 2304)\n        return features\n\n    def forward(self, train_inputs, test_inputs, targets=None):\n        train_features = self.encode(text_inputs=train_inputs[\"text\"], image_inputs=train_inputs[\"image\"])  # (B, seq_len + vid_len, 2304)\n        test_features = self.encode(text_inputs=test_inputs[\"text\"], image_inputs=test_inputs[\"image\"])  # (B, seq_len + vid_len, 2304)\n\n        outputs, mu, log_var = self.cvae(train_features, test_features, output_len=30)  # (B, cond_seq_len, 30)\n        \n        B = outputs.shape[0]\n        outputs = outputs[:, 0, :].reshape(B, self.output_dim * self.output_dim, 10).cpu().float()\n        labels = F.one_hot(torch.tensor(targets).reshape(B, self.output_dim * self.output_dim), num_classes=10).float()\n\n        if targets is not None:\n            loss = self.cvae_loss(outputs, labels, mu, log_var)\n            return {\"loss\": loss, \"outputs\": outputs, \"mu\": mu, \"log_var\": log_var}\n\n        # we will only take (B, 30, 30) for the loss calculation\n        return {\"loss\": None, \"outputs\": outputs, \"mu\": mu, \"log_var\": log_var}\n\n    def from_pretrained(self, path):\n        # self.space_model.load_state_dict(torch.load(f\"{path}/space_model.pth\"))\n        # self.classifier.load_state_dict(torch.load(f\"{path}/classifier.pth\"))\n        # return self\n        ...\n\n    def save_pretrained(self, path):\n        # self.base_model.save_pretrained(f\"{path}/base\")\n        # torch.save(self.space_model.state_dict(), f\"{path}/space_model.pth\")\n        # torch.save(self.classifier.state_dict(), f\"{path}/classifier.pth\")\n        ...","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arc_model = ARCModel(models[\"llm\"], models[\"vllm\"])\narc_model.to(\"cuda:0\")","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pad_matrix(matrix, target_rows, target_cols, pad_value=0):\n    # Pad existing rows to target column length\n    padded_matrix = [row + [pad_value] * (target_cols - len(row)) for row in matrix]\n\n    # Add new rows if necessary\n    while len(padded_matrix) < target_rows:\n        padded_matrix.append([pad_value] * target_cols)\n\n    return padded_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate(mode, tokenizer, processor):\n    def convert_to_pil_image(image_dict):\n        if isinstance(image_dict, dict) and \"bytes\" in image_dict:\n            return Image.open(io.BytesIO(image_dict[\"bytes\"]))\n        return image_dict\n\n    def prepare_inputs(text_messages, image_messages):\n        \n        def clean_none_values(messages):\n            return [{k: v for k, v in message.items() if v is not None} for message in messages]\n        \n        image_messages = [[{**msg, 'content': clean_none_values(msg['content'])} for msg in msgs] for msgs in image_messages]\n        \n        text_encodings = tokenizer.apply_chat_template(\n            text_messages,\n            tokenize=True,\n            add_generation_prompt=(mode not in [\"train\", \"val\"]),\n            return_tensors=\"pt\",\n            return_dict=True,\n            padding=True,\n        )\n\n        image_text = processor.apply_chat_template(\n            image_messages, tokenize=False, add_generation_prompt=True\n        )\n        image_inputs, _ = process_vision_info(image_messages)\n\n        image_encodings = processor(\n            text=image_text,\n            images=image_inputs,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n\n        return text_encodings, image_encodings\n\n    def collate_fn(batch):\n        # Separate the different components of the batch\n        # For 'test' mode, remove the last assistant message from each entry\n        train_text_messages = [item[\"messages\"][\"train_text_messages\"] for item in batch]\n        train_image_messages = [item[\"messages\"][\"train_image_messages\"] for item in batch]\n\n        test_text_messages = [item[\"messages\"][\"test_text_messages\"] for item in batch]\n        test_image_messages = [item[\"messages\"][\"test_image_messages\"] for item in batch]\n\n        # Tokenize the texts\n        train_text_encodings, train_image_encodings = prepare_inputs(train_text_messages, train_image_messages)\n        test_text_encodings, test_image_encodings = prepare_inputs(test_text_messages, test_image_messages)\n\n        # If 'solution' is present (for training/validation data)\n        if \"solution\" in batch[0]:\n            solutions = [pad_matrix(item[\"solution\"], target_rows=30, target_cols=30) for item in batch]\n            return {\n                \"train_inputs\": {\"text\": train_text_encodings, \"image\": train_image_encodings},\n                \"test_inputs\": {\"text\": test_text_encodings, \"image\": test_image_encodings},\n                \"targets\": solutions,\n            }\n        else:\n            return {\n                \"train_inputs\": {\"text\": train_text_encodings, \"image\": train_image_encodings},\n                \"test_inputs\": {\"text\": test_text_encodings, \"image\": test_image_encodings},\n            }\n\n    return collate_fn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloader = torch.utils.data.DataLoader(\n    dataset[\"train\"], batch_size=1, collate_fn=collate(mode=\"train\", tokenizer=models[\"tokenizer\"], processor=models[\"processor\"])\n)\n\n\ndef print_recursive(obj, indent=0):\n    if isinstance(obj, torch.Tensor):\n        print(\"  \" * indent + str(obj.shape))\n    elif (\n        isinstance(obj, dict)\n        or isinstance(obj, transformers.tokenization_utils_base.BatchEncoding)\n        or isinstance(obj, transformers.feature_extraction_utils.BatchFeature)\n    ):\n        for key, value in obj.items():\n            print(\"  \" * indent + str(key) + \":\")\n            print_recursive(value, indent + 1)\n    elif isinstance(obj, list):\n        print(\"  \" * indent + f\"List of length: {len(obj)}, {len(obj[0])}, {len(obj[0][0])}\")\n    else:\n        print(\"  \" * indent + str(obj))\n\n\nfor batch in dataloader:\n    print_recursive(batch)\n#     outputs = arc_model(**batch)\n#     print('-'* 30)\n#     print_recursive(outputs)\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(pred):\n    raise ValueError(pred)\n    return {\n        \"accuracy\": accuracy,\n        \"f1\": f1,\n        \"precision\": precision,\n        \"recall\": recall,\n    }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@train\ndef training(model, tokenizer, processor, dataset, config):\n    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"])\n    \n    train_dataloader = torch.utils.data.DataLoader(\n        dataset[\"train\"], batch_size=config[\"batch_size\"], collate_fn=collate(mode=\"train\", tokenizer=tokenizer, processor=processor)\n    )\n    \n    val_dataloader = torch.utils.data.DataLoader(\n        dataset[\"val\"], batch_size=config[\"batch_size\"], collate_fn=collate(mode=\"val\", tokenizer=tokenizer, processor=processor)\n    )\n    \n    model.train()\n    \n    train_loss = 0\n    \n    history = {'train_loss': [], 'val_loss': []}\n    for epoch in tqdm(range(config[\"epochs\"]), desc=\"Epochs\", total=config[\"epochs\"]):\n        for batch in tqdm(train_dataloader, desc=\"Train Batches\", total=len(train_dataloader)):\n            optimizer.zero_grad()\n            outputs = model(**batch)\n            \n            loss = outputs[\"loss\"]\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            \n            break\n\n        print(f\"Epoch {epoch + 1}, Loss: {train_loss / len(train_dataloader)}\")\n        \n        val_loss = 0\n        for batch in tqdm(val_dataloader, desc=\"Val Batches\", total=len(val_dataloader)):\n            outputs = model(**batch)\n            loss = outputs[\"loss\"]\n            val_loss += loss.item()\n            \n            break\n            \n        print(f\"Epoch {epoch + 1}, Val Loss: {val_loss / len(val_dataloader)}\")\n        \n        history['train_loss'].append(train_loss / len(train_dataloader))\n        history['val_loss'].append(val_loss / len(val_dataloader))\n        \n    return history","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {\n    'epochs': 5,\n    'batch_size': 2,\n    'lr': 2e-5,\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = training(arc_model, models['tokenizer'], models['processor'], dataset, config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}