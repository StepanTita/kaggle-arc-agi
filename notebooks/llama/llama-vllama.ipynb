{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optimum\n",
      "  Downloading optimum-1.23.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting coloredlogs (from optimum)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: sympy in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from optimum) (1.13.3)\n",
      "Requirement already satisfied: transformers>=4.29 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.29->optimum) (4.45.1)\n",
      "Requirement already satisfied: torch>=1.11 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from optimum) (2.4.1)\n",
      "Requirement already satisfied: packaging in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from optimum) (24.1)\n",
      "Requirement already satisfied: numpy in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from optimum) (1.26.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from optimum) (0.25.1)\n",
      "Requirement already satisfied: datasets in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from optimum) (3.0.1)\n",
      "Requirement already satisfied: filelock in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.11->optimum) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.11->optimum) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->optimum) (12.6.68)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from transformers>=4.29->transformers[sentencepiece]>=4.29->optimum) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from transformers>=4.29->transformers[sentencepiece]>=4.29->optimum) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from transformers>=4.29->transformers[sentencepiece]>=4.29->optimum) (0.20.0)\n",
      "Requirement already satisfied: protobuf in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.29->optimum) (3.20.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.29->optimum) (0.2.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->optimum)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from datasets->optimum) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from datasets->optimum) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from datasets->optimum) (2.2.3)\n",
      "Requirement already satisfied: xxhash in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from datasets->optimum) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from datasets->optimum) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from datasets->optimum) (3.10.8)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (2.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.13.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from pandas->datasets->optimum) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from pandas->datasets->optimum) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\n",
      "Downloading optimum-1.23.1-py3-none-any.whl (422 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.6/422.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, optimum\n",
      "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 optimum-1.23.1\n",
      "Collecting auto-gptq\n",
      "  Downloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from auto-gptq) (0.34.2)\n",
      "Requirement already satisfied: datasets in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from auto-gptq) (3.0.1)\n",
      "Requirement already satisfied: sentencepiece in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from auto-gptq) (0.2.0)\n",
      "Requirement already satisfied: numpy in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from auto-gptq) (1.26.4)\n",
      "Collecting rouge (from auto-gptq)\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting gekko (from auto-gptq)\n",
      "  Downloading gekko-1.2.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: torch>=1.13.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from auto-gptq) (2.4.1)\n",
      "Requirement already satisfied: safetensors in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from auto-gptq) (0.4.5)\n",
      "Requirement already satisfied: transformers>=4.31.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from auto-gptq) (4.45.1)\n",
      "Requirement already satisfied: peft>=0.5.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from auto-gptq) (0.13.0)\n",
      "Requirement already satisfied: tqdm in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from auto-gptq) (4.66.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (24.1)\n",
      "Requirement already satisfied: psutil in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (0.25.1)\n",
      "Requirement already satisfied: filelock in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->auto-gptq) (12.6.68)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (2024.9.11)\n",
      "Requirement already satisfied: requests in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (0.20.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from datasets->auto-gptq) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from datasets->auto-gptq) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from datasets->auto-gptq) (2.2.3)\n",
      "Requirement already satisfied: xxhash in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from datasets->auto-gptq) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from datasets->auto-gptq) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from datasets->auto-gptq) (3.10.8)\n",
      "Requirement already satisfied: six in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from rouge->auto-gptq) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (2.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.13.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2024.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.conda/envs/llm-py310/lib/python3.10/site-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n",
      "Downloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gekko-1.2.1-py3-none-any.whl (13.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: rouge, gekko, auto-gptq\n",
      "Successfully installed auto-gptq-0.7.1 gekko-1.2.1 rouge-1.0.1\n"
     ]
    }
   ],
   "source": [
    "# ! pip install torch==2.4.1 torchvision==0.19.0\n",
    "# ! pip install accelerate==0.34.2\n",
    "# ! pip install transformers==4.45.1\n",
    "# ! pip install unsloth==2024.9.post3\n",
    "# ! pip install bitsandbytes==0.44.0\n",
    "# ! pip install qwen-vl-utils\n",
    "! pip install optimum\n",
    "! pip install auto-gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:45:46.834705Z",
     "iopub.status.busy": "2024-10-19T21:45:46.834127Z",
     "iopub.status.idle": "2024-10-19T21:45:46.845960Z",
     "shell.execute_reply": "2024-10-19T21:45:46.845031Z",
     "shell.execute_reply.started": "2024-10-19T21:45:46.834659Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1\n",
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:45:47.022694Z",
     "iopub.status.busy": "2024-10-19T21:45:47.022377Z",
     "iopub.status.idle": "2024-10-19T21:45:47.027326Z",
     "shell.execute_reply": "2024-10-19T21:45:47.026308Z",
     "shell.execute_reply.started": "2024-10-19T21:45:47.022659Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# BASE_PATH = \"/kaggle/input\"\n",
    "BASE_PATH = \"/home/stepan/kaggle-arc-agi\"\n",
    "# MODEL_ID = f\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"\n",
    "MODEL_ID = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
    "# VLLM_MODEL_ID = \"unsloth/Llama-3.2-11B-Vision-Instruct\"\n",
    "VLLM_MODEL_ID = \"Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4\"\n",
    "# VLLM_MODEL_ID = \"Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int4\"\n",
    "MAX_NEW_TOKENS = 2048\n",
    "MAX_SEQ_LENGTH = 32768 - MAX_NEW_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM_HIDDEN_SIZE = 4096 # 8B\n",
    "# VLLM_HIDDEN_SIZE = 3584 #7B\n",
    "LLM_HIDDEN_SIZE = 3072  # 3B\n",
    "VLLM_HIDDEN_SIZE = 1536  # 2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:45:47.204747Z",
     "iopub.status.busy": "2024-10-19T21:45:47.204460Z",
     "iopub.status.idle": "2024-10-19T21:45:47.209116Z",
     "shell.execute_reply": "2024-10-19T21:45:47.207956Z",
     "shell.execute_reply.started": "2024-10-19T21:45:47.204717Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(BASE_PATH)\n",
    "sys.path.append(f\"{BASE_PATH}/scripts\")\n",
    "# sys.path.append('/kaggle/input/arc-agi-python-utilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:45:47.367264Z",
     "iopub.status.busy": "2024-10-19T21:45:47.366953Z",
     "iopub.status.idle": "2024-10-19T21:45:54.700277Z",
     "shell.execute_reply": "2024-10-19T21:45:54.699457Z",
     "shell.execute_reply.started": "2024-10-19T21:45:47.367230Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stepan/.conda/envs/llm-py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig  # type: ignore\n",
    "from transformers import MllamaForConditionalGeneration, Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from datasets import Dataset, DatasetDict  # type: ignore\n",
    "from datasets import concatenate_datasets  # type: ignore\n",
    "\n",
    "from qwen_vl_utils import process_vision_info  # type: ignore\n",
    "\n",
    "import data_utils  # type: ignore\n",
    "from logger import get_logger  # type: ignore\n",
    "import train_utils  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_memory():\n",
    "    memory_gpu_0 = train_utils.gpu_stats(device_id=0)\n",
    "    memory_gpu_1 = train_utils.gpu_stats(device_id=1)\n",
    "\n",
    "    total_gpu_0 = int(memory_gpu_0[\"max_memory\"])\n",
    "    total_gpu_1 = int(memory_gpu_1[\"max_memory\"])\n",
    "\n",
    "    max_mem_gpu_0 = int(total_gpu_0) * 0.9\n",
    "    max_mem_gpu_1 = int(total_gpu_1) * 0.9\n",
    "    block_mem_gpu_0 = max_mem_gpu_0\n",
    "    block_mem_gpu_1 = max_mem_gpu_1\n",
    "\n",
    "    x = torch.rand((256, 1024, block_mem_gpu_0)).to(\"cuda:0\")\n",
    "    y = torch.rand((256, 1024, block_mem_gpu_1)).to(\"cuda:1\")\n",
    "\n",
    "    del x\n",
    "    del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allocate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:45:54.702749Z",
     "iopub.status.busy": "2024-10-19T21:45:54.702008Z",
     "iopub.status.idle": "2024-10-19T21:45:54.707196Z",
     "shell.execute_reply": "2024-10-19T21:45:54.706164Z",
     "shell.execute_reply.started": "2024-10-19T21:45:54.702700Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:45:54.708973Z",
     "iopub.status.busy": "2024-10-19T21:45:54.708563Z",
     "iopub.status.idle": "2024-10-19T21:45:54.717299Z",
     "shell.execute_reply": "2024-10-19T21:45:54.716271Z",
     "shell.execute_reply.started": "2024-10-19T21:45:54.708925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n",
    "    llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\",\n",
    "        max_memory={0: \"23.5GiB\", \"cpu\": \"16GiB\"},\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        output_hidden_states=True,\n",
    "        return_dict_in_generate=True,\n",
    "        quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n",
    "    )\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(VLLM_MODEL_ID)\n",
    "    vllm_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        VLLM_MODEL_ID,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\",\n",
    "        max_memory={1: \"23.5GiB\", \"cpu\": \"16GiB\"},\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        output_hidden_states=True,\n",
    "        return_dict_in_generate=True,\n",
    "        #         quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n",
    "    )\n",
    "\n",
    "    return {\"llm\": llm_model, \"tokenizer\": tokenizer, \"vllm\": vllm_model, \"processor\": processor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:45:54.720694Z",
     "iopub.status.busy": "2024-10-19T21:45:54.720017Z",
     "iopub.status.idle": "2024-10-19T21:46:05.371321Z",
     "shell.execute_reply": "2024-10-19T21:46:05.370090Z",
     "shell.execute_reply.started": "2024-10-19T21:45:54.720635Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/home/stepan/.conda/envs/llm-py310/lib/python3.10/site-packages/transformers/quantizers/auto.py:182: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "/home/stepan/.conda/envs/llm-py310/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:411: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq):\n",
      "/home/stepan/.conda/envs/llm-py310/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:419: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/stepan/.conda/envs/llm-py310/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n",
      "/home/stepan/.conda/envs/llm-py310/lib/python3.10/site-packages/transformers/modeling_utils.py:4779: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "models = get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<logger.SelectiveLogger at 0x72ff826533d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = get_logger(log_path=f\"{BASE_PATH}/logs\", log_file=\"llama-vllama\")\n",
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:46:05.373048Z",
     "iopub.status.busy": "2024-10-19T21:46:05.372728Z",
     "iopub.status.idle": "2024-10-19T21:46:05.388016Z",
     "shell.execute_reply": "2024-10-19T21:46:05.386879Z",
     "shell.execute_reply.started": "2024-10-19T21:46:05.373011Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "LLAMA_3_CHAT_TEMPLATE = \"\"\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"\"\"\n",
    "\n",
    "TRAIN_IMAGE_SYSTEM_PROMPT = (\n",
    "    \"Puzzle Analysis Task: You are a puzzle-solving expert analyzing image sets from the abstraction and reasoning corpus by Francois Chollet. \"\n",
    "    \"Steps to Follow: \"\n",
    "    \"1. Overview: Count images, describe grid size and colored squares, list colors present. \"\n",
    "    \"2. Image Analysis: Detail arrangement and frequency of colors, identify patterns or structures. \"\n",
    "    \"3. Comparison: Note common color combinations and patterns, identify consistent color relationships. \"\n",
    "    \"4. Rule Identification: Consider rules like color replacement, positional, and relational rules. \"\n",
    "    \"5. Hypothesis Testing: Verify rules across images, noting exceptions. \"\n",
    "    \"6. Pattern Refinement: Generalize the pattern to fit all images. \"\n",
    "    \"7. Detailed Description: Explain the pattern with precise language and conditions. \"\n",
    "    \"8. Verification: Ensure the pattern applies to all images, including hypothetical ones. \"\n",
    "    \"9. Summary: Provide a concise summary of the pattern's logic.\"\n",
    ")\n",
    "\n",
    "TEST_IMAGE_SYSTEM_PROMPT = (\n",
    "    \"Puzzle Analysis Task: You are a puzzle-solving expert analyzing a single image from the abstraction and reasoning corpus by Francois Chollet. \"\n",
    "    \"Steps to Follow: \"\n",
    "    \"1. Overview: Describe grid size, count colored squares, list colors used. \"\n",
    "    \"2. Color Analysis: Count squares per color, note dominant or rare colors. \"\n",
    "    \"3. Spatial Analysis: Describe colored square positions and patterns. \"\n",
    "    \"4. Edge and Corner Analysis: Detail colors on edges and corners, noting patterns. \"\n",
    "    \"5. Symmetry and Balance: Check for symmetry and color balance. \"\n",
    "    \"6. Pattern Identification: Identify repeating color patterns and structures. \"\n",
    "    \"7. Color Relationships: Analyze color adjacency and arrangements. \"\n",
    "    \"8. Unique Features: Highlight unusual characteristics. \"\n",
    "    \"9. Quantitative Analysis: Calculate color ratios, note numerical patterns. \"\n",
    "    \"10. Comparative Analysis: Compare elements like left vs right or top vs bottom. \"\n",
    "    \"11. Abstraction: Describe the image abstractly, considering larger patterns. \"\n",
    "    \"12. Summary: Summarize the image's key features and significant aspects.\"\n",
    ")\n",
    "\n",
    "TRAIN_TEXT_SYSTEM_PROMPT = (\n",
    "    \"Puzzle Analysis Task: You are a puzzle-solving expert analyzing matrices from the abstraction and reasoning corpus by Francois Chollet. \"\n",
    "    \"Steps to Follow: \"\n",
    "    \"1. Overview: Count matrices, describe structure, list numbers present. \"\n",
    "    \"2. Matrix Analysis: Detail number arrangement and frequency, identify patterns or structures. \"\n",
    "    \"3. Comparison: Note common number combinations and patterns, identify consistent number relationships. \"\n",
    "    \"4. Rule Identification: Consider rules like number replacement, positional, quantity, shape, relational, and mathematical operations. \"\n",
    "    \"5. Hypothesis Testing: Verify rules across matrices, noting exceptions. \"\n",
    "    \"6. Pattern Refinement: Generalize the pattern to fit all matrices. \"\n",
    "    \"7. Detailed Description: Explain the pattern with precise language and conditions. \"\n",
    "    \"8. Verification: Ensure the pattern applies to all matrices, including hypothetical ones. \"\n",
    "    \"9. Summary: Provide a concise summary of the pattern's logic.\"\n",
    ")\n",
    "\n",
    "TEST_TEXT_SYSTEM_PROMPT = (\n",
    "    \"Puzzle Analysis Task: You are a puzzle-solving expert analyzing a single matrix from the abstraction and reasoning corpus by Francois Chollet. \"\n",
    "    \"Steps to Follow: \"\n",
    "    \"1. Overview: Describe matrix dimensions, number range, note immediate patterns. \"\n",
    "    \"2. Number Analysis: Count number frequency, note common or rare numbers. \"\n",
    "    \"3. Spatial Analysis: Describe key number positions and patterns. \"\n",
    "    \"4. Edge and Corner Analysis: Detail numbers on edges and corners, noting patterns. \"\n",
    "    \"5. Symmetry and Balance: Check for symmetry and number balance. \"\n",
    "    \"6. Pattern Identification: Identify repeating number patterns and structures. \"\n",
    "    \"7. Number Relationships: Analyze number adjacency and arrangements, note mathematical relationships. \"\n",
    "    \"8. Unique Features: Highlight unusual characteristics. \"\n",
    "    \"9. Quantitative Analysis: Calculate relevant statistics, note numerical patterns. \"\n",
    "    \"10. Comparative Analysis: Compare elements like rows vs columns or quadrants. \"\n",
    "    \"11. Abstraction: Describe the matrix abstractly, considering larger patterns. \"\n",
    "    \"12. Summary: Summarize the matrix's key features and significant aspects.\"\n",
    ")\n",
    "\n",
    "TRAIN_TEXT_PROMPT = (\n",
    "    \"Learn the underlying rule from these example input-output pairs to predict the output for the test input: \"\n",
    "    \"----------------- \"\n",
    "    \"{training_data}\"\n",
    ")\n",
    "\n",
    "TEST_TEXT_PROMPT = \"Analyze the following test input data: \" \"----------------- \" \"{input_test_data}\"\n",
    "\n",
    "TRAIN_IMAGE_PROMPT = \"Analyze the images.\"\n",
    "TEST_IMAGE_PROMPT = \"Analyze the image.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models[\"tokenizer\"].chat_template = LLAMA_3_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:46:05.389795Z",
     "iopub.status.busy": "2024-10-19T21:46:05.389486Z",
     "iopub.status.idle": "2024-10-19T21:46:05.399540Z",
     "shell.execute_reply": "2024-10-19T21:46:05.398565Z",
     "shell.execute_reply.started": "2024-10-19T21:46:05.389761Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def list_to_image(integer_list_2d, target_size=30):\n",
    "    # Convert the 2D list to a NumPy array\n",
    "    array = np.array(integer_list_2d)\n",
    "\n",
    "    # Get the unique values in the array\n",
    "    unique_values = np.unique(array)\n",
    "\n",
    "    # Create a colormap\n",
    "    cmap = plt.get_cmap(\"tab10\")\n",
    "\n",
    "    # Create a color lookup dictionary\n",
    "    color_lookup = {value: cmap(i % 10)[:3] for i, value in enumerate(unique_values)}\n",
    "\n",
    "    # Create an RGB array\n",
    "    rgb_array = np.array([[color_lookup[val] for val in row] for row in array])\n",
    "\n",
    "    # Convert to 8-bit color values\n",
    "    rgb_array = (rgb_array * 255).astype(np.uint8)\n",
    "\n",
    "    # Create an image from the colored array\n",
    "    image = Image.fromarray(rgb_array, mode=\"RGB\")\n",
    "\n",
    "    # Create a new blank image with the target size\n",
    "    new_image = Image.new(\"RGB\", (target_size, target_size), color=(0, 0, 0))\n",
    "\n",
    "    # Paste the original image onto the new image\n",
    "    new_image.paste(image, (0, 0))\n",
    "\n",
    "    new_image = new_image.resize((target_size * 15, target_size * 15), Image.NEAREST)\n",
    "\n",
    "    return new_image\n",
    "\n",
    "\n",
    "def pil_image_to_base64(image):\n",
    "    buffered = io.BytesIO()\n",
    "    image.save(buffered, format=\"PNG\")\n",
    "    return \"data:image;base64,\" + base64.b64encode(buffered.getvalue()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:46:05.401505Z",
     "iopub.status.busy": "2024-10-19T21:46:05.400786Z",
     "iopub.status.idle": "2024-10-19T21:46:05.410279Z",
     "shell.execute_reply": "2024-10-19T21:46:05.409335Z",
     "shell.execute_reply.started": "2024-10-19T21:46:05.401458Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_inputs(dct, prepare_solution=False):\n",
    "    if prepare_solution:\n",
    "        return \"<output>\\n\" + \"\\n\".join(\" \".join(map(str, row)) for row in dct) + \"\\n</output>\"\n",
    "    else:\n",
    "        input_str = \"\\n\".join(\" \".join(map(str, row)) for row in dct[\"input\"])\n",
    "        output_str = \"\\n\".join(\" \".join(map(str, row)) for row in dct[\"output\"]) if \"output\" in dct else \"\"\n",
    "        text = f\"<input>\\n{input_str}\\n</input>\"\n",
    "        if output_str:\n",
    "            text += f\"\\n\\n<output>\\n{output_str}\\n</output>\"\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:46:59.784233Z",
     "iopub.status.busy": "2024-10-19T21:46:59.783805Z",
     "iopub.status.idle": "2024-10-19T21:46:59.791544Z",
     "shell.execute_reply": "2024-10-19T21:46:59.790523Z",
     "shell.execute_reply.started": "2024-10-19T21:46:59.784186Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pad_matrix(matrix, target_rows, target_cols, pad_value=0):\n",
    "    # Convert input to numpy array if it's not already\n",
    "    matrix = np.array(matrix)\n",
    "\n",
    "    # Get current dimensions\n",
    "    current_rows, current_cols = matrix.shape\n",
    "\n",
    "    # Pad rows\n",
    "    if current_rows < target_rows:\n",
    "        pad_rows = np.full((target_rows - current_rows, current_cols), pad_value)\n",
    "        matrix = np.vstack((matrix, pad_rows))\n",
    "\n",
    "    # Pad columns\n",
    "    if current_cols < target_cols:\n",
    "        pad_cols = np.full((target_rows, target_cols - current_cols), pad_value)\n",
    "        matrix = np.hstack((matrix, pad_cols))\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_size(matrix):\n",
    "    matrix = np.array(matrix)\n",
    "    h, w = matrix.shape\n",
    "    if h < 1 or w < 1:\n",
    "        return np.array([[0]])  # Minimum size 1x1\n",
    "    return matrix[:30, :30]  # Maximum size 30x30\n",
    "\n",
    "\n",
    "def rotate_grid(matrix, k):\n",
    "    return ensure_size(np.rot90(matrix, k=k))\n",
    "\n",
    "\n",
    "def flip_grid(matrix, axis):\n",
    "    return ensure_size(np.flip(matrix, axis=axis))\n",
    "\n",
    "\n",
    "def expand_grid(matrix, factor=3):\n",
    "    expanded = np.repeat(np.repeat(matrix, factor, axis=0), factor, axis=1)\n",
    "    return ensure_size(expanded)\n",
    "\n",
    "\n",
    "def shrink_grid(matrix, factor=3):\n",
    "    matrix = np.array(matrix)\n",
    "    h, w = matrix.shape\n",
    "    shrunk = matrix[: h // factor * factor : factor, : w // factor * factor : factor]\n",
    "    return ensure_size(shrunk)\n",
    "\n",
    "\n",
    "def roll_grid(matrix, shift, axis):\n",
    "    return ensure_size(np.roll(matrix, shift=shift, axis=axis))\n",
    "\n",
    "\n",
    "def add_border(matrix, value=0):\n",
    "    bordered = np.pad(matrix, pad_width=1, mode=\"constant\", constant_values=value)\n",
    "    return ensure_size(bordered)\n",
    "\n",
    "\n",
    "def remove_border(matrix):\n",
    "    matrix = np.array(matrix)\n",
    "    if matrix.shape[0] <= 2 or matrix.shape[1] <= 2:\n",
    "        return ensure_size(matrix)  # Can't remove border from very small matrices\n",
    "    return ensure_size(matrix[1:-1, 1:-1])\n",
    "\n",
    "\n",
    "def replace_color(matrix, old_color, new_color):\n",
    "    return ensure_size(np.where(matrix == old_color, new_color, matrix))\n",
    "\n",
    "\n",
    "def add_noise(matrix, noise, noise_values):\n",
    "    return ensure_size(np.where(noise, noise_values, matrix))\n",
    "\n",
    "\n",
    "def mirror_grid(matrix):\n",
    "    mirrored = np.hstack([matrix, np.fliplr(matrix)])\n",
    "    return ensure_size(mirrored)\n",
    "\n",
    "\n",
    "def tile_grid(matrix, tiles=(2, 2)):\n",
    "    tiled = np.tile(matrix, tiles)\n",
    "    return ensure_size(tiled)\n",
    "\n",
    "\n",
    "def diagonal_shift(matrix, shift):\n",
    "    shifted = np.pad(matrix, ((shift, 0), (shift, 0)), mode=\"constant\")[:-shift, :-shift]\n",
    "    return ensure_size(shifted)\n",
    "\n",
    "\n",
    "def apply_mask(matrix, mask):\n",
    "    return ensure_size(matrix * mask)\n",
    "\n",
    "\n",
    "def swap_quadrants(matrix, order):\n",
    "    matrix = ensure_size(matrix)\n",
    "    h, w = matrix.shape\n",
    "    if h < 2 or w < 2:\n",
    "        return matrix  # Can't swap quadrants for very small matrices\n",
    "    quadrants = [matrix[: h // 2, : w // 2], matrix[: h // 2, w // 2 :], matrix[h // 2 :, : w // 2], matrix[h // 2 :, w // 2 :]]\n",
    "    reordered = [quadrants[i] for i in order]\n",
    "    swapped = np.vstack([np.hstack([reordered[0], reordered[1]]), np.hstack([reordered[2], reordered[3]])])\n",
    "    return ensure_size(swapped)\n",
    "\n",
    "\n",
    "def generate_augmentation_pipeline(num_augmentations=3, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    operations = [\n",
    "        (\"rotate\", lambda: {\"k\": random.choice([1, 2, 3])}),\n",
    "        (\"flip\", lambda: {\"axis\": random.choice([0, 1])}),\n",
    "        (\"expand\", lambda: {\"factor\": random.randint(2, 4)}),\n",
    "        # (\"shrink\", lambda: {\"factor\": random.randint(2, 4)}),\n",
    "        # (\"roll\", lambda: {\"shift\": random.randint(1, 5), \"axis\": random.choice([0, 1])}),\n",
    "        (\"add_border\", lambda: {\"value\": random.randint(0, 9)}),\n",
    "        # (\"remove_border\", lambda: {}),\n",
    "        (\"replace_color\", lambda: {\"old_color\": random.randint(0, 9), \"new_color\": random.randint(0, 9)}),\n",
    "        (\"add_noise\", lambda: {\"noise_prob\": random.uniform(0.05, 0.2)}),\n",
    "        (\"mirror\", lambda: {}),\n",
    "        (\"tile\", lambda: {\"tiles\": (random.randint(1, 3), random.randint(1, 3))}),\n",
    "        # (\"diagonal_shift\", lambda: {\"shift\": random.randint(1, 5)}),\n",
    "        (\"apply_mask\", lambda: {\"mask_prob\": random.uniform(0.1, 0.3)}),\n",
    "        (\"swap_quadrants\", lambda: {\"order\": random.sample(range(4), 4)}),\n",
    "    ]\n",
    "\n",
    "    pipeline = []\n",
    "    for _ in range(num_augmentations):\n",
    "        op, param_func = random.choice(operations)\n",
    "        pipeline.append((op, param_func()))\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def apply_augmentation_pipeline(matrix, pipeline):\n",
    "    matrix = np.array(matrix)\n",
    "    for op, params in pipeline:\n",
    "        if op == \"rotate\":\n",
    "            matrix = rotate_grid(matrix, **params)\n",
    "        elif op == \"flip\":\n",
    "            matrix = flip_grid(matrix, **params)\n",
    "        elif op == \"expand\":\n",
    "            matrix = expand_grid(matrix, **params)\n",
    "        elif op == \"shrink\":\n",
    "            matrix = shrink_grid(matrix, **params)\n",
    "        elif op == \"roll\":\n",
    "            matrix = roll_grid(matrix, **params)\n",
    "        elif op == \"add_border\":\n",
    "            matrix = add_border(matrix, **params)\n",
    "        elif op == \"remove_border\":\n",
    "            matrix = remove_border(matrix)\n",
    "        elif op == \"replace_color\":\n",
    "            matrix = replace_color(matrix, **params)\n",
    "        elif op == \"add_noise\":\n",
    "            noise = np.random.choice([0, 1], size=matrix.shape, p=[1 - params[\"noise_prob\"], params[\"noise_prob\"]])\n",
    "            noise_values = np.random.randint(0, 10, size=matrix.shape)\n",
    "            matrix = add_noise(matrix, noise, noise_values)\n",
    "        elif op == \"mirror\":\n",
    "            matrix = mirror_grid(matrix)\n",
    "        elif op == \"tile\":\n",
    "            matrix = tile_grid(matrix, **params)\n",
    "        elif op == \"diagonal_shift\":\n",
    "            matrix = diagonal_shift(matrix, **params)\n",
    "        elif op == \"apply_mask\":\n",
    "            mask = np.random.choice([0, 1], size=matrix.shape, p=[params[\"mask_prob\"], 1 - params[\"mask_prob\"]])\n",
    "            matrix = apply_mask(matrix, mask)\n",
    "        elif op == \"swap_quadrants\":\n",
    "            matrix = swap_quadrants(matrix, **params)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:46:05.411945Z",
     "iopub.status.busy": "2024-10-19T21:46:05.411580Z",
     "iopub.status.idle": "2024-10-19T21:46:05.436820Z",
     "shell.execute_reply": "2024-10-19T21:46:05.435947Z",
     "shell.execute_reply.started": "2024-10-19T21:46:05.411910Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def augment_challenge(challenge, solution, pipeline):\n",
    "    new_challenge = {\n",
    "        **challenge,\n",
    "        \"train\": [\n",
    "            {\n",
    "                \"input\": apply_augmentation_pipeline(grid[\"input\"], pipeline).tolist(),\n",
    "                \"output\": apply_augmentation_pipeline(grid[\"output\"], pipeline).tolist(),\n",
    "            }\n",
    "            for grid in challenge[\"train\"]\n",
    "        ],\n",
    "        \"test\": {\"input\": apply_augmentation_pipeline(challenge[\"test\"][\"input\"], pipeline).tolist()},\n",
    "    }\n",
    "    return {\"challenge\": new_challenge, \"solution\": apply_augmentation_pipeline(solution, pipeline).tolist()}\n",
    "\n",
    "\n",
    "def augment_dataset(dataset, num_augmentations=3, total_number=1000, seed=11):\n",
    "    augmented_dataset = {\n",
    "        \"id\": [],\n",
    "        \"challenge\": [],\n",
    "        \"solution\": [],\n",
    "    }\n",
    "\n",
    "    while len(augmented_dataset[\"id\"]) < total_number:\n",
    "        idx = random.randint(0, len(dataset[\"id\"]) - 1)\n",
    "        challenge = dataset[\"challenge\"][idx]\n",
    "\n",
    "        solution = dataset[\"solution\"][idx]\n",
    "\n",
    "        pipeline = generate_augmentation_pipeline(num_augmentations, seed)\n",
    "        augmented_challenge = augment_challenge(challenge, solution, pipeline)\n",
    "\n",
    "        augmented_dataset[\"id\"].append(f'aug-{dataset[\"id\"][idx]}-{num_augmentations}')\n",
    "        augmented_dataset[\"challenge\"].append(augmented_challenge[\"challenge\"])\n",
    "        augmented_dataset[\"solution\"].append(augmented_challenge[\"solution\"])\n",
    "\n",
    "    return augmented_dataset\n",
    "\n",
    "\n",
    "def to_dataset(data, solutions=None, augment=False):\n",
    "    restructured_data = {\n",
    "        \"id\": [],\n",
    "        \"challenge\": [],\n",
    "    }\n",
    "    if solutions is not None:\n",
    "        restructured_data[\"solution\"] = []\n",
    "\n",
    "    for challenge_id, challenge_data in data.items():  # for all challenges\n",
    "        for test_id, task in enumerate(\n",
    "            challenge_data[\"test\"]\n",
    "        ):  # for all test tasks in this challenge we want to expand dataset so that each test task is separate dataset record\n",
    "            restructured_data[\"id\"].append(challenge_id)\n",
    "            restructured_data[\"challenge\"].append({\"train\": challenge_data[\"train\"], \"test\": task, \"order\": test_id})\n",
    "            if solutions is not None:\n",
    "                restructured_data[\"solution\"].append(solutions[challenge_id][test_id])\n",
    "\n",
    "    if augment:\n",
    "        augmented_data = augment_dataset(restructured_data, num_augmentations=3, total_number=10_000, seed=11)\n",
    "        restructured_data[\"id\"].extend(augmented_data[\"id\"])\n",
    "        restructured_data[\"challenge\"].extend(augmented_data[\"challenge\"])\n",
    "        restructured_data[\"solution\"].extend(augmented_data[\"solution\"])\n",
    "\n",
    "        augmented_data = augment_dataset(restructured_data, num_augmentations=5, total_number=10_000, seed=22)\n",
    "        restructured_data[\"id\"].extend(augmented_data[\"id\"])\n",
    "        restructured_data[\"challenge\"].extend(augmented_data[\"challenge\"])\n",
    "        restructured_data[\"solution\"].extend(augmented_data[\"solution\"])\n",
    "\n",
    "        augmented_data = augment_dataset(restructured_data, num_augmentations=7, total_number=10_000, seed=33)\n",
    "        restructured_data[\"id\"].extend(augmented_data[\"id\"])\n",
    "        restructured_data[\"challenge\"].extend(augmented_data[\"challenge\"])\n",
    "        restructured_data[\"solution\"].extend(augmented_data[\"solution\"])\n",
    "\n",
    "    return Dataset.from_dict(restructured_data)\n",
    "\n",
    "\n",
    "def prepare_inputs(dct, prepare_solution=False):\n",
    "    if prepare_solution:\n",
    "        return \"<output>\\n\" + \"\\n\".join(\" \".join(map(str, row)) for row in dct) + \"\\n</output>\"\n",
    "    else:\n",
    "        input_str = \"\\n\".join(\" \".join(map(str, row)) for row in dct[\"input\"])\n",
    "        output_str = \"\\n\".join(\" \".join(map(str, row)) for row in dct[\"output\"]) if \"output\" in dct else \"\"\n",
    "        text = f\"<input>\\n{input_str}\\n</input>\"\n",
    "        if output_str:\n",
    "            text += f\"\\n\\n<output>\\n{output_str}\\n</output>\"\n",
    "        return text\n",
    "\n",
    "\n",
    "def prepare_dataset(tokenizer, base_path=None, final_training=False):\n",
    "    # Load all datasets\n",
    "    training_challenges = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_training_challenges.json\")\n",
    "    training_solutions = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_training_solutions.json\")\n",
    "    evaluation_challenges = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_evaluation_challenges.json\")\n",
    "    evaluation_solutions = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_evaluation_solutions.json\")\n",
    "    test_challenges = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_test_challenges.json\")\n",
    "\n",
    "    train_dataset = to_dataset(training_challenges, training_solutions, augment=True)\n",
    "    eval_dataset = to_dataset(evaluation_challenges, evaluation_solutions)\n",
    "    pred_dataset = to_dataset(test_challenges)\n",
    "\n",
    "    def create_train_image_content(challenge):\n",
    "        content = [{\"type\": \"text\", \"text\": TRAIN_IMAGE_SYSTEM_PROMPT}]\n",
    "\n",
    "        for i, example in enumerate(challenge[\"train\"]):\n",
    "            content.extend(\n",
    "                [\n",
    "                    {\"type\": \"text\", \"text\": f\"Input Task {i+1}\"},\n",
    "                    {\"type\": \"image\", \"image\": pil_image_to_base64(list_to_image(example[\"input\"]))},\n",
    "                    {\"type\": \"text\", \"text\": f\"Output Task {i+1}\"},\n",
    "                    {\"type\": \"image\", \"image\": pil_image_to_base64(list_to_image(example[\"output\"]))},\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        content.append({\"type\": \"text\", \"text\": TRAIN_IMAGE_PROMPT})\n",
    "        return content\n",
    "\n",
    "    def create_chat(challenge, solution=None):\n",
    "        train_input = TRAIN_TEXT_SYSTEM_PROMPT.format(\n",
    "            training_data=\"\\n\\n\".join([prepare_inputs(ex) for ex in challenge[\"train\"]]),\n",
    "        )\n",
    "        test_input = TEST_TEXT_SYSTEM_PROMPT.format(\n",
    "            input_test_data=prepare_inputs(challenge[\"test\"]),\n",
    "        )\n",
    "\n",
    "        train_text_messages = [\n",
    "            {\"role\": \"system\", \"content\": TRAIN_TEXT_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": train_input},\n",
    "        ]\n",
    "\n",
    "        test_text_messages = [\n",
    "            {\"role\": \"system\", \"content\": TEST_TEXT_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": test_input},\n",
    "        ]\n",
    "\n",
    "        train_image_messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": create_train_image_content(challenge),\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        test_image_messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": TEST_IMAGE_SYSTEM_PROMPT},\n",
    "                    {\"type\": \"text\", \"text\": f\"Input Test Task\"},\n",
    "                    {\"type\": \"image\", \"image\": pil_image_to_base64(list_to_image(challenge[\"test\"][\"input\"]))},\n",
    "                    {\"type\": \"text\", \"text\": TEST_IMAGE_PROMPT},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        if solution:\n",
    "            test_text_messages.append(\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": prepare_inputs(solution, prepare_solution=True),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"train_text_messages\": train_text_messages,\n",
    "            \"test_text_messages\": test_text_messages,\n",
    "            \"train_image_messages\": train_image_messages,\n",
    "            \"test_image_messages\": test_image_messages,\n",
    "        }\n",
    "\n",
    "    def process_dataset(examples, solutions=None):\n",
    "        # Create messages for each challenge-solution pair\n",
    "        chats = []\n",
    "        for challenge, solution in zip(examples[\"challenge\"], solutions or [None] * len(examples[\"challenge\"])):\n",
    "            chat = create_chat(challenge, solution)\n",
    "            chats.append(chat)\n",
    "\n",
    "        return {\"messages\": chats}\n",
    "\n",
    "    pred_dataset = pred_dataset.map(lambda x: process_dataset(x), batched=True)\n",
    "    train_dataset = train_dataset.map(lambda x: process_dataset(x, train_dataset[\"solution\"]), batched=True)\n",
    "    eval_dataset = eval_dataset.map(lambda x: process_dataset(x, eval_dataset[\"solution\"]), batched=True)\n",
    "\n",
    "    if final_training:  # if final training, we need to add the validation dataset to the training dataset\n",
    "        train_dataset = concatenate_datasets([train_dataset, eval_dataset]).shuffle(seed=42)\n",
    "\n",
    "        return DatasetDict(\n",
    "            {\n",
    "                \"train\": train_dataset,\n",
    "                \"predict\": pred_dataset,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    test_dataset = eval_dataset.train_test_split(test_size=0.3)\n",
    "\n",
    "    dataset = DatasetDict(\n",
    "        {\n",
    "            \"train\": train_dataset,\n",
    "            \"test\": test_dataset[\"train\"],\n",
    "            \"val\": test_dataset[\"test\"],\n",
    "            \"predict\": pred_dataset,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:46:05.438204Z",
     "iopub.status.busy": "2024-10-19T21:46:05.437863Z",
     "iopub.status.idle": "2024-10-19T21:46:59.262576Z",
     "shell.execute_reply": "2024-10-19T21:46:59.261665Z",
     "shell.execute_reply.started": "2024-10-19T21:46:05.438169Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30416/30416 [28:00<00:00, 18.10 examples/s]\n",
      "Map: 100%|██████████| 419/419 [00:10<00:00, 38.60 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'challenge', 'solution', 'messages'],\n",
       "        num_rows: 30416\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'challenge', 'solution', 'messages'],\n",
       "        num_rows: 293\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['id', 'challenge', 'solution', 'messages'],\n",
       "        num_rows: 126\n",
       "    })\n",
       "    predict: Dataset({\n",
       "        features: ['id', 'challenge', 'messages'],\n",
       "        num_rows: 105\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = prepare_dataset(models[\"tokenizer\"], base_path=BASE_PATH, final_training=False)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example(sample, cmap=\"Set1\", title=None):\n",
    "    # print train examples as a multiplot with input -> output\n",
    "    fig, axs = plt.subplots(len(sample[\"train\"]) + 1, 2, figsize=(20, 20))\n",
    "    for i, ex in enumerate(sample[\"train\"]):\n",
    "        axs[i][0].imshow(ex[\"input\"], cmap=cmap)\n",
    "        axs[i][1].imshow(ex[\"output\"], cmap=cmap)\n",
    "\n",
    "        axs[i][0].axis(\"off\")\n",
    "        axs[i][1].axis(\"off\")\n",
    "\n",
    "    idx = len(sample[\"train\"])\n",
    "\n",
    "    axs[idx][0].imshow(sample[\"test\"][\"input\"], cmap=cmap)\n",
    "    # plot empty image if no output is available black and white cmap\n",
    "    # axs[i][1].imshow(np.zeros_like(ex['input']), cmap='gray')\n",
    "\n",
    "    axs[idx][0].axis(\"off\")\n",
    "    axs[idx][1].axis(\"off\")\n",
    "\n",
    "    fig.suptitle(title)\n",
    "\n",
    "\n",
    "def augment_challenge(challenge, augmentation):\n",
    "    new_challenge = {\n",
    "        \"train\": [{\"input\": augmentation(grid[\"input\"]), \"output\": augmentation(grid[\"output\"])} for grid in challenge[\"train\"]],\n",
    "        \"test\": {\"input\": augmentation(challenge[\"test\"][\"input\"])},\n",
    "    }\n",
    "    return new_challenge\n",
    "\n",
    "\n",
    "def demonstrate_transformations(dataset):\n",
    "    # Sample a few examples from the dataset\n",
    "    # samples = random.sample(list(dataset['train']), 3)\n",
    "    samples = list(dataset[\"train\"])[:3]\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        challenge = augment_challenge(sample[\"challenge\"], lambda x: rotate_grid(x, k=1))\n",
    "        plot_example(challenge, title=f\"Example {i+1} - Rotate 90\")\n",
    "\n",
    "        challenge = augment_challenge(sample[\"challenge\"], lambda x: flip_grid(x, axis=0))\n",
    "        plot_example(challenge, title=f\"Example {i+1} - Flip Vertical\")\n",
    "\n",
    "        challenge = augment_challenge(sample[\"challenge\"], lambda x: expand_grid(x, factor=3))\n",
    "        plot_example(challenge, title=f\"Example {i+1} - Expand\")\n",
    "\n",
    "        challenge = augment_challenge(sample[\"challenge\"], lambda x: shrink_grid(x, factor=2))\n",
    "        plot_example(challenge, title=f\"Example {i+1} - Shrink\")\n",
    "\n",
    "        challenge = augment_challenge(sample[\"challenge\"], lambda x: roll_grid(x, shift=2, axis=0))\n",
    "        plot_example(challenge, title=f\"Example {i+1} - Roll\")\n",
    "\n",
    "        challenge = augment_challenge(sample[\"challenge\"], lambda x: add_border(x, value=1))\n",
    "        plot_example(challenge, title=f\"Example {i+1} - Add Border\")\n",
    "\n",
    "        challenge = augment_challenge(sample[\"challenge\"], lambda x: remove_border(x))\n",
    "        plot_example(challenge, title=f\"Example {i+1} - Remove Border\")\n",
    "\n",
    "        challenge = augment_challenge(sample[\"challenge\"], lambda x: replace_color(x, old_color=1, new_color=2))\n",
    "        plot_example(challenge, title=f\"Example {i+1} - Replace Color\")\n",
    "\n",
    "        challenge = augment_challenge(sample[\"challenge\"], lambda x: add_noise(x, noise_prob=0.1))\n",
    "        plot_example(challenge, title=f\"Example {i+1} - Add Noise\")\n",
    "\n",
    "        challenge = augment_challenge(sample[\"challenge\"], lambda x: mirror_grid(x))\n",
    "        plot_example(challenge, title=f\"Example {i+1} - Mirror\")\n",
    "\n",
    "        challenge = augment_challenge(sample[\"challenge\"], lambda x: tile_grid(x, tiles=(2, 2)))\n",
    "        plot_example(challenge, title=f\"Example {i+1} - Tile\")\n",
    "\n",
    "        challenge = augment_challenge(sample[\"challenge\"], lambda x: diagonal_shift(x, shift=2))\n",
    "        plot_example(challenge, title=f\"Example {i+1} - Diagonal Shift\")\n",
    "\n",
    "        challenge = augment_challenge(sample[\"challenge\"], lambda x: apply_mask(x, mask=np.random.choice([0, 1], size=(3, 3), p=[0.2, 0.8])))\n",
    "        plot_example(challenge, title=f\"Example {i+1} - Apply Mask\")\n",
    "\n",
    "        challenge = augment_challenge(sample[\"challenge\"], lambda x: swap_quadrants(x, order=[1, 2, 3, 0]))\n",
    "        plot_example(challenge, title=f\"Example {i+1} - Swap Quadrants\")\n",
    "\n",
    "        ...\n",
    "\n",
    "\n",
    "# Demonstrate the transformations\n",
    "# demonstrate_transformations(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_long_entries(dataset):\n",
    "    def check_length(example):\n",
    "        messages = example[\"messages\"]\n",
    "        inputs = models[\"processor\"](messages=messages, return_tensors=\"pt\", padding=True)\n",
    "        pixel_values = inputs.pixel_values\n",
    "        return pixel_values.numel() <= 16000\n",
    "\n",
    "    filtered_dataset = dataset.filter(check_length)\n",
    "    return filtered_dataset\n",
    "\n",
    "\n",
    "# Apply the filter to all splits in the dataset\n",
    "# filtered_dataset = DatasetDict({\n",
    "#     split: filter_long_entries(dataset[split])\n",
    "#     for split in dataset.keys()\n",
    "# })\n",
    "\n",
    "# print(\"Original dataset sizes:\")\n",
    "# for split, ds in dataset.items():\n",
    "#     print(f\"{split}: {len(ds)}\")\n",
    "\n",
    "# print(\"\\nFiltered dataset sizes:\")\n",
    "# for split, ds in filtered_dataset.items():\n",
    "#     print(f\"{split}: {len(ds)}\")\n",
    "\n",
    "# # Update the dataset variable with the filtered version\n",
    "# dataset = filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:46:59.281974Z",
     "iopub.status.busy": "2024-10-19T21:46:59.281555Z",
     "iopub.status.idle": "2024-10-19T21:46:59.290075Z",
     "shell.execute_reply": "2024-10-19T21:46:59.289169Z",
     "shell.execute_reply.started": "2024-10-19T21:46:59.281923Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def eval(f):\n",
    "    def wrapper(model, *args, **kwargs):\n",
    "        if hasattr(model, \"to_inference\"):\n",
    "            model.to_inference()\n",
    "        else:\n",
    "            model.eval()\n",
    "        with torch.no_grad():\n",
    "            return f(model, *args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def train(f):\n",
    "    def wrapper(model, *args, **kwargs):\n",
    "        if hasattr(model, \"to_training\"):\n",
    "            model.to_training()\n",
    "        else:\n",
    "            model.train()\n",
    "        return f(model, *args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:46:59.291581Z",
     "iopub.status.busy": "2024-10-19T21:46:59.291226Z",
     "iopub.status.idle": "2024-10-19T21:46:59.302408Z",
     "shell.execute_reply": "2024-10-19T21:46:59.301619Z",
     "shell.execute_reply.started": "2024-10-19T21:46:59.291545Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@eval\n",
    "def describe_puzzle(model, processor, image, prompt):\n",
    "    # Create prompt\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(text=[text], images=[image], return_tensors=\"pt\")\n",
    "    inputs = inputs.to(model.device)\n",
    "\n",
    "    # Run inference\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    generated_ids = generated_ids[0, inputs.input_ids.shape[1] :]\n",
    "    generated_text = processor.decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:46:59.303880Z",
     "iopub.status.busy": "2024-10-19T21:46:59.303553Z",
     "iopub.status.idle": "2024-10-19T21:46:59.310947Z",
     "shell.execute_reply": "2024-10-19T21:46:59.310085Z",
     "shell.execute_reply.started": "2024-10-19T21:46:59.303846Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# image = list_to_image(dataset[\"train\"][10][\"challenge\"][\"train\"][0][\"input\"])\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:46:59.312922Z",
     "iopub.status.busy": "2024-10-19T21:46:59.312213Z",
     "iopub.status.idle": "2024-10-19T21:46:59.319317Z",
     "shell.execute_reply": "2024-10-19T21:46:59.318462Z",
     "shell.execute_reply.started": "2024-10-19T21:46:59.312835Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# describe_puzzle(models['vllm'], models['processor'], image, \"Describe the image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:46:59.320730Z",
     "iopub.status.busy": "2024-10-19T21:46:59.320408Z",
     "iopub.status.idle": "2024-10-19T21:46:59.330883Z",
     "shell.execute_reply": "2024-10-19T21:46:59.329870Z",
     "shell.execute_reply.started": "2024-10-19T21:46:59.320696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, latent_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.condition_dim = condition_dim\n",
    "\n",
    "        self.query = nn.Linear(input_dim, hidden_dim, dtype=dtype)\n",
    "        self.key = nn.Linear(input_dim, hidden_dim, dtype=dtype)\n",
    "        self.value = nn.Linear(input_dim, hidden_dim, dtype=dtype)\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4, dtype=dtype)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim, dtype=dtype)\n",
    "\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim, dtype=dtype)  # Mean of the latent space\n",
    "        self.fc_var = nn.Linear(hidden_dim, latent_dim, dtype=dtype)  # Variance of the latent space\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        # Add the condition to the input\n",
    "        x_cond = torch.cat([x, condition], dim=1)\n",
    "\n",
    "        # Apply attention\n",
    "        attn_output, _ = self.attention(self.query(x_cond), self.key(x_cond), self.value(x_cond))\n",
    "        h = F.relu(self.fc1(attn_output.mean(dim=1)))  # Reduce to a single representation per sample\n",
    "\n",
    "        # Compute the mean and variance for the latent space\n",
    "        mu = self.fc_mu(h)\n",
    "        log_var = self.fc_var(h)\n",
    "\n",
    "        return mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:46:59.332321Z",
     "iopub.status.busy": "2024-10-19T21:46:59.331979Z",
     "iopub.status.idle": "2024-10-19T21:46:59.341125Z",
     "shell.execute_reply": "2024-10-19T21:46:59.340249Z",
     "shell.execute_reply.started": "2024-10-19T21:46:59.332286Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def reparameterize(mu, log_var):\n",
    "    std = torch.exp(0.5 * log_var)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps * std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:46:59.342635Z",
     "iopub.status.busy": "2024-10-19T21:46:59.342269Z",
     "iopub.status.idle": "2024-10-19T21:46:59.352531Z",
     "shell.execute_reply": "2024-10-19T21:46:59.351423Z",
     "shell.execute_reply.started": "2024-10-19T21:46:59.342589Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, condition_dim, output_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.condition_dim = condition_dim\n",
    "        self.fc1 = nn.Linear(latent_dim + condition_dim, hidden_dim, dtype=dtype)\n",
    "\n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim, dtype=dtype)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim, dtype=dtype)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim, dtype=dtype)\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4, dtype=dtype)\n",
    "        self.fc_output = nn.Linear(\n",
    "            hidden_dim, output_dim * output_dim * 10, dtype=dtype\n",
    "        )  # output is the 30x30 image with each pixel being a vector of logits\n",
    "\n",
    "    def forward(self, z, condition, output_len):\n",
    "        # Combine latent variable z and condition\n",
    "        z_cond = torch.cat([z.unsqueeze(1).repeat(1, condition.shape[1], 1), condition], dim=-1)\n",
    "\n",
    "        h = F.relu(self.fc1(z_cond))\n",
    "\n",
    "        # Apply attention to guide the generation process\n",
    "        attn_output, _ = self.attention(self.query(h), self.key(h), self.value(h))\n",
    "\n",
    "        # Generate output\n",
    "        output = torch.softmax(self.fc_output(attn_output), dim=-1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:46:59.353868Z",
     "iopub.status.busy": "2024-10-19T21:46:59.353575Z",
     "iopub.status.idle": "2024-10-19T21:46:59.363004Z",
     "shell.execute_reply": "2024-10-19T21:46:59.362159Z",
     "shell.execute_reply.started": "2024-10-19T21:46:59.353824Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, latent_dim, output_dim, hidden_dim):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, condition_dim, latent_dim, hidden_dim)\n",
    "        self.decoder = Decoder(latent_dim, condition_dim, output_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, condition, output_len):\n",
    "        # Encode\n",
    "        mu, log_var = self.encoder(x, condition)\n",
    "\n",
    "        # Reparameterization trick\n",
    "        z = reparameterize(mu, log_var)  # (B, latent_dim)\n",
    "\n",
    "        # Decode\n",
    "        output = self.decoder(z, condition, output_len)  # (B, output_len, output_dim * output_dim * 10)\n",
    "\n",
    "        return output, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:46:59.364799Z",
     "iopub.status.busy": "2024-10-19T21:46:59.364344Z",
     "iopub.status.idle": "2024-10-19T21:46:59.383018Z",
     "shell.execute_reply": "2024-10-19T21:46:59.382166Z",
     "shell.execute_reply.started": "2024-10-19T21:46:59.364754Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ARCModel(torch.nn.Module):\n",
    "    def __init__(self, llm_model, vllm_model, beta=1.0, gamma=1.0):\n",
    "        super().__init__()\n",
    "        self.llm_model = llm_model\n",
    "        self.vllm_model = vllm_model\n",
    "\n",
    "        self.llm_model.requires_grad_(False)\n",
    "        self.vllm_model.requires_grad_(False)\n",
    "\n",
    "        self.text_proj = nn.Linear(LLM_HIDDEN_SIZE, 2304, dtype=dtype)\n",
    "        self.image_proj = nn.Linear(VLLM_HIDDEN_SIZE, 2304, dtype=dtype)\n",
    "\n",
    "        self.cvae = CVAE(input_dim=2304, condition_dim=2304, latent_dim=512, output_dim=30, hidden_dim=1024)\n",
    "\n",
    "        self.output_dim = 30\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def to(self, device):\n",
    "        self.device = device\n",
    "        self.cvae.to(device)\n",
    "        self.text_proj.to(device)\n",
    "        self.image_proj.to(device)\n",
    "        return self\n",
    "\n",
    "    def to_inference(self):\n",
    "        self.llm_model.eval()\n",
    "        self.vllm_model.eval()\n",
    "\n",
    "    def to_training(self):\n",
    "        self.llm_model.train()\n",
    "        self.vllm_model.train()\n",
    "\n",
    "    # def cvae_loss(self, recon_x, x, mu, log_var, rows, cols):\n",
    "    #     recon_loss = F.binary_cross_entropy_with_logits(recon_x, x, reduction=\"sum\")\n",
    "    #     # KL Divergence loss\n",
    "    #     kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    #     return recon_loss + kl_loss\n",
    "\n",
    "    def cvae_loss(self, recon_x, x, mu, log_var, rows, cols):\n",
    "        \"\"\"\n",
    "        recon_x: (B, 30, 30, 10)\n",
    "        x: (B, 30, 30)\n",
    "        beta: weight for KL divergence loss\n",
    "        gamma: weight for dimensions loss\n",
    "        \"\"\"\n",
    "        B = recon_x.shape[0]\n",
    "\n",
    "        # Create a mask for the actual content area\n",
    "        mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "        for i in range(B):\n",
    "            mask[i, : rows[i], : cols[i]] = True\n",
    "\n",
    "        # Calculate reconstruction loss (normalized)\n",
    "        recon_loss = F.cross_entropy(recon_x.view(-1, 10), x.view(-1), reduction=\"mean\")\n",
    "\n",
    "        # Calculate dimensions loss (normalized)\n",
    "        dims_loss = (torch.sum(recon_x[mask].argmax(dim=-1) * x[mask]) - torch.sum(recon_x[~mask].argmax(dim=-1) * x[~mask])) / mask.sum()\n",
    "\n",
    "        # KL Divergence loss (scaled)\n",
    "        kl_loss = -0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "        # Combine losses with scaling factors\n",
    "        total_loss = recon_loss + self.beta * kl_loss + self.gamma * dims_loss\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def encode(self, text_inputs, image_inputs):\n",
    "        with torch.no_grad():\n",
    "            text_features = self.llm_model(**text_inputs.to(self.llm_model.device)).hidden_states[-1]  # (batch_size, seq_len, 3072)\n",
    "            image_features = self.vllm_model(**image_inputs.to(self.vllm_model.device)).hidden_states[-1]  # (batch_size, vid_len, 3584)\n",
    "\n",
    "        # -- todo: cleanup\n",
    "        text_inputs.to(\"cpu\")\n",
    "        image_inputs.to(\"cpu\")\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        # -- todo: cleanup\n",
    "\n",
    "        text_features = self.text_proj(text_features.to(self.device))\n",
    "        image_features = self.image_proj(image_features.to(self.device))\n",
    "\n",
    "        features = torch.cat([text_features, image_features], dim=1)  # (batch_size, seq_len + vid_len, 2304)\n",
    "        return features\n",
    "\n",
    "    def forward(self, train_inputs, test_inputs, targets=None):\n",
    "        train_features = self.encode(text_inputs=train_inputs[\"text\"], image_inputs=train_inputs[\"image\"])  # (B, seq_len + vid_len, 2304)\n",
    "        test_features = self.encode(text_inputs=test_inputs[\"text\"], image_inputs=test_inputs[\"image\"])  # (B, seq_len + vid_len, 2304)\n",
    "\n",
    "        outputs, mu, log_var = self.cvae(train_features, test_features, output_len=30)  # (B, cond_seq_len, 30)\n",
    "\n",
    "        B = outputs.shape[0]\n",
    "        outputs = outputs[:, 0, :].reshape(B, self.output_dim, self.output_dim, 10).cpu().float()\n",
    "\n",
    "        if targets is not None:\n",
    "            rows = targets[\"original_rows\"]\n",
    "            cols = targets[\"original_cols\"]\n",
    "            padded_matrices = targets[\"padded_matrix\"]\n",
    "\n",
    "            labels = torch.tensor(np.array(padded_matrices)).reshape(B, self.output_dim, self.output_dim)\n",
    "\n",
    "            loss = self.cvae_loss(outputs, labels, mu, log_var, rows, cols)\n",
    "            return {\"loss\": loss, \"outputs\": outputs, \"mu\": mu, \"log_var\": log_var}\n",
    "\n",
    "        # we will only take (B, 30, 30) for the loss calculation\n",
    "        return {\"loss\": None, \"outputs\": outputs, \"mu\": mu, \"log_var\": log_var}\n",
    "\n",
    "    def from_pretrained(self, path):\n",
    "        self.text_proj.load_state_dict(torch.load(f\"{path}/text_proj.pth\"))\n",
    "        self.image_proj.load_state_dict(torch.load(f\"{path}/image_proj.pth\"))\n",
    "        self.cvae.load_state_dict(torch.load(f\"{path}/cvae.pth\"))\n",
    "        return self\n",
    "\n",
    "    def save_pretrained(self, path):\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        torch.save(self.text_proj.state_dict(), f\"{path}/text_proj.pth\")\n",
    "        torch.save(self.image_proj.state_dict(), f\"{path}/image_proj.pth\")\n",
    "        torch.save(self.cvae.state_dict(), f\"{path}/cvae.pth\")\n",
    "        # Save any other non-llm and non-vllm weights here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:46:59.384565Z",
     "iopub.status.busy": "2024-10-19T21:46:59.384239Z",
     "iopub.status.idle": "2024-10-19T21:46:59.782468Z",
     "shell.execute_reply": "2024-10-19T21:46:59.781443Z",
     "shell.execute_reply.started": "2024-10-19T21:46:59.384527Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ARCModel(\n",
       "  (llm_model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(128256, 3072)\n",
       "      (layers): ModuleList(\n",
       "        (0-27): 28 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaFlashAttention2(\n",
       "            (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "  )\n",
       "  (vllm_model): Qwen2VLForConditionalGeneration(\n",
       "    (visual): Qwen2VisionTransformerPretrainedModel(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "      )\n",
       "      (rotary_pos_emb): VisionRotaryEmbedding()\n",
       "      (blocks): ModuleList(\n",
       "        (0-31): 32 x Qwen2VLVisionBlock(\n",
       "          (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): VisionFlashAttention2(\n",
       "            (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "            (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (mlp): VisionMlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): QuickGELUActivation()\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (merger): PatchMerger(\n",
       "        (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (model): Qwen2VLModel(\n",
       "      (embed_tokens): Embedding(151936, 1536)\n",
       "      (layers): ModuleList(\n",
       "        (0-27): 28 x Qwen2VLDecoderLayer(\n",
       "          (self_attn): Qwen2VLFlashAttention2(\n",
       "            (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "            (k_proj): QuantLinear()\n",
       "            (o_proj): QuantLinear()\n",
       "            (q_proj): QuantLinear()\n",
       "            (v_proj): QuantLinear()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (act_fn): SiLU()\n",
       "            (down_proj): QuantLinear()\n",
       "            (gate_proj): QuantLinear()\n",
       "            (up_proj): QuantLinear()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       "  )\n",
       "  (text_proj): Linear(in_features=3072, out_features=2304, bias=True)\n",
       "  (image_proj): Linear(in_features=1536, out_features=2304, bias=True)\n",
       "  (cvae): CVAE(\n",
       "    (encoder): Encoder(\n",
       "      (query): Linear(in_features=2304, out_features=1024, bias=True)\n",
       "      (key): Linear(in_features=2304, out_features=1024, bias=True)\n",
       "      (value): Linear(in_features=2304, out_features=1024, bias=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (fc1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (fc_mu): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (fc_var): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (fc1): Linear(in_features=2816, out_features=1024, bias=True)\n",
       "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (fc_output): Linear(in_features=1024, out_features=9000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arc_model = ARCModel(models[\"llm\"], models[\"vllm\"], gamma=0.5, beta=0.9)\n",
    "arc_model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 43,456,808\n"
     ]
    }
   ],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Count and print the number of trainable parameters\n",
    "trainable_params = count_trainable_parameters(arc_model)\n",
    "print(f\"Number of trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:46:59.793562Z",
     "iopub.status.busy": "2024-10-19T21:46:59.793032Z",
     "iopub.status.idle": "2024-10-19T21:46:59.807222Z",
     "shell.execute_reply": "2024-10-19T21:46:59.806264Z",
     "shell.execute_reply.started": "2024-10-19T21:46:59.793514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate(mode, tokenizer, processor):\n",
    "    def prepare_inputs(text_messages, image_messages):\n",
    "\n",
    "        def clean_none_values(messages):\n",
    "            return [{k: v for k, v in message.items() if v is not None} for message in messages]\n",
    "\n",
    "        image_messages = [[{**msg, \"content\": clean_none_values(msg[\"content\"])} for msg in msgs] for msgs in image_messages]\n",
    "\n",
    "        text_encodings = tokenizer.apply_chat_template(\n",
    "            text_messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=(mode not in [\"train\", \"val\"]),\n",
    "            return_tensors=\"pt\",\n",
    "            return_dict=True,\n",
    "            padding=True,\n",
    "        )\n",
    "\n",
    "        image_text = processor.apply_chat_template(image_messages, tokenize=False, add_generation_prompt=True)\n",
    "        image_inputs, _ = process_vision_info(image_messages)\n",
    "\n",
    "        image_encodings = processor(\n",
    "            text=image_text,\n",
    "            images=image_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return text_encodings, image_encodings\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        # Separate the different components of the batch\n",
    "        # For 'test' mode, remove the last assistant message from each entry\n",
    "        train_text_messages = [item[\"messages\"][\"train_text_messages\"] for item in batch]\n",
    "        train_image_messages = [item[\"messages\"][\"train_image_messages\"] for item in batch]\n",
    "\n",
    "        test_text_messages = [item[\"messages\"][\"test_text_messages\"] for item in batch]\n",
    "        test_image_messages = [item[\"messages\"][\"test_image_messages\"] for item in batch]\n",
    "\n",
    "        # Tokenize the texts\n",
    "        train_text_encodings, train_image_encodings = prepare_inputs(train_text_messages, train_image_messages)\n",
    "        test_text_encodings, test_image_encodings = prepare_inputs(test_text_messages, test_image_messages)\n",
    "\n",
    "        # If 'solution' is present (for training/validation data)\n",
    "        if \"solution\" in batch[0]:\n",
    "            return {\n",
    "                \"train_inputs\": {\"text\": train_text_encodings, \"image\": train_image_encodings},\n",
    "                \"test_inputs\": {\"text\": test_text_encodings, \"image\": test_image_encodings},\n",
    "                \"targets\": {\n",
    "                    \"padded_matrix\": [pad_matrix(item[\"solution\"], target_rows=30, target_cols=30) for item in batch],\n",
    "                    \"original_rows\": [len(item[\"solution\"]) for item in batch],\n",
    "                    \"original_cols\": [len(item[\"solution\"][0]) for item in batch],\n",
    "                },\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"train_inputs\": {\"text\": train_text_encodings, \"image\": train_image_encodings},\n",
    "                \"test_inputs\": {\"text\": test_text_encodings, \"image\": test_image_encodings},\n",
    "            }\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:46:59.808646Z",
     "iopub.status.busy": "2024-10-19T21:46:59.808286Z",
     "iopub.status.idle": "2024-10-19T21:47:00.079846Z",
     "shell.execute_reply": "2024-10-19T21:47:00.078832Z",
     "shell.execute_reply.started": "2024-10-19T21:46:59.808604Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inputs:\n",
      "  text:\n",
      "    input_ids:\n",
      "      torch.Size([1, 393])\n",
      "    attention_mask:\n",
      "      torch.Size([1, 393])\n",
      "  image:\n",
      "    input_ids:\n",
      "      torch.Size([1, 2824])\n",
      "    attention_mask:\n",
      "      torch.Size([1, 2824])\n",
      "    pixel_values:\n",
      "      torch.Size([10240, 1176])\n",
      "    image_grid_thw:\n",
      "      torch.Size([10, 3])\n",
      "test_inputs:\n",
      "  text:\n",
      "    input_ids:\n",
      "      torch.Size([1, 630])\n",
      "    attention_mask:\n",
      "      torch.Size([1, 630])\n",
      "  image:\n",
      "    input_ids:\n",
      "      torch.Size([1, 499])\n",
      "    attention_mask:\n",
      "      torch.Size([1, 499])\n",
      "    pixel_values:\n",
      "      torch.Size([1024, 1176])\n",
      "    image_grid_thw:\n",
      "      torch.Size([1, 3])\n",
      "targets:\n",
      "  padded_matrix:\n",
      "    List of length: 1\n",
      "      [[7 0 7 0 0 0 7 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [7 0 7 0 0 0 7 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [7 7 0 0 0 0 7 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [7 0 7 0 0 0 7 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [7 0 7 0 0 0 7 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [7 7 0 0 0 0 7 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [7 0 7 7 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [7 0 7 7 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [7 7 0 7 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "  original_rows:\n",
      "    List of length: 1\n",
      "      9\n",
      "  original_cols:\n",
      "    List of length: 1\n",
      "      9\n"
     ]
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset[\"train\"], batch_size=1, collate_fn=collate(mode=\"train\", tokenizer=models[\"tokenizer\"], processor=models[\"processor\"])\n",
    ")\n",
    "\n",
    "\n",
    "def print_recursive(obj, indent=0):\n",
    "    if isinstance(obj, torch.Tensor):\n",
    "        print(\"  \" * indent + str(obj.shape))\n",
    "    elif (\n",
    "        isinstance(obj, dict)\n",
    "        or isinstance(obj, transformers.tokenization_utils_base.BatchEncoding)\n",
    "        or isinstance(obj, transformers.feature_extraction_utils.BatchFeature)\n",
    "    ):\n",
    "        for key, value in obj.items():\n",
    "            print(\"  \" * indent + str(key) + \":\")\n",
    "            print_recursive(value, indent + 1)\n",
    "    elif isinstance(obj, list):\n",
    "        if len(obj) > 0 and isinstance(obj[0], list):\n",
    "            if len(obj[0]) > 0 and isinstance(obj[0][0], list):\n",
    "                print(\"  \" * indent + f\"List of length: {len(obj)}, {len(obj[0])}, {len(obj[0][0])}\")\n",
    "            else:\n",
    "                print(\"  \" * indent + f\"List of length: {len(obj)}, {len(obj[0])}\")\n",
    "        else:\n",
    "            print(\"  \" * indent + f\"List of length: {len(obj)}\")\n",
    "            print_recursive(obj[0], indent + 1)\n",
    "    else:\n",
    "        print(\"  \" * indent + str(obj))\n",
    "\n",
    "\n",
    "for batch in dataloader:\n",
    "    print_recursive(batch)\n",
    "    #     outputs = arc_model(**batch)\n",
    "    #     print('-'* 30)\n",
    "    #     print_recursive(outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:47:00.081562Z",
     "iopub.status.busy": "2024-10-19T21:47:00.081232Z",
     "iopub.status.idle": "2024-10-19T21:47:00.088702Z",
     "shell.execute_reply": "2024-10-19T21:47:00.087748Z",
     "shell.execute_reply.started": "2024-10-19T21:47:00.081526Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_partial_match(pred, label, masks):\n",
    "    # Convert inputs to numpy arrays if they're not already\n",
    "    pred = np.array(pred)\n",
    "    label = np.array(label)\n",
    "    masks = np.array(masks)\n",
    "\n",
    "    # Calculate the match only where mask is 1\n",
    "    matched = (pred == label) & (masks == 1)\n",
    "\n",
    "    # Sum of matched elements divided by sum of mask elements\n",
    "    return matched.sum() / masks.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:47:00.090186Z",
     "iopub.status.busy": "2024-10-19T21:47:00.089831Z",
     "iopub.status.idle": "2024-10-19T21:47:00.101595Z",
     "shell.execute_reply": "2024-10-19T21:47:00.100714Z",
     "shell.execute_reply.started": "2024-10-19T21:47:00.090148Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(pred, label, masks):\n",
    "    return (calculate_partial_match(pred, label, masks) == 1.0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:47:00.102965Z",
     "iopub.status.busy": "2024-10-19T21:47:00.102663Z",
     "iopub.status.idle": "2024-10-19T21:47:00.111059Z",
     "shell.execute_reply": "2024-10-19T21:47:00.110157Z",
     "shell.execute_reply.started": "2024-10-19T21:47:00.102932Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(outputs, labels, masks):\n",
    "    return {\n",
    "        \"accuracy\": calculate_accuracy(outputs, labels, masks),\n",
    "        \"partial_match\": calculate_partial_match(outputs, labels, masks),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:47:00.112788Z",
     "iopub.status.busy": "2024-10-19T21:47:00.112496Z",
     "iopub.status.idle": "2024-10-19T21:47:00.129812Z",
     "shell.execute_reply": "2024-10-19T21:47:00.128820Z",
     "shell.execute_reply.started": "2024-10-19T21:47:00.112755Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@train\n",
    "def training(model, tokenizer, processor, dataset, config):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset[\"train\"], batch_size=config[\"batch_size\"], collate_fn=collate(mode=\"train\", tokenizer=tokenizer, processor=processor), shuffle=True\n",
    "    )\n",
    "\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset[\"val\"], batch_size=config[\"batch_size\"], collate_fn=collate(mode=\"val\", tokenizer=tokenizer, processor=processor), shuffle=False\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"accuracy\": [], \"partial_match\": []}\n",
    "    # Calculate total number of training steps\n",
    "    total_steps = len(train_dataloader) * config[\"epochs\"]\n",
    "\n",
    "    print(f\"Total steps: {total_steps}\")\n",
    "\n",
    "    # Create the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=config[\"warmup_steps\"], num_training_steps=total_steps)\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        train_loss = 0\n",
    "        steps = 0\n",
    "        for batch in tqdm(train_dataloader, desc=\"Train Batches\", total=len(train_dataloader)):\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            loss = outputs[\"loss\"] / config[\"gradient_accumulation_steps\"]\n",
    "            loss.backward()\n",
    "\n",
    "            if (steps + 1) % config[\"gradient_accumulation_steps\"] == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item() * config[\"gradient_accumulation_steps\"]\n",
    "            steps += 1\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {train_loss / len(train_dataloader)}\")\n",
    "\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc=\"Val Batches\", total=len(val_dataloader)):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs[\"loss\"]\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # outputs (B, 900, 10)\n",
    "                B = outputs[\"outputs\"].shape[0]\n",
    "                pred = outputs[\"outputs\"].reshape(B, 30, 30, 10).argmax(dim=-1).numpy()  # (B, 30, 30)\n",
    "                labels = np.array(batch[\"targets\"][\"padded_matrix\"])\n",
    "                masks = np.array(\n",
    "                    [pad_matrix(np.ones((r, c)), 30, 30) for r, c in zip(batch[\"targets\"][\"original_rows\"], batch[\"targets\"][\"original_cols\"])]\n",
    "                )  # (B, 30, 30), where 1 is the original value and 0 is the padded value\n",
    "\n",
    "                metrics = compute_metrics(pred, labels, masks)\n",
    "\n",
    "                history[\"accuracy\"].append(metrics[\"accuracy\"])\n",
    "                history[\"partial_match\"].append(metrics[\"partial_match\"])\n",
    "\n",
    "        log.info(f\"Epoch {epoch + 1}, Train Loss: {train_loss / len(train_dataloader)}\", terminal=True)\n",
    "        log.info(f\"Epoch {epoch + 1}, Val Loss: {val_loss / len(val_dataloader)}\", terminal=True)\n",
    "        log.info(f\"Epoch {epoch + 1}, Accuracy: {np.mean(history['accuracy'])}\", terminal=True)\n",
    "        log.info(f\"Epoch {epoch + 1}, Partial Match: {np.mean(history['partial_match'])}\", terminal=True)\n",
    "        \n",
    "        model.save_pretrained(f\"models/checkpoints/arc-agi-llama-vllm-{epoch + 1}\")\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss / len(train_dataloader))\n",
    "        history[\"val_loss\"].append(val_loss / len(val_dataloader))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:47:00.135502Z",
     "iopub.status.busy": "2024-10-19T21:47:00.135105Z",
     "iopub.status.idle": "2024-10-19T21:47:00.140408Z",
     "shell.execute_reply": "2024-10-19T21:47:00.139486Z",
     "shell.execute_reply.started": "2024-10-19T21:47:00.135451Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = {\"epochs\": 50, \"batch_size\": 2, \"lr\": 5e-5, \"gradient_accumulation_steps\": 4, \"warmup_steps\": 200, \"weight_decay\": 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T21:47:00.141790Z",
     "iopub.status.busy": "2024-10-19T21:47:00.141504Z",
     "iopub.status.idle": "2024-10-19T22:03:52.189435Z",
     "shell.execute_reply": "2024-10-19T22:03:52.187819Z",
     "shell.execute_reply.started": "2024-10-19T21:47:00.141759Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 760400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Batches:   0%|          | 0/15208 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "Train Batches:  37%|███▋      | 5562/15208 [3:46:22<5:59:01,  2.23s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://192.168.1.206:7000/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "history = training(arc_model, models[\"tokenizer\"], models[\"processor\"], dataset, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@eval\n",
    "def evaluate(model, tokenizer, processor, dataset, config):\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset[\"test\"], batch_size=config[\"batch_size\"], collate_fn=collate(mode=\"test\", tokenizer=tokenizer, processor=processor)\n",
    "    )\n",
    "\n",
    "    history = {\"accuracy\": [], \"partial_match\": []}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Test Batches\", total=len(test_dataloader)):\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            B = outputs[\"outputs\"].shape[0]\n",
    "            pred = outputs[\"outputs\"].reshape(B, 30, 30, 10).argmax(dim=-1).numpy()  # (B, 30, 30)\n",
    "            labels = batch[\"targets\"][\"padded_matrix\"]\n",
    "            masks = np.array(\n",
    "                [pad_matrix(np.ones((r, c)), 30, 30) for r, c in zip(batch[\"targets\"][\"original_rows\"], batch[\"targets\"][\"original_cols\"])]\n",
    "            )  # (B, 30, 30), where 1 is the original value and 0 is the padded value\n",
    "\n",
    "            metrics = compute_metrics(pred, labels, masks)\n",
    "\n",
    "            history[\"accuracy\"].append(metrics[\"accuracy\"])\n",
    "            history[\"partial_match\"].append(metrics[\"partial_match\"])\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"batch_size\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Batches: 100%|██████████| 147/147 [06:40<00:00,  2.73s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'partial_match': [0.5643564356435643,\n",
       "  0.5576923076923077,\n",
       "  0.7714285714285715,\n",
       "  0.7358262967430639,\n",
       "  0.5055452865064695,\n",
       "  0.10833333333333334,\n",
       "  0.125,\n",
       "  0.6770186335403726,\n",
       "  0.5176715176715176,\n",
       "  0.3888888888888889,\n",
       "  0.19622641509433963,\n",
       "  0.365,\n",
       "  0.6245901639344262,\n",
       "  0.7062780269058296,\n",
       "  0.4280701754385965,\n",
       "  0.05825242718446602,\n",
       "  0.0901077375122429,\n",
       "  0.6677852348993288,\n",
       "  0.5446808510638298,\n",
       "  0.45977011494252873,\n",
       "  0.6578125,\n",
       "  0.6563573883161512,\n",
       "  0.6708860759493671,\n",
       "  0.12903225806451613,\n",
       "  0.328125,\n",
       "  0.44,\n",
       "  0.6647058823529411,\n",
       "  0.015277777777777777,\n",
       "  0.7682403433476395,\n",
       "  0.435,\n",
       "  0.38926174496644295,\n",
       "  0.39893617021276595,\n",
       "  0.040123456790123455,\n",
       "  0.5294117647058824,\n",
       "  0.6717557251908397,\n",
       "  0.6929824561403509,\n",
       "  0.48470209339774556,\n",
       "  0.7264,\n",
       "  0.5163934426229508,\n",
       "  0.058823529411764705,\n",
       "  0.08996212121212122,\n",
       "  0.5461538461538461,\n",
       "  0.6069364161849711,\n",
       "  0.17857142857142858,\n",
       "  0.6843853820598007,\n",
       "  0.5255972696245734,\n",
       "  0.46062567421790723,\n",
       "  0.12091503267973856,\n",
       "  0.5688405797101449,\n",
       "  0.32,\n",
       "  0.439873417721519,\n",
       "  0.5203252032520326,\n",
       "  0.5218295218295218,\n",
       "  0.4895833333333333,\n",
       "  0.3161764705882353,\n",
       "  0.006472491909385114,\n",
       "  0.47096774193548385,\n",
       "  0.7863501483679525,\n",
       "  0.275,\n",
       "  0.3888888888888889,\n",
       "  0.7834782608695652,\n",
       "  0.4721189591078067,\n",
       "  0.5625,\n",
       "  0.4382284382284382,\n",
       "  0.5948275862068966,\n",
       "  0.7410805300713558,\n",
       "  0.6812227074235808,\n",
       "  0.6418732782369146,\n",
       "  0.43564356435643564,\n",
       "  0.3684210526315789,\n",
       "  0.73992673992674,\n",
       "  0.2594859241126071,\n",
       "  0.5927672955974843,\n",
       "  0.4055299539170507,\n",
       "  0.2606060606060606,\n",
       "  0.28,\n",
       "  0.15151515151515152,\n",
       "  0.06896551724137931,\n",
       "  0.6927710843373494,\n",
       "  0.4657534246575342,\n",
       "  0.6374269005847953,\n",
       "  0.6341463414634146,\n",
       "  0.34591194968553457,\n",
       "  0.346875,\n",
       "  0.648854961832061,\n",
       "  0.20588235294117646,\n",
       "  0.40298507462686567,\n",
       "  0.3224181360201511,\n",
       "  0.5911949685534591,\n",
       "  0.3212669683257919,\n",
       "  0.44,\n",
       "  0.611,\n",
       "  0.7189542483660131,\n",
       "  0.49537037037037035,\n",
       "  0.5714285714285714,\n",
       "  0.30153846153846153,\n",
       "  0.75,\n",
       "  0.7091194968553459,\n",
       "  0.16615384615384615,\n",
       "  0.6848203939745076,\n",
       "  0.3192660550458716,\n",
       "  0.24444444444444444,\n",
       "  0.5146520146520146,\n",
       "  0.5138461538461538,\n",
       "  0.6224188790560472,\n",
       "  0.6865248226950355,\n",
       "  0.6278538812785388,\n",
       "  0.5993227990970654,\n",
       "  0.08605851979345955,\n",
       "  0.723404255319149,\n",
       "  0.49283667621776506,\n",
       "  0.08630952380952381,\n",
       "  0.6436781609195402,\n",
       "  0.7275862068965517,\n",
       "  0.6677316293929713,\n",
       "  0.39580209895052476,\n",
       "  0.1282051282051282,\n",
       "  0.7092555331991952,\n",
       "  0.12088888888888889,\n",
       "  0.03571428571428571,\n",
       "  0.13333333333333333,\n",
       "  0.3978201634877384,\n",
       "  0.6753623188405797,\n",
       "  0.3523809523809524,\n",
       "  0.25806451612903225,\n",
       "  0.12459016393442623,\n",
       "  0.03699284009546539,\n",
       "  0.3211805555555556,\n",
       "  0.48546511627906974,\n",
       "  0.28,\n",
       "  0.09090909090909091,\n",
       "  0.16260162601626016,\n",
       "  0.08904109589041095,\n",
       "  0.6453488372093024,\n",
       "  0.04794520547945205,\n",
       "  0.2379182156133829,\n",
       "  0.7056277056277056,\n",
       "  0.616580310880829,\n",
       "  0.23032739804709937,\n",
       "  0.7066246056782335,\n",
       "  0.13286713286713286,\n",
       "  0.436,\n",
       "  0.43859649122807015,\n",
       "  0.6666666666666666,\n",
       "  0.6164021164021164,\n",
       "  0.3772455089820359,\n",
       "  0.3462603878116344]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_history = evaluate(arc_model, models[\"tokenizer\"], models[\"processor\"], dataset, config)\n",
    "eval_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.4407089855812507)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(eval_history[\"accuracy\"]), np.mean(eval_history[\"partial_match\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8951125,
     "sourceId": 67357,
     "sourceType": "competition"
    },
    {
     "datasetId": 5754327,
     "sourceId": 9635165,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
