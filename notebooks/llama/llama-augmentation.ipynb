{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c9ea186d5046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b886cea3ea3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/home/stepan/kaggle-arc-agi\"\n",
    "MODEL_ID = f\"{BASE_PATH}/models/llama-3_2-3b-it\"\n",
    "MAX_NEW_TOKENS = 2048\n",
    "MAX_SEQ_LENGTH = 32768 - MAX_NEW_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d515f651d366f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(BASE_PATH)\n",
    "sys.path.append(f\"{BASE_PATH}/scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d053f55d36a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch  # type: ignore\n",
    "import numpy as np  # type: ignore\n",
    "\n",
    "from datasets import DatasetDict, Dataset  # type: ignore\n",
    "\n",
    "from unsloth import FastLanguageModel  # type: ignore\n",
    "\n",
    "from tqdm.auto import tqdm  # type: ignore\n",
    "\n",
    "from logger import get_logger  # type: ignore\n",
    "import train_utils  # type: ignore\n",
    "import data_utils  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31950c8f19e622c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(f\"{BASE_PATH}/logs/llama-3_2-3b-it\", \"arc-agi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ec45b558ea6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_tokenizer(dtype=None, load_in_4bit=True):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=MODEL_ID,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        device_map=\"auto\",\n",
    "        max_memory={0: \"23GiB\", \"cpu\": \"16GiB\"},\n",
    "    )\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da726f0b32fd2573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(f):\n",
    "    def wrapper(model, tokenizer, *args, **kwargs):\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        return f(model, tokenizer, *args, **kwargs)\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5c5c94712a79c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = get_model_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade7bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs(dct, prepare_solution=False):\n",
    "    if prepare_solution:\n",
    "        return \"<output>\\n\" + \"\\n\".join(\" \".join(map(str, row)) for row in dct) + \"\\n</output>\"\n",
    "    else:\n",
    "        input_str = \"\\n\".join(\" \".join(map(str, row)) for row in dct[\"input\"])\n",
    "        output_str = \"\\n\".join(\" \".join(map(str, row)) for row in dct[\"output\"]) if \"output\" in dct else \"\"\n",
    "        text = f\"<input>\\n{input_str}\\n</input>\\n\\n<output>\\n{output_str}\\n</output>\"\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c603253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_augmentation_dataset(data):\n",
    "    restructured_data = {\n",
    "        \"id\": [],\n",
    "        \"challenge\": [],\n",
    "        \"solution\": [],\n",
    "    }\n",
    "\n",
    "    for challenge_id, challenge_data in data.items():  # for all challenges\n",
    "        for train_id, task in enumerate(challenge_data[\"train\"]):\n",
    "            restructured_data[\"id\"].append(challenge_id)\n",
    "            restructured_data[\"challenge\"].append(\n",
    "                {\"train\": challenge_data[\"train\"][:train_id] + challenge_data[\"train\"][train_id + 1 :], \"test\": task, \"order\": train_id}\n",
    "            )\n",
    "            restructured_data[\"solution\"].append(task[\"output\"])\n",
    "\n",
    "    return Dataset.from_dict(restructured_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d425336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat(challenge, solution=None):\n",
    "    user_content = data_utils.BASIC_PROMPT.format(\n",
    "        training_data=\"\\n\\n\".join([prepare_inputs(ex) for ex in challenge[\"train\"]]),\n",
    "        input_test_data=prepare_inputs(challenge[\"test\"]),\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": data_utils.SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ]\n",
    "\n",
    "    if solution:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": prepare_inputs(solution, prepare_solution=True),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b620259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(tokenizer, base_path=None, final_training=False, create_chat_func=create_chat):\n",
    "    # The system_prompt defines the initial instructions for the model, setting the context for solving ARC tasks.\n",
    "    system_prompt = (\n",
    "        \"\"\"You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.\"\"\"\n",
    "    )\n",
    "\n",
    "    # Load all datasets\n",
    "    training_challenges = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_training_challenges.json\")\n",
    "    training_solutions = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_training_solutions.json\")\n",
    "    evaluation_challenges = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_evaluation_challenges.json\")\n",
    "    evaluation_solutions = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_evaluation_solutions.json\")\n",
    "    test_challenges = data_utils.load_data(f\"{base_path}/arc-prize-2024/arc-agi_test_challenges.json\")\n",
    "\n",
    "    train_dataset = to_augmentation_dataset(training_challenges)\n",
    "    eval_dataset = to_augmentation_dataset(evaluation_challenges)\n",
    "    pred_dataset = to_augmentation_dataset(test_challenges)\n",
    "\n",
    "    def process_dataset(examples, solutions=None):\n",
    "        # Create messages for each challenge-solution pair\n",
    "        chats = []\n",
    "        for challenge, solution in zip(examples[\"challenge\"], solutions or [None] * len(examples[\"challenge\"])):\n",
    "            chat = create_chat_func(challenge, solution)\n",
    "            chats.append(chat)\n",
    "\n",
    "        # Apply chat template to each message\n",
    "        texts = [tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False) for chat in chats]\n",
    "\n",
    "        return {\"texts\": texts, \"messages\": chats}\n",
    "\n",
    "    pred_dataset = pred_dataset.map(lambda x: process_dataset(x), batched=True)\n",
    "    train_dataset = train_dataset.map(lambda x: process_dataset(x, train_dataset[\"solution\"]), batched=True)\n",
    "    eval_dataset = eval_dataset.map(lambda x: process_dataset(x, eval_dataset[\"solution\"]), batched=True)\n",
    "\n",
    "    if final_training:  # if final training, we need to add the validation dataset to the training dataset\n",
    "        train_dataset = data_utils.concatenate_datasets([train_dataset, eval_dataset]).shuffle(seed=42)\n",
    "        return DatasetDict(\n",
    "            {\n",
    "                \"train\": train_dataset,\n",
    "                \"predict\": pred_dataset,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    dataset = DatasetDict(\n",
    "        {\n",
    "            \"train\": train_dataset,\n",
    "            \"test\": eval_dataset,\n",
    "            \"predict\": pred_dataset,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe44745c16e12d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = prepare_dataset(tokenizer, base_path=BASE_PATH, create_chat_func=create_chat)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13b3692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_temp(model, inputs, temperature):\n",
    "    outputs = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS, do_sample=True, temperature=temperature, top_k=50, use_cache=True)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def evaluate_batch(model, tokenizer, batch):\n",
    "    inputs = {\n",
    "        \"input_ids\": batch[\"input_ids\"],\n",
    "        \"attention_mask\": batch[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = generate_with_temp(model, inputs, 0.5)\n",
    "\n",
    "    input_ids_length = inputs[\"input_ids\"].shape[1]  # sequence length without new tokens\n",
    "    new_tokens = outputs[:, input_ids_length:]\n",
    "\n",
    "    generated_texts = tokenizer.batch_decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9868e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_correction(pred, label):\n",
    "    if pred is None:\n",
    "        return None, \"Output is not in the correct format\"\n",
    "\n",
    "    output = [[str(cell) for cell in row] for row in pred]\n",
    "\n",
    "    if pred == label:\n",
    "        return output, \"Output is correct\"\n",
    "\n",
    "    if len(pred) != len(label) or any(len(p) != len(l) for p, l in zip(pred, label)):\n",
    "        return output, \"Output shape is wrong\"\n",
    "\n",
    "    for i in range(len(pred)):\n",
    "        for j in range(len(pred[i])):\n",
    "            if pred[i][j] != label[i][j]:\n",
    "                output[i][j] = f\"({pred[i][j]})->({label[i][j]})\"\n",
    "\n",
    "    return output, \"Output has errors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217070e6743f193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@eval\n",
    "def predict(model, tokenizer, dataset, batch_size):\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=train_utils.collate(mode=\"test\", tokenizer=tokenizer),\n",
    "    )\n",
    "\n",
    "    challenge_ids = []\n",
    "    preds = []\n",
    "    for i, batch in tqdm(enumerate(eval_dataloader), total=len(eval_dataloader)):\n",
    "        generated_texts = evaluate_batch(model, tokenizer, batch)\n",
    "\n",
    "        ids = batch[\"id\"]\n",
    "        challenges = batch[\"challenge\"]\n",
    "\n",
    "        for gen_text, challenge_id, challenge in zip(generated_texts, ids, challenges):\n",
    "            parsed_output = train_utils.parse_output(gen_text)\n",
    "            preds.append(error_correction(parsed_output, challenge[\"test\"][\"output\"]))\n",
    "            challenge_ids.append((challenge_id, challenge[\"order\"]))\n",
    "    return {\"ids\": challenge_ids, \"preds\": preds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af75db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = predict(model, tokenizer, dataset[\"train\"], batch_size=1)\n",
    "train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638801bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = predict(model, tokenizer, dataset[\"test\"], batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a031f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_results(results):\n",
    "    ids = results[\"ids\"]\n",
    "    preds = results[\"preds\"]\n",
    "\n",
    "    output = {}\n",
    "    for id_order, pred in zip(ids, preds):\n",
    "        challenge_id, order = id_order\n",
    "        if challenge_id not in output:\n",
    "            output[challenge_id] = {}\n",
    "        output[challenge_id][order] = pred\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb120d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "with open(f\"{BASE_PATH}/data/train_corrections.json\", \"w\") as f:\n",
    "    json.dump(transform_results(train_results), f, indent=4)\n",
    "\n",
    "with open(f\"{BASE_PATH}/data/test_corrections.json\", \"w\") as f:\n",
    "    json.dump(transform_results(test_results), f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e072e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{BASE_PATH}/data/train_corrections.json\", \"r\") as f:\n",
    "    train_corrections = json.load(f)\n",
    "\n",
    "with open(f\"{BASE_PATH}/data/test_corrections.json\", \"r\") as f:\n",
    "    test_corrections = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f6e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count types of responses\n",
    "train_correction_counts = {}\n",
    "test_correction_counts = {}\n",
    "\n",
    "for challenge_id, corrections in train_corrections.items():\n",
    "    for order, correction in corrections.items():\n",
    "        train_correction_counts[correction[1]] = train_correction_counts.get(correction[1], 0) + 1\n",
    "\n",
    "for challenge_id, corrections in test_corrections.items():\n",
    "    for order, correction in corrections.items():\n",
    "        test_correction_counts[correction[1]] = test_correction_counts.get(correction[1], 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42606d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_correction_counts, test_correction_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432af727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find train instance with 30x30 input size:\n",
    "for i, train_instance in enumerate(dataset[\"train\"]):\n",
    "    if len(train_instance['challenge']['test']['output']) == 30 and len(train_instance['challenge']['test']['output'][0]) == 30:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813264e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89461f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d01b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]['challenge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c378dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_augmented_dataset(dataset, train_corrections, test_corrections):\n",
    "    train_augmented_dataset = []\n",
    "    test_augmented_dataset = []\n",
    "    for train_instance in dataset[\"train\"]:\n",
    "        challenge = train_instance['challenge'].copy()\n",
    "        challenge_id = train_instance['id']\n",
    "        order = challenge['order']\n",
    "        \n",
    "        correction = train_corrections[challenge_id][str(order)]\n",
    "        \n",
    "        challenge['correction'] = {}\n",
    "        challenge['correction']['output'] = correction[0]\n",
    "        challenge['correction']['message'] = correction[1]\n",
    "        \n",
    "        train_augmented_dataset.append({**train_instance, 'challenge': challenge})\n",
    "        \n",
    "    for test_instance in dataset[\"test\"]:\n",
    "        challenge = test_instance['challenge'].copy()\n",
    "        challenge_id = test_instance['id']\n",
    "        order = challenge['order']\n",
    "        \n",
    "        correction = test_corrections[challenge_id][str(order)]\n",
    "        \n",
    "        challenge['correction'] = {}\n",
    "        challenge['correction']['output'] = correction[0]\n",
    "        challenge['correction']['message'] = correction[1]\n",
    "\n",
    "        test_augmented_dataset.append({**test_instance, 'challenge': challenge})\n",
    "    \n",
    "    return DatasetDict({'train': Dataset.from_list(train_augmented_dataset), 'test': Dataset.from_list(test_augmented_dataset)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babe45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = build_augmented_dataset(dataset, train_corrections, test_corrections)\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccce744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_with_correction(challenge, solution=None):\n",
    "    user_content = data_utils.BASIC_PROMPT.format(\n",
    "        training_data=\"\\n\\n\".join([prepare_inputs(ex) for ex in challenge[\"train\"]]),\n",
    "        correction_message=challenge[\"correction\"][\"message\"], # TODO: add this to the prompt\n",
    "        input_test_data=prepare_inputs(challenge[\"test\"]),\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": data_utils.SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ]\n",
    "\n",
    "    if solution:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": prepare_inputs(solution, prepare_solution=True),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
