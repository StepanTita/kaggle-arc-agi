{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c9ea186d5046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b886cea3ea3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/home/stepan/kaggle-arc-agi\"\n",
    "MODEL_ID = f\"{BASE_PATH}/models/gemma-2-2b-it/checkpoint-500\"\n",
    "MAX_NEW_TOKENS = 2048\n",
    "MAX_SEQ_LENGTH = 8192 - MAX_NEW_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d515f651d366f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(BASE_PATH)\n",
    "sys.path.append(f\"{BASE_PATH}/scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d053f55d36a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # type: ignore\n",
    "import numpy as np  # type: ignore\n",
    "\n",
    "from datasets import DatasetDict, Dataset  # type: ignore\n",
    "\n",
    "from unsloth import FastLanguageModel  # type: ignore\n",
    "\n",
    "from tqdm.auto import tqdm  # type: ignore\n",
    "\n",
    "from logger import get_logger  # type: ignore\n",
    "import train_utils  # type: ignore\n",
    "import data_utils  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31950c8f19e622c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(f\"{BASE_PATH}/logs/gemma-2-2b\", \"arc-agi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ec45b558ea6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_tokenizer(dtype=None, load_in_4bit=True):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=MODEL_ID,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        device_map={\"\": 0},\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        # token = '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', # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    )\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da726f0b32fd2573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(f):\n",
    "    def wrapper(model, tokenizer, *args, **kwargs):\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        return f(model, tokenizer, *args, **kwargs)\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5c5c94712a79c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = get_model_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe44745c16e12d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_utils.prepare_dataset(tokenizer, fit_dataset=True, base_path=BASE_PATH)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13b3692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_temp(model, inputs, temperature):\n",
    "    outputs = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS, do_sample=True, temperature=temperature, top_k=50, use_cache=True)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def evaluate_batch(model, tokenizer, batch):\n",
    "    inputs = {\n",
    "        \"input_ids\": batch[\"input_ids\"],\n",
    "        \"attention_mask\": batch[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs1 = generate_with_temp(model, inputs, 0.3)\n",
    "        outputs2 = generate_with_temp(model, inputs, 0.7)\n",
    "\n",
    "    input_ids_length = inputs[\"input_ids\"].shape[1]  # sequence length without new tokens\n",
    "    new_tokens1 = outputs1[:, input_ids_length:]\n",
    "    new_tokens2 = outputs2[:, input_ids_length:]\n",
    "\n",
    "    generated_texts1 = tokenizer.batch_decode(new_tokens1, skip_special_tokens=True)\n",
    "    generated_texts2 = tokenizer.batch_decode(new_tokens2, skip_special_tokens=True)\n",
    "\n",
    "    return generated_texts1, generated_texts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217070e6743f193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@eval\n",
    "def evaluate(model, tokenizer, dataset, batch_size):\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=train_utils.collate(mode=\"test\", tokenizer=tokenizer),\n",
    "    )\n",
    "\n",
    "    challenge_ids = []\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for i, batch in tqdm(enumerate(eval_dataloader), total=len(eval_dataloader)):\n",
    "        generated_texts1, generated_texts2 = evaluate_batch(model, tokenizer, batch)\n",
    "\n",
    "        # Ensure solutions is always a list\n",
    "        ids = batch[\"id\"]\n",
    "        challenges = batch[\"challenge\"]\n",
    "        solutions = batch[\"solution\"]\n",
    "\n",
    "        # I don't like how complicated this is, but I don't see an easier way to do it right now\n",
    "        for gen_text1, gen_text2, label, challenge_id, challenge in zip(generated_texts1, generated_texts2, solutions, ids, challenges):\n",
    "            parsed_output1 = train_utils.parse_output(gen_text1)\n",
    "            parsed_output2 = train_utils.parse_output(gen_text2)\n",
    "\n",
    "            if parsed_output1 is None and parsed_output2 is None:\n",
    "                print(f\"Failed to parse both outputs: {gen_text1} and {gen_text2}\")\n",
    "                preds.append(None)\n",
    "            else:\n",
    "                # Choose the best prediction based on partial match score\n",
    "                score1 = train_utils.calculate_partial_match(parsed_output1, train_utils.tensor_to_int(label)) if parsed_output1 is not None else 0\n",
    "                score2 = train_utils.calculate_partial_match(parsed_output2, train_utils.tensor_to_int(label)) if parsed_output2 is not None else 0\n",
    "                best_pred = parsed_output1 if score1 >= score2 else parsed_output2\n",
    "                preds.append(best_pred)\n",
    "\n",
    "            labels.append(train_utils.tensor_to_int(label))\n",
    "            challenge_ids.append((challenge_id, challenge[\"order\"]))\n",
    "\n",
    "    return {\n",
    "        \"ids\": challenge_ids,\n",
    "        \"preds\": preds,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af75db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(model, tokenizer, dataset[\"test\"], batch_size=1)\n",
    "# Calculate metrics\n",
    "accuracy, avg_partial_match = train_utils.calculate_metrics(results[\"preds\"], results[\"labels\"])\n",
    "\n",
    "log.info(f\"Exact match accuracy: {accuracy:.4f}\")\n",
    "log.info(f\"Average partial match score: {avg_partial_match:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6109ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
