{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c420919fcdbe178",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# ! pip install pip3-autoremove\n",
    "# ! pip-autoremove torch torchvision torchaudio -y\n",
    "# ! pip install torch==2.3.0 xformers triton\n",
    "# ! pip install transformers==4.45.0\n",
    "# ! pip install unsloth==2024.9.post3\n",
    "# ! pip install --no-deps --upgrade \"flash-attn>=2.6.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c9ea186d5046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b886cea3ea3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/home/stepan/kaggle-arc-agi\"\n",
    "MODEL_ID = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
    "MAX_NEW_TOKENS = 4096\n",
    "MAX_SEQ_LENGTH = 32768 - MAX_NEW_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d515f651d366f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(BASE_PATH)\n",
    "sys.path.append(f\"{BASE_PATH}/scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d053f55d36a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # type: ignore\n",
    "import numpy as np  # type: ignore\n",
    "\n",
    "from datasets import DatasetDict, Dataset  # type: ignore\n",
    "\n",
    "from unsloth import FastLanguageModel  # type: ignore\n",
    "\n",
    "from tqdm.auto import tqdm  # type: ignore\n",
    "\n",
    "from trl import SFTTrainer  # type: ignore\n",
    "from transformers import TrainingArguments  # type: ignore\n",
    "from unsloth import is_bfloat16_supported  # type: ignore\n",
    "\n",
    "from logger import get_logger  # type: ignore\n",
    "import train_utils  # type: ignore\n",
    "import data_utils  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31950c8f19e622c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(f\"{BASE_PATH}/logs/llama-3_2-3b\", \"arc-agi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ec45b558ea6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_tokenizer(dtype=None, load_in_4bit=True, add_lora=False):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=MODEL_ID,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        device_map=\"auto\",\n",
    "        max_memory={0: \"23GiB\", \"cpu\": \"16GiB\"},\n",
    "    )\n",
    "\n",
    "    if add_lora:\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r=16,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "            target_modules=[\n",
    "                \"q_proj\",\n",
    "                \"k_proj\",\n",
    "                \"v_proj\",\n",
    "                \"o_proj\",\n",
    "                \"gate_proj\",\n",
    "                \"up_proj\",\n",
    "                \"down_proj\",\n",
    "            ],\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "            bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
    "            # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "            use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "            random_state=3407,\n",
    "            use_rslora=False,  # We support rank stabilized LoRA\n",
    "            loftq_config=None,  # And LoftQ\n",
    "        )\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da726f0b32fd2573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(f):\n",
    "    def wrapper(model, tokenizer, *args, **kwargs):\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        log.info(f\"Evaluating model {model}, tokenizer {tokenizer.padding_side}\")\n",
    "        return f(model, tokenizer, *args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def train(f):\n",
    "    def wrapper(model, tokenizer, *args, **kwargs):\n",
    "        FastLanguageModel.for_training(model)\n",
    "        return f(model, tokenizer, *args, **kwargs)\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5c5c94712a79c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = get_model_tokenizer(add_lora=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05b87ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs(dct, prepare_solution=False):\n",
    "    if prepare_solution:\n",
    "        return \"<output>\\n\" + \"\\n\".join(\" \".join(map(str, row)) for row in dct) + \"\\n</output>\"\n",
    "    else:\n",
    "        input_str = \"\\n\".join(\" \".join(map(str, row)) for row in dct[\"input\"])\n",
    "        output_str = \"\\n\".join(\" \".join(map(str, row)) for row in dct[\"output\"]) if \"output\" in dct else \"\"\n",
    "        text = f\"<input>\\n{input_str}\\n</input>\\n\\n<output>\\n{output_str}\\n</output>\"\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe44745c16e12d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_utils.prepare_dataset(tokenizer, fit_dataset=False, base_path=BASE_PATH, final_training=False, prepare_inputs_func=prepare_inputs)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f87107",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"train\"][0][\"texts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877bb5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate min, max, and avg number of training samples per dataset record for each split\n",
    "def calculate_training_samples_stats(dataset_split):\n",
    "    training_samples_counts = [len(challenge[\"train\"]) for challenge in dataset_split[\"challenge\"]]\n",
    "    return {\n",
    "        \"min\": min(training_samples_counts),\n",
    "        \"max\": max(training_samples_counts),\n",
    "        \"avg\": sum(training_samples_counts) / len(training_samples_counts),\n",
    "    }\n",
    "\n",
    "\n",
    "splits = [\"train\", \"val\", \"test\", \"predict\"]\n",
    "for split in splits:\n",
    "    if split not in dataset:\n",
    "        continue\n",
    "    stats = calculate_training_samples_stats(dataset[split])\n",
    "    print(f\"{split.capitalize()} split:\")\n",
    "    print(f\"  Min training samples: {stats['min']}\")\n",
    "    print(f\"  Max training samples: {stats['max']}\")\n",
    "    print(f\"  Avg training samples: {stats['avg']:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a769215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset_by_token_limit(dataset, max_seq_length):\n",
    "    def filter_split(split_name):\n",
    "        filtered_split = dataset[split_name].filter(lambda x: data_utils.count_tokens(tokenizer, x[\"texts\"]) <= max_seq_length)\n",
    "        filtered_out_tasks = len(dataset[split_name]) - len(filtered_split)\n",
    "        return filtered_split, filtered_out_tasks\n",
    "\n",
    "    filtered_splits = {}\n",
    "    for split in [\"train\", \"val\", \"test\", \"predict\"]:\n",
    "        if split not in dataset:\n",
    "            continue\n",
    "        filtered_splits[split], filtered_out_tasks = filter_split(split)\n",
    "        print(f\"{filtered_out_tasks} {split} tasks were filtered out because they exceed the {max_seq_length} token limit.\")\n",
    "        print(f\"The filtered {split} dataset contains {len(filtered_splits[split])} tasks for {'fine-tuning' if split == 'train' else 'evaluation'}.\")\n",
    "\n",
    "    return filtered_splits\n",
    "\n",
    "\n",
    "filtered_dataset = filter_dataset_by_token_limit(dataset, MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13b3692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_temp(model, inputs, temperature):\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_k=50,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def evaluate_batch(model, tokenizer, batch):\n",
    "    inputs = {\"input_ids\": batch[\"input_ids\"], \"attention_mask\": batch[\"attention_mask\"]}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs1 = generate_with_temp(model, inputs, 0.3)\n",
    "        outputs2 = generate_with_temp(model, inputs, 0.7)\n",
    "\n",
    "    input_ids_length = inputs[\"input_ids\"].shape[1]  # sequence length without new tokens\n",
    "    new_tokens1 = outputs1[:, input_ids_length:]\n",
    "    new_tokens2 = outputs2[:, input_ids_length:]\n",
    "\n",
    "    generated_texts1 = tokenizer.batch_decode(new_tokens1, skip_special_tokens=True)\n",
    "    generated_texts2 = tokenizer.batch_decode(new_tokens2, skip_special_tokens=True)\n",
    "\n",
    "    return generated_texts1, generated_texts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a1d37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@train\n",
    "def training(model, tokenizer, dataset, max_seq_length):\n",
    "    common_args = {\n",
    "        \"model\": model,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"train_dataset\": dataset[\"train\"],\n",
    "        \"dataset_text_field\": \"texts\",\n",
    "        \"max_seq_length\": max_seq_length,\n",
    "        \"dataset_num_proc\": 2,\n",
    "        \"packing\": False,\n",
    "    }\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        logging_steps=100,\n",
    "        warmup_steps=5,\n",
    "        max_steps=500,\n",
    "        learning_rate=2e-5,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=f\"{BASE_PATH}/models/llama-3_1-8b-it\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=250,\n",
    "        save_total_limit=2,\n",
    "    )\n",
    "\n",
    "    if \"val\" in dataset:\n",
    "        common_args[\"eval_dataset\"] = dataset[\"val\"]\n",
    "        training_args.per_device_eval_batch_size = 1\n",
    "        training_args.eval_strategy = \"steps\"\n",
    "        training_args.eval_steps = 100\n",
    "        training_args.metric_for_best_model = \"eval_loss\"\n",
    "        training_args.save_best_model = True\n",
    "\n",
    "    trainer = SFTTrainer(args=training_args, **common_args)\n",
    "    stats = trainer.train()\n",
    "    return trainer, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3a7382",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer, stats = training(model, tokenizer, dataset, max_seq_length=MAX_SEQ_LENGTH)\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932124f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(f\"{BASE_PATH}/models/llama-3_3-3b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72d26b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
