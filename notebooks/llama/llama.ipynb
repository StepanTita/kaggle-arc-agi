{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c420919fcdbe178",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# ! pip install pip3-autoremove\n",
    "# ! pip-autoremove torch torchvision torchaudio -y\n",
    "# ! pip install torch==2.3.0 xformers triton\n",
    "# ! pip install unsloth\n",
    "# ! pip install --no-deps --upgrade \"flash-attn>=2.6.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df5c9ea186d5046a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n",
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b6b886cea3ea3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/home/stepan/kaggle-arc-agi\"\n",
    "MODEL_ID = \"unsloth/Meta-Llama-3.1-8B-Instruct\"\n",
    "MAX_NEW_TOKENS = 8192\n",
    "MAX_SEQ_LENGTH = 32768 - MAX_NEW_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d515f651d366f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(BASE_PATH)\n",
    "sys.path.append(f\"{BASE_PATH}/scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148d053f55d36a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stepan/.conda/envs/llm-py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "import torch  # type: ignore\n",
    "import numpy as np  # type: ignore\n",
    "\n",
    "from datasets import DatasetDict, Dataset  # type: ignore\n",
    "\n",
    "from unsloth import FastLanguageModel  # type: ignore\n",
    "\n",
    "from tqdm.auto import tqdm  # type: ignore\n",
    "\n",
    "from trl import SFTTrainer  # type: ignore\n",
    "from transformers import TrainingArguments  # type: ignore\n",
    "from unsloth import is_bfloat16_supported  # type: ignore\n",
    "\n",
    "from logger import get_logger  # type: ignore\n",
    "import train_utils  # type: ignore\n",
    "import data_utils  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31950c8f19e622c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(f\"{BASE_PATH}/logs/llama-3_1-8b\", \"arc-agi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "988ec45b558ea6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_tokenizer(dtype=None, load_in_4bit=True, add_lora=False):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=MODEL_ID,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        device_map=\"auto\",\n",
    "        max_memory={0: \"23GiB\", \"cpu\": \"16GiB\"},\n",
    "    )\n",
    "\n",
    "    if add_lora:\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r=16,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "            target_modules=[\n",
    "                \"q_proj\",\n",
    "                \"k_proj\",\n",
    "                \"v_proj\",\n",
    "                \"o_proj\",\n",
    "                \"gate_proj\",\n",
    "                \"up_proj\",\n",
    "                \"down_proj\",\n",
    "            ],\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "            bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
    "            # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "            use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "            random_state=3407,\n",
    "            use_rslora=False,  # We support rank stabilized LoRA\n",
    "            loftq_config=None,  # And LoftQ\n",
    "        )\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da726f0b32fd2573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(f):\n",
    "    def wrapper(model, tokenizer, *args, **kwargs):\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        log.info(f\"Evaluating model {model}, tokenizer {tokenizer.padding_side}\")\n",
    "        return f(model, tokenizer, *args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def train(f):\n",
    "    def wrapper(model, tokenizer, *args, **kwargs):\n",
    "        FastLanguageModel.for_training(model)\n",
    "        return f(model, tokenizer, *args, **kwargs)\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb5c5c94712a79c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.9: Fast Llama patching. Transformers = 4.43.4.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.679 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.9 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_model_tokenizer(add_lora=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe44745c16e12d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:00<00:00, 1405.82 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 416/416 [00:00<00:00, 957.63 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:00<00:00, 950.46 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 416/416 [00:00<00:00, 1440.96 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:00<00:00, 948.86 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'challenge', 'solution', 'texts', 'messages'],\n",
       "        num_rows: 416\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'challenge', 'solution', 'texts', 'messages'],\n",
       "        num_rows: 293\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['id', 'challenge', 'solution', 'texts', 'messages'],\n",
       "        num_rows: 126\n",
       "    })\n",
       "    predict: Dataset({\n",
       "        features: ['id', 'challenge', 'texts', 'messages'],\n",
       "        num_rows: 105\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = data_utils.prepare_dataset(tokenizer, fit_dataset=False, base_path=BASE_PATH, final_training=False)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "877bb5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split:\n",
      "  Min training samples: 2\n",
      "  Max training samples: 10\n",
      "  Avg training samples: 3.35\n",
      "\n",
      "Val split:\n",
      "  Min training samples: 2\n",
      "  Max training samples: 7\n",
      "  Avg training samples: 3.48\n",
      "\n",
      "Test split:\n",
      "  Min training samples: 2\n",
      "  Max training samples: 7\n",
      "  Avg training samples: 3.47\n",
      "\n",
      "Predict split:\n",
      "  Min training samples: 2\n",
      "  Max training samples: 8\n",
      "  Avg training samples: 3.34\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate min, max, and avg number of training samples per dataset record for each split\n",
    "def calculate_training_samples_stats(dataset_split):\n",
    "    training_samples_counts = [len(challenge[\"train\"]) for challenge in dataset_split[\"challenge\"]]\n",
    "    return {\n",
    "        \"min\": min(training_samples_counts),\n",
    "        \"max\": max(training_samples_counts),\n",
    "        \"avg\": sum(training_samples_counts) / len(training_samples_counts),\n",
    "    }\n",
    "\n",
    "\n",
    "splits = [\"train\", \"val\", \"test\", \"predict\"]\n",
    "for split in splits:\n",
    "    if split not in dataset:\n",
    "        continue\n",
    "    stats = calculate_training_samples_stats(dataset[split])\n",
    "    print(f\"{split.capitalize()} split:\")\n",
    "    print(f\"  Min training samples: {stats['min']}\")\n",
    "    print(f\"  Max training samples: {stats['max']}\")\n",
    "    print(f\"  Avg training samples: {stats['avg']:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a769215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 416/416 [00:00<00:00, 815.47 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train tasks were filtered out because they exceed the 24576 token limit.\n",
      "The filtered train dataset contains 416 tasks for fine-tuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:00<00:00, 604.41 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 val tasks were filtered out because they exceed the 24576 token limit.\n",
      "The filtered val dataset contains 126 tasks for evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 293/293 [00:00<00:00, 449.63 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 test tasks were filtered out because they exceed the 24576 token limit.\n",
      "The filtered test dataset contains 293 tasks for evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:00<00:00, 829.03 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 predict tasks were filtered out because they exceed the 24576 token limit.\n",
      "The filtered predict dataset contains 105 tasks for evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def filter_dataset_by_token_limit(dataset, max_seq_length):\n",
    "    def filter_split(split_name):\n",
    "        filtered_split = dataset[split_name].filter(lambda x: data_utils.count_tokens(tokenizer, x[\"texts\"]) <= max_seq_length)\n",
    "        filtered_out_tasks = len(dataset[split_name]) - len(filtered_split)\n",
    "        return filtered_split, filtered_out_tasks\n",
    "\n",
    "    filtered_splits = {}\n",
    "    for split in [\"train\", \"val\", \"test\", \"predict\"]:\n",
    "        if split not in dataset:\n",
    "            continue\n",
    "        filtered_splits[split], filtered_out_tasks = filter_split(split)\n",
    "        print(f\"{filtered_out_tasks} {split} tasks were filtered out because they exceed the {max_seq_length} token limit.\")\n",
    "        print(f\"The filtered {split} dataset contains {len(filtered_splits[split])} tasks for {'fine-tuning' if split == 'train' else 'evaluation'}.\")\n",
    "\n",
    "    return filtered_splits\n",
    "\n",
    "\n",
    "filtered_dataset = filter_dataset_by_token_limit(dataset, MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b13b3692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_temp(model, inputs, temperature):\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_k=50,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def evaluate_batch(model, tokenizer, batch):\n",
    "    inputs = {\"input_ids\": batch[\"input_ids\"], \"attention_mask\": batch[\"attention_mask\"]}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs1 = generate_with_temp(model, inputs, 0.3)\n",
    "        outputs2 = generate_with_temp(model, inputs, 0.7)\n",
    "\n",
    "    input_ids_length = inputs[\"input_ids\"].shape[1]  # sequence length without new tokens\n",
    "    new_tokens1 = outputs1[:, input_ids_length:]\n",
    "    new_tokens2 = outputs2[:, input_ids_length:]\n",
    "\n",
    "    generated_texts1 = tokenizer.batch_decode(new_tokens1, skip_special_tokens=True)\n",
    "    generated_texts2 = tokenizer.batch_decode(new_tokens2, skip_special_tokens=True)\n",
    "\n",
    "    return generated_texts1, generated_texts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9a1d37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@train\n",
    "def training(model, tokenizer, dataset, max_seq_length):\n",
    "    common_args = {\n",
    "        \"model\": model,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"train_dataset\": dataset[\"train\"],\n",
    "        \"dataset_text_field\": \"texts\",\n",
    "        \"max_seq_length\": max_seq_length,\n",
    "        \"dataset_num_proc\": 2,\n",
    "        \"packing\": False,\n",
    "    }\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        logging_steps=100,\n",
    "        warmup_steps=5,\n",
    "        max_steps=500,\n",
    "        learning_rate=2e-5,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=f\"{BASE_PATH}/models/llama-3_1-8b-it\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=250,\n",
    "        save_total_limit=2,\n",
    "    )\n",
    "\n",
    "    if \"val\" in dataset:\n",
    "        common_args[\"eval_dataset\"] = dataset[\"val\"]\n",
    "        training_args.per_device_eval_batch_size = 1\n",
    "        training_args.eval_strategy = \"steps\"\n",
    "        training_args.eval_steps = 100\n",
    "        training_args.metric_for_best_model = \"eval_loss\"\n",
    "        training_args.save_best_model = True\n",
    "\n",
    "    trainer = SFTTrainer(args=training_args, **common_args)\n",
    "    stats = trainer.train()\n",
    "    return trainer, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff3a7382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 416/416 [00:00<00:00, 456.79 examples/s]\n",
      "Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:00<00:00, 142.25 examples/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 416 | Num Epochs = 10\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 8 | Total steps = 500\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 52:01, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.898200</td>\n",
       "      <td>0.703220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.673624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.496600</td>\n",
       "      <td>0.655589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.473700</td>\n",
       "      <td>0.648164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.452800</td>\n",
       "      <td>0.646578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.5692619857788086, metrics={'train_runtime': 3129.3461, 'train_samples_per_second': 1.278, 'train_steps_per_second': 0.16, 'total_flos': 1.2146171904833126e+17, 'train_loss': 0.5692619857788086, 'epoch': 9.615384615384615})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer, stats = training(model, tokenizer, dataset, max_seq_length=MAX_SEQ_LENGTH)\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "932124f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(f\"{BASE_PATH}/models/llama-3_1-8b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72d26b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
