{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df5c9ea186d5046a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n",
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b6b886cea3ea3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/home/stepan/kaggle-arc-agi'\n",
    "MODEL_ID = '/home/stepan/kaggle-arc-agi/models/gemma-2-9b-it/checkpoint-750'\n",
    "MAX_NEW_TOKENS = 2048\n",
    "MAX_SEQ_LENGTH = 8192 - MAX_NEW_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d515f651d366f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(BASE_PATH)\n",
    "sys.path.append(f\"{BASE_PATH}/scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148d053f55d36a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stepan/.conda/envs/llm-py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "import torch  # type: ignore\n",
    "import numpy as np  # type: ignore\n",
    "\n",
    "from datasets import DatasetDict, Dataset  # type: ignore\n",
    "\n",
    "from unsloth import FastLanguageModel  # type: ignore\n",
    "\n",
    "from tqdm.auto import tqdm  # type: ignore\n",
    "\n",
    "from logger import get_logger  # type: ignore\n",
    "import train_utils  # type: ignore\n",
    "import data_utils  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31950c8f19e622c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(f\"{BASE_PATH}/logs/gemma-2-9b\", \"arc-agi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "988ec45b558ea6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_tokenizer(dtype=None, load_in_4bit=True):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=MODEL_ID,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        device_map={\"\": 0},\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        # token = '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', # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    )\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da726f0b32fd2573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(f):\n",
    "    def wrapper(model, tokenizer, *args, **kwargs):\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        return f(model, tokenizer, *args, **kwargs)\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb5c5c94712a79c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.9: Fast Gemma2 patching. Transformers = 4.43.4.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.679 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.9 patched 42 layers with 42 QKV layers, 42 O layers and 42 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_model_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe44745c16e12d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 430/430 [00:00<00:00, 1397.47 examples/s]\n",
      "Map: 100%|██████████| 112/112 [00:00<00:00, 495.15 examples/s]\n",
      "Map: 100%|██████████| 459/459 [00:00<00:00, 929.61 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'challenge', 'solution', 'texts', 'messages'],\n",
       "        num_rows: 430\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'challenge', 'solution', 'texts', 'messages'],\n",
       "        num_rows: 321\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['id', 'challenge', 'solution', 'texts', 'messages'],\n",
       "        num_rows: 138\n",
       "    })\n",
       "    predict: Dataset({\n",
       "        features: ['id', 'challenge', 'texts', 'messages'],\n",
       "        num_rows: 112\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = data_utils.prepare_dataset(tokenizer, fit_dataset=True, base_path=BASE_PATH)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b13b3692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_temp(model, inputs, temperature):\n",
    "    outputs = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS, do_sample=True, temperature=temperature, top_k=50, use_cache=True)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def evaluate_batch(model, tokenizer, batch):\n",
    "    inputs = {\n",
    "        \"input_ids\": batch[\"input_ids\"],\n",
    "        \"attention_mask\": batch[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs1 = generate_with_temp(model, inputs, 0.3)\n",
    "        outputs2 = generate_with_temp(model, inputs, 0.7)\n",
    "\n",
    "    input_ids_length = inputs[\"input_ids\"].shape[1]  # sequence length without new tokens\n",
    "    new_tokens1 = outputs1[:, input_ids_length:]\n",
    "    new_tokens2 = outputs2[:, input_ids_length:]\n",
    "\n",
    "    generated_texts1 = tokenizer.batch_decode(new_tokens1, skip_special_tokens=True)\n",
    "    generated_texts2 = tokenizer.batch_decode(new_tokens2, skip_special_tokens=True)\n",
    "\n",
    "    return generated_texts1, generated_texts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "217070e6743f193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@eval\n",
    "def evaluate(model, tokenizer, dataset, batch_size):\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=train_utils.collate(mode=\"test\", tokenizer=tokenizer),\n",
    "    )\n",
    "\n",
    "    challenge_ids = []\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for i, batch in tqdm(enumerate(eval_dataloader), total=len(eval_dataloader)):\n",
    "        generated_texts1, generated_texts2 = evaluate_batch(model, tokenizer, batch)\n",
    "\n",
    "        # Ensure solutions is always a list\n",
    "        ids = batch[\"id\"]\n",
    "        challenges = batch[\"challenge\"]\n",
    "        solutions = batch[\"solution\"]\n",
    "\n",
    "        # I don't like how complicated this is, but I don't see an easier way to do it right now\n",
    "        for gen_text1, gen_text2, label, challenge_id, challenge in zip(generated_texts1, generated_texts2, solutions, ids, challenges):\n",
    "            parsed_output1 = train_utils.parse_output(gen_text1)\n",
    "            parsed_output2 = train_utils.parse_output(gen_text2)\n",
    "\n",
    "            if parsed_output1 is None and parsed_output2 is None:\n",
    "                print(f\"Failed to parse both outputs: {gen_text1} and {gen_text2}\")\n",
    "                preds.append(None)\n",
    "            else:\n",
    "                # Choose the best prediction based on partial match score\n",
    "                score1 = train_utils.calculate_partial_match(parsed_output1, train_utils.tensor_to_int(label)) if parsed_output1 is not None else 0\n",
    "                score2 = train_utils.calculate_partial_match(parsed_output2, train_utils.tensor_to_int(label)) if parsed_output2 is not None else 0\n",
    "                best_pred = parsed_output1 if score1 >= score2 else parsed_output2\n",
    "                preds.append(best_pred)\n",
    "\n",
    "            labels.append(train_utils.tensor_to_int(label))\n",
    "            challenge_ids.append((challenge_id, challenge[\"order\"]))\n",
    "\n",
    "    return {\n",
    "        \"ids\": challenge_ids,\n",
    "        \"preds\": preds,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78e85f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.\n",
      "\n",
      "Here are the example input and output pairs from which you should learn the underlying rule to later predict the output for the given test input:\n",
      "-----------------\n",
      "<input>\n",
      "0001000000\n",
      "0011100000\n",
      "0000006000\n",
      "6600006600\n",
      "6600000000\n",
      "0011100000\n",
      "0111100000\n",
      "0000000000\n",
      "0000000000\n",
      "0000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "0000000000\n",
      "0000000000\n",
      "0000000000\n",
      "0000000000\n",
      "0000000000\n",
      "6000100000\n",
      "6601110000\n",
      "0000000000\n",
      "6600111000\n",
      "6601111000\n",
      "</output>\n",
      "\n",
      "<input>\n",
      "0000000000\n",
      "0000000000\n",
      "0333300333\n",
      "0330000000\n",
      "0330444000\n",
      "0000044000\n",
      "0000040040\n",
      "0000000040\n",
      "0000000440\n",
      "0000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "0000000000\n",
      "0000000000\n",
      "0000000000\n",
      "0000004000\n",
      "0000004000\n",
      "3330044000\n",
      "0000000000\n",
      "3333044400\n",
      "3300004400\n",
      "3300004000\n",
      "</output>\n",
      "\n",
      "<input>\n",
      "0000000000\n",
      "8800004000\n",
      "8000444400\n",
      "8880004000\n",
      "0000000000\n",
      "0000044440\n",
      "0008800000\n",
      "0088880000\n",
      "0000000000\n",
      "0000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "0000000000\n",
      "0000000000\n",
      "0000000000\n",
      "0000000000\n",
      "0880000000\n",
      "8888044440\n",
      "0000000000\n",
      "8800000400\n",
      "8000044440\n",
      "8880000400\n",
      "</output>\n",
      "\n",
      "<input>\n",
      "0000000000\n",
      "0770009990\n",
      "7777009990\n",
      "0770000000\n",
      "0000000000\n",
      "0099999000\n",
      "0000000000\n",
      "0000007770\n",
      "0000007770\n",
      "0000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "0000000000\n",
      "0000000000\n",
      "0000000000\n",
      "0000000000\n",
      "7770000000\n",
      "7770000000\n",
      "0000099900\n",
      "0770099900\n",
      "7777000000\n",
      "0770099999\n",
      "</output>\n",
      "-----------------\n",
      "Now, solve the following puzzle based on its input grid by applying the rules you have learned from the training data.:\n",
      "-----------------\n",
      "<input>\n",
      "0000000000\n",
      "2222003330\n",
      "0002000000\n",
      "0022000000\n",
      "0000000000\n",
      "0003300000\n",
      "0333302220\n",
      "0003002220\n",
      "0000000000\n",
      "0000000000\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "\n",
      "</output>\n",
      "-----------------\n",
      "What is the output grid? Only provide the output grid in the form as in the example input and output pairs. Do not provide any additional information:<end_of_turn>\n",
      "<start_of_turn>model\n",
      "<output>\n",
      "0000000000\n",
      "0000000000\n",
      "0000000000\n",
      "0000000000\n",
      "2220000000\n",
      "2220033300\n",
      "0000000000\n",
      "2222000330\n",
      "0002033330\n",
      "0022000300\n",
      "</output><end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"test\"][0]['texts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af75db5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 229/321 [2:25:05<1:50:09, 71.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to parse both outputs: <output>\n",
      "683338183881631231113122682386\n",
      "166881336388211368831218238882\n",
      "883631683863128182132111111836\n",
      "131281822213183631131111111263\n",
      "812386888888888813811111118368\n",
      "613186888888888811316111116283\n",
      "333383888888888813116288888813\n",
      "331321888888888881132632188828\n",
      "183283888888888868882831336131\n",
      "812316888888888883866183328881\n",
      "331128888888888883818186281116\n",
      "611216832281638263213338383112\n",
      "132812881322223338321166383316\n",
      "812631328133818366831813323388\n",
      "881328131131183883333138288331\n",
      "683281311311838863183888116331\n",
      "381182332816166813183888881163\n",
      "881236331883368181223838826861\n",
      "318233633631131181318281121338\n",
      "888128831133336333333138288128\n",
      "683812838318331333333838383311\n",
      "816312881138238333333838383313\n",
      "212218631388822333333881311336\n",
      "113663228263862333333881311326\n",
      "136183213313838333333883833112\n",
      "883882222886833333333822161868\n",
      "331118833328233333333822811218\n",
      "663336338888136833811161262638\n",
      "128321333313866181621633383818\n",
      "862638288311611218121318363338\n",
      "</output and <output>\n",
      "683338183881631231113122682386\n",
      "166881336388211368831218238882\n",
      "883631683863128182132111111836\n",
      "131281822213183631131111111263\n",
      "812386888888888813811111118368\n",
      "613186888888888811316111116283\n",
      "333383888888888813116288888813\n",
      "331321888888888881132632188828\n",
      "183283888888888868882831336131\n",
      "812316888888888838661383328881\n",
      "331128888888888838181862811163\n",
      "611216832281638263213361286222\n",
      "132812881322223338321166383316\n",
      "812631328133818366831813323336\n",
      "881328112121811333333138288131\n",
      "683281311311838833333833136328\n",
      "381182332816166133333162838113\n",
      "881236331883368181223838826861\n",
      "318233633631131181318281121338\n",
      "888128831133336333333838383313\n",
      "683812838318331333333838383311\n",
      "816312881138238333333833838311\n",
      "212218631388822333333822161336\n",
      "113663228263862333333811338333\n",
      "136183213313838333333883833812\n",
      "883882222886833333333221618686\n",
      "331118833328233333333228112188\n",
      "663336338888136833333112811263\n",
      "128321333313866181621633383818\n",
      "862638288311611218121318363338\n",
      "</output<unused93>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 320/321 [3:20:38<01:17, 77.52s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to parse both outputs: <output>\n",
      "000000000333333300000000000000\n",
      "000000000333333300000000000000\n",
      "000000000033333000000000000000\n",
      "000000000003333300000000000000\n",
      "000444400333333300000000000000\n",
      "000444400333333300000000000000\n",
      "000444400033333300000000000000\n",
      "000444400333333300000000000000\n",
      "000000000333333300000000000000\n",
      "000000000333333300000000000000\n",
      "000000000333333300000000000000\n",
      "000000000333333000000000000000\n",
      "000000000333333300000000000000\n",
      "600000000033333300000000000000\n",
      "660000000033333330000000000000\n",
      "666000000333333330000000000000\n",
      "666600000333333330000002222000\n",
      "000000000333333000000002222000\n",
      "000000000033333300000002200000\n",
      "000000000003333330000002200000\n",
      "000000000033333333000002000000\n",
      "000000000333333333000002000000\n",
      "000000000333333333000002000000\n",
      "000000000333333333000000000000\n",
      "000000000033333333000000000000\n",
      "000000000333333333000000000000\n",
      "000000000333333333000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "000000000333333330000000000000\n",
      "00000000033333333000000000000 and <output>\n",
      "000000000333333300000000000000\n",
      "000000000333333300000000000000\n",
      "000000000033333000000000000000\n",
      "000000000003333300000000000000\n",
      "000444400333333300000000000000\n",
      "000444400333333300000000000000\n",
      "000444400033333000000000000000\n",
      "000444400003333300000000000000\n",
      "000000000003333300000000000000\n",
      "000000000003333300000000000000\n",
      "000000000003333300000000000000\n",
      "000000000003333300000000000000\n",
      "000000000003333300000000000000\n",
      "600000000003333300000000000000\n",
      "660000000003333330000000000000\n",
      "666000000333333330000000000000\n",
      "666600000333333330000002222200\n",
      "000000000333333000000002222200\n",
      "000000000033333000000002200200\n",
      "000000000003333300000002000200\n",
      "000000000033333300000000000200\n",
      "000000000333333300000000000000\n",
      "000000000333333300000000000000\n",
      "000000000333333300000000000000\n",
      "000000000033333300000001111100\n",
      "00000000033333330000011111100\n",
      "00000000033333330000011331100\n",
      "00000000033333330000011331100\n",
      "000000000033333000000000000000\n",
      "000000000033333000000000000000\n",
      "000000000003333300000000000000\n",
      "000000000033333300000000000000\n",
      "000000000033333300000000000000\n",
      "000000000033333300000000000000\n",
      "000000000033333300000000000000\n",
      "</output>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [3:21:23<00:00, 37.64s/it]\n",
      "Exact match accuracy: 0.0654\n",
      "Average partial match score: 0.6283\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(model, tokenizer, dataset[\"test\"], batch_size=1)\n",
    "# Calculate metrics\n",
    "accuracy, avg_partial_match = train_utils.calculate_metrics(results[\"preds\"], results[\"labels\"])\n",
    "\n",
    "log.info(f\"Exact match accuracy: {accuracy:.4f}\")\n",
    "log.info(f\"Average partial match score: {avg_partial_match:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6109ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58414d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
