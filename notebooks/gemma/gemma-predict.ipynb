{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c9ea186d5046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b886cea3ea3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/home/stepan/kaggle-arc-agi\"\n",
    "MODEL_ID = f\"{BASE_PATH}/models/gemma-2-2b-it/checkpoint-500\"\n",
    "MAX_NEW_TOKENS = 2048\n",
    "MAX_SEQ_LENGTH = 8192 - MAX_NEW_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d515f651d366f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(BASE_PATH)\n",
    "sys.path.append(f\"{BASE_PATH}/scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d053f55d36a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch  # type: ignore\n",
    "import numpy as np  # type: ignore\n",
    "\n",
    "from datasets import DatasetDict, Dataset  # type: ignore\n",
    "\n",
    "from unsloth import FastLanguageModel  # type: ignore\n",
    "\n",
    "from tqdm.auto import tqdm  # type: ignore\n",
    "\n",
    "from logger import get_logger  # type: ignore\n",
    "\n",
    "from logger import get_logger  # type: ignore\n",
    "import train_utils  # type: ignore\n",
    "import data_utils  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31950c8f19e622c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(f\"{BASE_PATH}/logs/gemma-2-2b\", \"arc-agi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ec45b558ea6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_tokenizer(max_seq_length=1024, dtype=None, load_in_4bit=True):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=f\"{BASE_PATH}/models/gemma-2-2b-it/baseline\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        device_map={\"\": 0},\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        # token = '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', # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    )\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da726f0b32fd2573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(f):\n",
    "    def wrapper(model, tokenizer, *args, **kwargs):\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        return f(model, tokenizer, *args, **kwargs)\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fadbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NEW_TOKENS = 2048\n",
    "MAX_SEQ_LENGTH = 8192 - MAX_NEW_TOKENS\n",
    "MAX_SEQ_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5c5c94712a79c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = get_model_tokenizer(max_seq_length=MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe44745c16e12d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_utils.prepare_dataset(tokenizer, fit_dataset=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13b3692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_temp(model, inputs, temperature):\n",
    "    outputs = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS, do_sample=True, temperature=temperature, top_k=50, use_cache=True)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def evaluate_batch(model, tokenizer, batch):\n",
    "    inputs = {\"input_ids\": batch[\"input_ids\"], \"attention_mask\": batch[\"attention_mask\"]}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs1 = generate_with_temp(model, inputs, 0.3)\n",
    "        outputs2 = generate_with_temp(model, inputs, 0.7)\n",
    "\n",
    "    input_ids_length = inputs[\"input_ids\"].shape[1]  # sequence length without new tokens\n",
    "    new_tokens1 = outputs1[:, input_ids_length:]\n",
    "    new_tokens2 = outputs2[:, input_ids_length:]\n",
    "\n",
    "    generated_texts1 = tokenizer.batch_decode(new_tokens1, skip_special_tokens=True)\n",
    "    generated_texts2 = tokenizer.batch_decode(new_tokens2, skip_special_tokens=True)\n",
    "\n",
    "    return generated_texts1, generated_texts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca19bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@eval\n",
    "def predict(model, tokenizer, dataset, batch_size):\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=train_utils.collate(mode=\"predict\", tokenizer=tokenizer),\n",
    "    )\n",
    "\n",
    "    challenge_ids = []\n",
    "    preds = []\n",
    "    for i, batch in tqdm(enumerate(eval_dataloader), total=len(eval_dataloader)):\n",
    "        generated_texts1, generated_texts2 = evaluate_batch(model, tokenizer, batch)\n",
    "\n",
    "        ids = batch[\"id\"]\n",
    "        challenges = batch[\"challenge\"]\n",
    "\n",
    "        for gen_text1, gen_text2, challenge_id, challenge in zip(generated_texts1, generated_texts2, ids, challenges):\n",
    "            parsed_output1 = train_utils.parse_output(gen_text1)\n",
    "            parsed_output2 = train_utils.parse_output(gen_text2)\n",
    "\n",
    "            if parsed_output1 is None and parsed_output2 is None:\n",
    "                print(f\"Failed to parse both outputs: {gen_text1} and {gen_text2}\")\n",
    "                preds.append({\"attempt_1\": [[0]], \"attempt_2\": [[0]]})\n",
    "            else:\n",
    "                parsed_output1 = parsed_output1 if parsed_output1 is not None else [[0]]\n",
    "                parsed_output2 = parsed_output2 if parsed_output2 is not None else [[0]]\n",
    "                preds.append({\"attempt_1\": parsed_output1, \"attempt_2\": parsed_output2})\n",
    "            challenge_ids.append((challenge_id, challenge[\"order\"]))\n",
    "    return {\"ids\": challenge_ids, \"preds\": preds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925bbd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_preds_by_challenge_id(challenge_ids, preds):\n",
    "    grouped_preds = {}\n",
    "    for (challenge_id, order), pred in zip(challenge_ids, preds):\n",
    "        if challenge_id not in grouped_preds:\n",
    "            grouped_preds[challenge_id] = []\n",
    "\n",
    "        # Check if we already have a prediction for this order\n",
    "        existing_pred = next((p for p in grouped_preds[challenge_id] if p[0] == order), None)\n",
    "\n",
    "        if existing_pred:\n",
    "            # If we have a duplicate (same id and order), choose any (here, we keep the first one)\n",
    "            continue\n",
    "        else:\n",
    "            # Add the new prediction with its order\n",
    "            grouped_preds[challenge_id].append((order, pred))\n",
    "\n",
    "    # Sort predictions by order for each challenge_id\n",
    "    for challenge_id in grouped_preds:\n",
    "        grouped_preds[challenge_id].sort(key=lambda x: x[0])\n",
    "        # Remove the order information, keeping only the predictions\n",
    "        grouped_preds[challenge_id] = [pred for _, pred in grouped_preds[challenge_id]]\n",
    "\n",
    "    return grouped_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704423ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = predict(model, tokenizer, dataset[\"predict\"], batch_size=1)\n",
    "grouped_preds = group_preds_by_challenge_id(pred_results[\"ids\"], pred_results[\"preds\"])\n",
    "grouped_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8b69d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(grouped_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01752611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare solutions with sample_submission.json\n",
    "with open(f\"{BASE_PATH}/arc-prize-2024/sample_submission.json\", \"r\") as json_file:\n",
    "    sample_submission = json.load(json_file)\n",
    "\n",
    "# Check if all challenge_ids in sample_submission are in grouped_preds, and all tests have correct number of predictions\n",
    "# also check if all predictions are 2d matrices of at least 1x1 size\n",
    "for challenge_id in sample_submission:\n",
    "    if challenge_id not in grouped_preds:\n",
    "        print(f\"Challenge ID {challenge_id} in sample_submission is not in grouped_preds.\")\n",
    "    elif len(grouped_preds[challenge_id]) != len(sample_submission[challenge_id]):\n",
    "        print(\n",
    "            f\"Challenge ID {challenge_id} in sample_submission has {len(sample_submission[challenge_id])} predictions, but grouped_preds has {len(grouped_preds[challenge_id])}.\"\n",
    "        )\n",
    "\n",
    "    for pred in grouped_preds[challenge_id]:\n",
    "        if not isinstance(pred, dict):\n",
    "            print(f\"Challenge ID {challenge_id} in sample_submission has invalid predictions: {pred}\")\n",
    "            continue\n",
    "        if not isinstance(pred[\"attempt_1\"], list) or not isinstance(pred[\"attempt_2\"], list):\n",
    "            print(f\"Challenge ID {challenge_id} in sample_submission has invalid predictions: {pred}\")\n",
    "        if pred[\"attempt_1\"] is None or pred[\"attempt_2\"] is None:\n",
    "            print(f\"Challenge ID {challenge_id} in sample_submission has invalid predictions: {pred}\")\n",
    "        elif pred[\"attempt_1\"] is None or len(pred[\"attempt_1\"]) < 1 or len(pred[\"attempt_1\"][0]) < 1:\n",
    "            print(f\"Challenge ID {challenge_id} in sample_submission has invalid predictions: {pred['attempt_1']}\")\n",
    "        elif pred[\"attempt_2\"] is None or len(pred[\"attempt_2\"]) < 1 or len(pred[\"attempt_2\"][0]) < 1:\n",
    "            print(f\"Challenge ID {challenge_id} in sample_submission has invalid predictions: {pred['attempt_2']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6109ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"submission.json\", \"w\") as json_file:\n",
    "    json.dump(grouped_preds, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e004a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
