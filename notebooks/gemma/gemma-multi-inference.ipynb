{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n",
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/home/stepan/kaggle-arc-agi\"\n",
    "MODEL_ID = f\"{BASE_PATH}/models/gemma-2-2b-it/checkpoint-500\"\n",
    "MAX_NEW_TOKENS = 2048\n",
    "MAX_SEQ_LENGTH = 8192 - MAX_NEW_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(BASE_PATH)\n",
    "sys.path.append(f\"{BASE_PATH}/scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stepan/.conda/envs/llm-py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch  # type: ignore\n",
    "import numpy as np  # type: ignore\n",
    "\n",
    "from datasets import DatasetDict, Dataset  # type: ignore\n",
    "\n",
    "from tqdm.auto import tqdm  # type: ignore\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig  # type: ignore\n",
    "\n",
    "from logger import get_logger  # type: ignore\n",
    "import train_utils  # type: ignore\n",
    "import data_utils  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(f\"{BASE_PATH}/logs/gemma-2-2b\", \"arc-agi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_tokenizer(load_in_4bit=True):\n",
    "    quantization_config = BitsAndBytesConfig(load_in_4bit=load_in_4bit)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        quantization_config=quantization_config,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map={\"\": 0},\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/home/stepan/.conda/envs/llm-py310/lib/python3.10/site-packages/transformers/quantizers/auto.py:174: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2FlashAttention2(\n",
       "          (q_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=2304, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=9216, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (up_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=9216, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (down_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=9216, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=2304, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm()\n",
       "        (post_attention_layernorm): Gemma2RMSNorm()\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm()\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tokenizer = get_model_tokenizer()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 430/430 [00:00<00:00, 1405.09 examples/s]\n",
      "Map: 100%|██████████| 112/112 [00:00<00:00, 1483.87 examples/s]\n",
      "Map: 100%|██████████| 459/459 [00:00<00:00, 818.90 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'challenge', 'solution', 'texts', 'messages'],\n",
       "        num_rows: 430\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'challenge', 'solution', 'texts', 'messages'],\n",
       "        num_rows: 321\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['id', 'challenge', 'solution', 'texts', 'messages'],\n",
       "        num_rows: 138\n",
       "    })\n",
       "    predict: Dataset({\n",
       "        features: ['id', 'challenge', 'texts', 'messages'],\n",
       "        num_rows: 112\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = data_utils.prepare_dataset(tokenizer, fit_dataset=True, base_path=BASE_PATH)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_batch(model, tokenizer, batch, num_seq=5):\n",
    "    inputs = {\"input_ids\": batch[\"input_ids\"], \"attention_mask\": batch[\"attention_mask\"]}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=True,\n",
    "            use_cache=True,\n",
    "            num_beams=5,\n",
    "            num_return_sequences=num_seq,\n",
    "            temperature=0.5,\n",
    "            top_k=50\n",
    "        )\n",
    "\n",
    "    input_ids_length = inputs[\"input_ids\"].shape[1]  # sequence length without new tokens\n",
    "    new_tokens = outputs[:, input_ids_length:]\n",
    "\n",
    "    generated_texts = tokenizer.batch_decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sequences(generated_texts, num_seq):\n",
    "    parsed_outputs = [train_utils.parse_output(text) for text in generated_texts]\n",
    "    res = []\n",
    "    for i in range(0, len(parsed_outputs), num_seq):\n",
    "        options = [opt for opt in parsed_outputs[i : i + num_seq] if opt is not None]\n",
    "        if not options:\n",
    "            res.append((None, None))\n",
    "            continue\n",
    "\n",
    "        # Group options by their structure (rows x columns)\n",
    "        structure_groups = {}\n",
    "        for option in options:\n",
    "            rows = len(option)\n",
    "            cols = len(option[0]) if rows > 0 else 0\n",
    "            structure = (rows, cols)\n",
    "            if structure not in structure_groups:\n",
    "                structure_groups[structure] = []\n",
    "            structure_groups[structure].append(option)\n",
    "\n",
    "        # Select the group with the most options\n",
    "        most_common_structure = max(structure_groups, key=lambda x: len(structure_groups[x]))\n",
    "        selected_options = structure_groups[most_common_structure]\n",
    "\n",
    "        # Get dimensions of the most common structure\n",
    "        rows, cols = most_common_structure\n",
    "\n",
    "        # Perform element-wise voting\n",
    "        voted_option = [[None for _ in range(cols)] for _ in range(rows)]\n",
    "        for row in range(rows):\n",
    "            for col in range(cols):\n",
    "                elements = [option[row][col] for option in selected_options]\n",
    "                voted_option[row][col] = max(set(elements), key=elements.count)\n",
    "\n",
    "        # Select the top 2 options based on similarity to the voted option\n",
    "        def similarity_score(option):\n",
    "            return sum(option[r][c] == voted_option[r][c] for r in range(rows) for c in range(cols))\n",
    "\n",
    "        top_2_options = sorted(selected_options, key=similarity_score, reverse=True)[:2]\n",
    "        res.append(tuple(top_2_options))  # TODO this or top2 + voted\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, dataset, batch_size, num_seq=5):\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=False, collate_fn=train_utils.collate(mode=\"test\", tokenizer=tokenizer)\n",
    "    )\n",
    "\n",
    "    challenge_ids = []\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for i, batch in tqdm(enumerate(eval_dataloader), total=len(eval_dataloader)):\n",
    "        generated_texts = evaluate_batch(model, tokenizer, batch, num_seq=num_seq)  # (batch_size * num_return_sequences, seq_len)\n",
    "\n",
    "        # Ensure solutions is always a list\n",
    "        ids = batch[\"id\"]\n",
    "        challenges = batch[\"challenge\"]\n",
    "        solutions = batch[\"solution\"]\n",
    "\n",
    "        processed_outputs = process_sequences(generated_texts, num_seq)\n",
    "\n",
    "        # I don't like how complicated this is, but I don't see an easier way to do it right now\n",
    "        for (parsed_output1, parsed_output2), label, challenge_id, challenge in zip(processed_outputs, solutions, ids, challenges):\n",
    "\n",
    "            if parsed_output1 is None or parsed_output2 is None:\n",
    "                preds.append(None)\n",
    "            else:\n",
    "                # Choose the best prediction based on partial match score\n",
    "                score1 = train_utils.calculate_partial_match(parsed_output1, train_utils.tensor_to_int(label)) if parsed_output1 is not None else 0\n",
    "                score2 = train_utils.calculate_partial_match(parsed_output2, train_utils.tensor_to_int(label)) if parsed_output2 is not None else 0\n",
    "                best_pred = parsed_output1 if score1 >= score2 else parsed_output2\n",
    "                preds.append(best_pred)\n",
    "\n",
    "            labels.append(train_utils.tensor_to_int(label))\n",
    "            challenge_ids.append((challenge_id, challenge[\"order\"]))\n",
    "\n",
    "    return {\n",
    "        \"ids\": challenge_ids,\n",
    "        \"preds\": preds,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/321 [08:38<22:59:16, 259.42s/it]\n",
      "Exact match accuracy: 0.0000\n",
      "Average partial match score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(model, tokenizer, dataset[\"test\"], batch_size=1, num_seq=3)\n",
    "# Calculate metrics\n",
    "accuracy, avg_partial_match = train_utils.calculate_metrics(results[\"preds\"], results[\"labels\"])\n",
    "\n",
    "log.info(f\"Exact match accuracy: {accuracy:.4f}\")\n",
    "log.info(f\"Average partial match score: {avg_partial_match:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
