{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/home/stepan/kaggle-arc-agi\"\n",
    "MODEL_ID = f\"{BASE_PATH}/models/gemma-2-2b-it/checkpoint-500\"\n",
    "MAX_NEW_TOKENS = 2048\n",
    "MAX_SEQ_LENGTH = 8192 - MAX_NEW_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(BASE_PATH)\n",
    "sys.path.append(f\"{BASE_PATH}/scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # type: ignore\n",
    "import numpy as np  # type: ignore\n",
    "\n",
    "from datasets import DatasetDict, Dataset  # type: ignore\n",
    "\n",
    "from tqdm.auto import tqdm  # type: ignore\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig  # type: ignore\n",
    "\n",
    "from logger import get_logger  # type: ignore\n",
    "import train_utils  # type: ignore\n",
    "import data_utils  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(f\"{BASE_PATH}/logs/gemma-2-2b\", \"arc-agi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_tokenizer(load_in_4bit=True):\n",
    "    quantization_config = BitsAndBytesConfig(load_in_4bit=load_in_4bit)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        quantization_config=quantization_config,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map={\"\": 0},\n",
    "    )\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = get_model_tokenizer()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_utils.prepare_dataset(tokenizer, fit_dataset=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_batch(model, tokenizer, batch, num_seq=5):\n",
    "    inputs = {\"input_ids\": batch[\"input_ids\"], \"attention_mask\": batch[\"attention_mask\"]}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=True,\n",
    "            use_cache=True,\n",
    "            num_beams=5,\n",
    "            num_return_sequences=num_seq,\n",
    "            temperature=0.5,\n",
    "            top_k=50\n",
    "        )\n",
    "\n",
    "    input_ids_length = inputs[\"input_ids\"].shape[1]  # sequence length without new tokens\n",
    "    new_tokens = outputs[:, input_ids_length:]\n",
    "\n",
    "    generated_texts = tokenizer.batch_decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sequences(generated_texts, num_seq):\n",
    "    parsed_outputs = [train_utils.parse_output(text) for text in generated_texts]\n",
    "    res = []\n",
    "    for i in range(0, len(parsed_outputs), num_seq):\n",
    "        options = [opt for opt in parsed_outputs[i : i + num_seq] if opt is not None]\n",
    "        if not options:\n",
    "            res.append((None, None))\n",
    "            continue\n",
    "\n",
    "        # Group options by their structure (rows x columns)\n",
    "        structure_groups = {}\n",
    "        for option in options:\n",
    "            rows = len(option)\n",
    "            cols = len(option[0]) if rows > 0 else 0\n",
    "            structure = (rows, cols)\n",
    "            if structure not in structure_groups:\n",
    "                structure_groups[structure] = []\n",
    "            structure_groups[structure].append(option)\n",
    "\n",
    "        # Select the group with the most options\n",
    "        most_common_structure = max(structure_groups, key=lambda x: len(structure_groups[x]))\n",
    "        selected_options = structure_groups[most_common_structure]\n",
    "\n",
    "        # Get dimensions of the most common structure\n",
    "        rows, cols = most_common_structure\n",
    "\n",
    "        # Perform element-wise voting\n",
    "        voted_option = [[None for _ in range(cols)] for _ in range(rows)]\n",
    "        for row in range(rows):\n",
    "            for col in range(cols):\n",
    "                elements = [option[row][col] for option in selected_options]\n",
    "                voted_option[row][col] = max(set(elements), key=elements.count)\n",
    "\n",
    "        # Select the top 2 options based on similarity to the voted option\n",
    "        def similarity_score(option):\n",
    "            return sum(option[r][c] == voted_option[r][c] for r in range(rows) for c in range(cols))\n",
    "\n",
    "        top_2_options = sorted(selected_options, key=similarity_score, reverse=True)[:2]\n",
    "        res.append(tuple(top_2_options))  # TODO this or top2 + voted\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, dataset, batch_size, num_seq=5):\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=False, collate_fn=train_utils.collate(mode=\"test\", tokenizer=tokenizer)\n",
    "    )\n",
    "\n",
    "    challenge_ids = []\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for i, batch in tqdm(enumerate(eval_dataloader), total=len(eval_dataloader)):\n",
    "        generated_texts = evaluate_batch(model, tokenizer, batch, num_seq=num_seq)  # (batch_size * num_return_sequences, seq_len)\n",
    "\n",
    "        # Ensure solutions is always a list\n",
    "        ids = batch[\"id\"]\n",
    "        challenges = batch[\"challenge\"]\n",
    "        solutions = batch[\"solution\"]\n",
    "\n",
    "        processed_outputs = process_sequences(generated_texts, num_seq)\n",
    "\n",
    "        # I don't like how complicated this is, but I don't see an easier way to do it right now\n",
    "        for (parsed_output1, parsed_output2), label, challenge_id, challenge in zip(processed_outputs, solutions, ids, challenges):\n",
    "\n",
    "            if parsed_output1 is None or parsed_output2 is None:\n",
    "                preds.append(None)\n",
    "            else:\n",
    "                # Choose the best prediction based on partial match score\n",
    "                score1 = train_utils.calculate_partial_match(parsed_output1, train_utils.tensor_to_int(label)) if parsed_output1 is not None else 0\n",
    "                score2 = train_utils.calculate_partial_match(parsed_output2, train_utils.tensor_to_int(label)) if parsed_output2 is not None else 0\n",
    "                best_pred = parsed_output1 if score1 >= score2 else parsed_output2\n",
    "                preds.append(best_pred)\n",
    "\n",
    "            labels.append(train_utils.tensor_to_int(label))\n",
    "            challenge_ids.append((challenge_id, challenge[\"order\"]))\n",
    "\n",
    "        if i % 2 == 0 and i > 0:\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"ids\": challenge_ids,\n",
    "        \"preds\": preds,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(model, tokenizer, dataset[\"test\"], batch_size=1)\n",
    "# Calculate metrics\n",
    "accuracy, avg_partial_match = train_utils.calculate_metrics(results[\"preds\"], results[\"labels\"])\n",
    "\n",
    "log.info(f\"Exact match accuracy: {accuracy:.4f}\")\n",
    "log.info(f\"Average partial match score: {avg_partial_match:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
