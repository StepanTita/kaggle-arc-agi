{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c420919fcdbe178",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# ! pip install pip3-autoremove\n",
    "# ! pip-autoremove torch torchvision torchaudio -y\n",
    "# ! pip install torch==2.3.0 xformers triton\n",
    "# ! pip install unsloth\n",
    "# ! pip install --no-deps --upgrade \"flash-attn>=2.6.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df5c9ea186d5046a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b6b886cea3ea3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/home/stepan/kaggle-arc-agi\"\n",
    "MODEL_ID = f\"{BASE_PATH}/models/gemma-2-2b-it/checkpoint-500\"\n",
    "MAX_NEW_TOKENS = 2048\n",
    "MAX_SEQ_LENGTH = 8192 - MAX_NEW_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d515f651d366f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(BASE_PATH)\n",
    "sys.path.append(f'{BASE_PATH}/scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "148d053f55d36a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "\n",
    "from datasets import DatasetDict, Dataset # type: ignore\n",
    "\n",
    "from unsloth import FastLanguageModel # type: ignore\n",
    "\n",
    "from tqdm.auto import tqdm # type: ignore\n",
    "\n",
    "from trl import SFTTrainer # type: ignore\n",
    "from transformers import TrainingArguments # type: ignore\n",
    "from unsloth import is_bfloat16_supported # type: ignore\n",
    "\n",
    "from logger import get_logger # type: ignore\n",
    "import train_utils  # type: ignore\n",
    "import data_utils  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31950c8f19e622c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(f'{BASE_PATH}/logs/gemma-2-2b', 'arc-agi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8599147d39964c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "988ec45b558ea6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_tokenizer(dtype=None, load_in_4bit=True, add_lora=False):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"unsloth/gemma-2-2b-it\",\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        device_map={'': 0},\n",
    "        attn_implementation='flash_attention_2',\n",
    "        # token = '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', # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    )\n",
    "\n",
    "    if add_lora:\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r=16,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                            \"gate_proj\", \"up_proj\", \"down_proj\", ],\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "            bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
    "            # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "            use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "            random_state=3407,\n",
    "            use_rslora=False,  # We support rank stabilized LoRA\n",
    "            loftq_config=None,  # And LoftQ\n",
    "        )\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da726f0b32fd2573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(f):\n",
    "    def wrapper(model, tokenizer, *args, **kwargs):\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        log.info(f\"Evaluating model {model}, tokenizer {tokenizer.padding_side}\")\n",
    "        return f(model, tokenizer, *args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def train(f):\n",
    "    def wrapper(model, tokenizer, *args, **kwargs):\n",
    "        FastLanguageModel.for_training(model)\n",
    "        log.info(f\"Training model {model}, tokenizer {tokenizer.padding_side}\")\n",
    "        return f(model, tokenizer, *args, **kwargs)\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb5c5c94712a79c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: If you want to finetune Gemma 2, install flash-attn to make it faster!\n",
      "To install flash-attn, do the below:\n",
      "\n",
      "pip install --no-deps --upgrade \"flash-attn>=2.6.3\"\n",
      "==((====))==  Unsloth 2024.8: Fast Gemma2 patching. Transformers = 4.43.4.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.677 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 26 layers with 26 QKV layers, 26 O layers and 26 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_model_tokenizer(add_lora=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe44745c16e12d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 430/430 [00:00<00:00, 963.16 examples/s]\n",
      "Map: 100%|██████████| 112/112 [00:00<00:00, 1520.09 examples/s]\n",
      "Map: 100%|██████████| 459/459 [00:00<00:00, 988.85 examples/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'challenge', 'solution', 'texts', 'messages'],\n",
       "        num_rows: 430\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'challenge', 'solution', 'texts', 'messages'],\n",
       "        num_rows: 321\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['id', 'challenge', 'solution', 'texts', 'messages'],\n",
       "        num_rows: 138\n",
       "    })\n",
       "    predict: Dataset({\n",
       "        features: ['id', 'challenge', 'texts', 'messages'],\n",
       "        num_rows: 112\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = data_utils.prepare_dataset(tokenizer, fit_dataset=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "877bb5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split:\n",
      "  Min training samples: 1\n",
      "  Max training samples: 10\n",
      "  Avg training samples: 3.24\n",
      "\n",
      "Val split:\n",
      "  Min training samples: 1\n",
      "  Max training samples: 7\n",
      "  Avg training samples: 3.16\n",
      "\n",
      "Test split:\n",
      "  Min training samples: 1\n",
      "  Max training samples: 7\n",
      "  Avg training samples: 3.18\n",
      "\n",
      "Predict split:\n",
      "  Min training samples: 1\n",
      "  Max training samples: 8\n",
      "  Avg training samples: 3.13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate min, max, and avg number of training samples per dataset record for each split\n",
    "def calculate_training_samples_stats(dataset_split):\n",
    "    training_samples_counts = [len(challenge['train']) for challenge in dataset_split['challenge']]\n",
    "    return {\n",
    "        'min': min(training_samples_counts),\n",
    "        'max': max(training_samples_counts),\n",
    "        'avg': sum(training_samples_counts) / len(training_samples_counts)\n",
    "    }\n",
    "\n",
    "splits = ['train', 'val', 'test', 'predict']\n",
    "for split in splits:\n",
    "    stats = calculate_training_samples_stats(dataset[split])\n",
    "    print(f\"{split.capitalize()} split:\")\n",
    "    print(f\"  Min training samples: {stats['min']}\")\n",
    "    print(f\"  Max training samples: {stats['max']}\")\n",
    "    print(f\"  Avg training samples: {stats['avg']:.2f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a769215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 430/430 [00:00<00:00, 978.28 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train tasks were filtered out because they exceed the 6144 token limit.\n",
      "The filtered train dataset contains 430 tasks for fine-tuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 138/138 [00:00<00:00, 680.78 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 val tasks were filtered out because they exceed the 6144 token limit.\n",
      "The filtered val dataset contains 138 tasks for evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 321/321 [00:00<00:00, 678.82 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 test tasks were filtered out because they exceed the 6144 token limit.\n",
      "The filtered test dataset contains 321 tasks for evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 112/112 [00:00<00:00, 1034.29 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 predict tasks were filtered out because they exceed the 6144 token limit.\n",
      "The filtered predict dataset contains 112 tasks for evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def filter_dataset_by_token_limit(dataset, max_seq_length):\n",
    "    def filter_split(split_name):\n",
    "        filtered_split = dataset[split_name].filter(lambda x: data_utils.count_tokens(tokenizer, x['texts']) <= max_seq_length)\n",
    "        filtered_out_tasks = len(dataset[split_name]) - len(filtered_split)\n",
    "        return filtered_split, filtered_out_tasks\n",
    "\n",
    "    filtered_splits = {}\n",
    "    for split in ['train', 'val', 'test', 'predict']:\n",
    "        filtered_splits[split], filtered_out_tasks = filter_split(split)\n",
    "        print(f\"{filtered_out_tasks} {split} tasks were filtered out because they exceed the {max_seq_length} token limit.\")\n",
    "        print(f\"The filtered {split} dataset contains {len(filtered_splits[split])} tasks for {'fine-tuning' if split == 'train' else 'evaluation'}.\")\n",
    "\n",
    "    return filtered_splits\n",
    "\n",
    "filtered_dataset = filter_dataset_by_token_limit(dataset, MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b13b3692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_temp(model, inputs, temperature):\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=MAX_NEW_TOKENS, \n",
    "        do_sample=True, \n",
    "        temperature=temperature,\n",
    "        top_k=50,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def evaluate_batch(model, tokenizer, batch):\n",
    "    inputs = {\n",
    "        'input_ids': batch['input_ids'],\n",
    "        'attention_mask': batch['attention_mask']\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs1 = generate_with_temp(model, inputs, 0.3)\n",
    "        outputs2 = generate_with_temp(model, inputs, 0.7)\n",
    "\n",
    "    input_ids_length = inputs['input_ids'].shape[1] # sequence length without new tokens\n",
    "    new_tokens1 = outputs1[:, input_ids_length:]\n",
    "    new_tokens2 = outputs2[:, input_ids_length:]\n",
    "    \n",
    "    generated_texts1 = tokenizer.batch_decode(new_tokens1, skip_special_tokens=True)\n",
    "    generated_texts2 = tokenizer.batch_decode(new_tokens2, skip_special_tokens=True)\n",
    "    \n",
    "    return generated_texts1, generated_texts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9a1d37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@train\n",
    "def training(model, tokenizer, dataset, max_seq_length):\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['val'],\n",
    "        dataset_text_field=\"texts\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dataset_num_proc=2,\n",
    "        packing=False,  # Can make training 5x faster for short sequences.\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=2,\n",
    "            per_device_eval_batch_size=2,\n",
    "            gradient_accumulation_steps=4,\n",
    "            eval_strategy='steps',\n",
    "            eval_steps=100,\n",
    "            logging_steps=100,\n",
    "            # num_train_epochs=1,  # Set this for 1 full training run.\n",
    "            warmup_steps = 5,\n",
    "            max_steps = 500,\n",
    "            learning_rate=2e-5,\n",
    "            fp16=not is_bfloat16_supported(),\n",
    "            bf16=is_bfloat16_supported(),\n",
    "            optim=\"adamw_8bit\",\n",
    "            weight_decay=0.01,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            seed=3407,\n",
    "            output_dir=f\"{BASE_PATH}/models/gemma-2-2b-it\",\n",
    "            # save_strategy='no',\n",
    "            save_strategy='steps',\n",
    "            save_steps=250,\n",
    "            save_total_limit=2,\n",
    "        ),\n",
    "        # compute_metrics=compute_metrics\n",
    "    )\n",
    "    stats = trainer.train()\n",
    "    return trainer, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff3a7382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2): 100%|██████████| 430/430 [00:01<00:00, 274.16 examples/s]\n",
      "Map (num_proc=2): 100%|██████████| 138/138 [00:01<00:00, 89.20 examples/s] \n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 430 | Num Epochs = 10\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 500\n",
      " \"-____-\"     Number of trainable parameters = 20,766,720\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 53:08, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.453300</td>\n",
       "      <td>0.306030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.302100</td>\n",
       "      <td>0.290039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.272700</td>\n",
       "      <td>0.281183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.255700</td>\n",
       "      <td>0.276471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.253400</td>\n",
       "      <td>0.274622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.30743975448608396, metrics={'train_runtime': 3207.7401, 'train_samples_per_second': 1.247, 'train_steps_per_second': 0.156, 'total_flos': 1.2082800391782912e+17, 'train_loss': 0.30743975448608396, 'epoch': 9.30232558139535})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer, stats = training(model, tokenizer, dataset, max_seq_length=MAX_SEQ_LENGTH)\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "932124f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(f\"{BASE_PATH}/models/short-finetune/gemma-2-2b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6109ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
