{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture\n",
    "! pip install pip3-autoremove\n",
    "! pip-autoremove torch torchvision torchaudio -y\n",
    "! pip install torch xformers triton\n",
    "# ! pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "! pip install unsloth"
   ],
   "id": "ef8fab14e5365864"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f92cd751ac5bf895",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T22:30:03.714823Z",
     "start_time": "2024-08-24T22:30:03.711023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "BASE_PATH = '..'",
   "id": "202fb9a104d3b0b"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6968b3350900c8ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T22:30:04.433615Z",
     "start_time": "2024-08-24T22:30:04.430710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getenv('CUDA_VISIBLE_DEVICES'))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys\n",
    "sys.path.append(BASE_PATH)"
   ],
   "id": "974d0f3bea97cfb"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b724f487f25ef1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T22:30:16.952733Z",
     "start_time": "2024-08-24T22:30:05.641904Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stepan/.conda/envs/llm-py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from logger import get_logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e38733b979eaea35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T22:30:16.956592Z",
     "start_time": "2024-08-24T22:30:16.954159Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install --no-deps --upgrade \"flash-attn>=2.6.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdbac104dec81c19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T22:30:16.986530Z",
     "start_time": "2024-08-24T22:30:16.957512Z"
    }
   },
   "outputs": [],
   "source": [
    "EVAL_BATCH_SIZE = 1\n",
    "MAX_SEQ_LENGTH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9465c79334251d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T22:30:16.993999Z",
     "start_time": "2024-08-24T22:30:16.987889Z"
    }
   },
   "outputs": [],
   "source": "log = get_logger(f'logs/gemma-2-2b', 'arc-agi')"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80575e3e20bdcc9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T22:30:17.000797Z",
     "start_time": "2024-08-24T22:30:16.994961Z"
    }
   },
   "outputs": [],
   "source": [
    "device_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8d5e2d7b55cc84a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T22:30:17.006851Z",
     "start_time": "2024-08-24T22:30:17.001643Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_tokenizer(max_seq_length=1024, dtype=None, load_in_4bit=True, add_lora=False):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"unsloth/gemma-2-2b-it\",\n",
    "        # model_name=\"models/gemma-2-2b/checkpoint-1562\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        device_map={'': 0},\n",
    "        # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    )\n",
    "\n",
    "    if add_lora:\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r=16,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                            \"gate_proj\", \"up_proj\", \"down_proj\", ],\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "            bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
    "            # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "            use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "            random_state=3407,\n",
    "            use_rslora=False,  # We support rank stabilized LoRA\n",
    "            loftq_config=None,  # And LoftQ\n",
    "        )\n",
    "\n",
    "    tokenizer.truncation_side = 'left'\n",
    "    tokenizer.padding_side = 'left'\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a9c2ee4404c9602",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T22:30:37.292060Z",
     "start_time": "2024-08-24T22:30:37.288610Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval(f):\n",
    "    def wrapper(model, *args, **kwargs):\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        return f(model, *args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def train(f):\n",
    "    def wrapper(model, *args, **kwargs):\n",
    "        FastLanguageModel.for_training(model)\n",
    "        return f(model, *args, **kwargs)\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "827043690be05487",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T22:30:43.401670Z",
     "start_time": "2024-08-24T22:30:37.834706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: If you want to finetune Gemma 2, install flash-attn to make it faster!\n",
      "To install flash-attn, do the below:\n",
      "\n",
      "pip install --no-deps --upgrade \"flash-attn>=2.6.3\"\n",
      "==((====))==  Unsloth 2024.8: Fast Gemma2 patching. Transformers = 4.43.4.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.677 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 26 layers with 26 QKV layers, 26 O layers and 26 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_model_tokenizer(max_seq_length=MAX_SEQ_LENGTH, add_lora=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "204e2191ca8c8110",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T22:30:43.405012Z",
     "start_time": "2024-08-24T22:30:43.402934Z"
    }
   },
   "outputs": [],
   "source": "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed12273d6642f5ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T22:30:43.435198Z",
     "start_time": "2024-08-24T22:30:43.405716Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(tokenizer, device, seed):\n",
    "    # 3 classes\n",
    "    # Load data from JSON files\n",
    "    def load_data(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    \n",
    "    # Load all datasets\n",
    "    training_challenges = load_data(f'{BASE_PATH}/arc-agi_training_challenges.json')\n",
    "    training_solutions = load_data(f'{BASE_PATH}/arc-agi_training_solutions.json')\n",
    "    evaluation_challenges = load_data(f'{BASE_PATH}/arc-agi_evaluation_challenges.json')\n",
    "    evaluation_solutions = load_data(f'{BASE_PATH}/arc-agi_evaluation_solutions.json')\n",
    "    test_challenges = load_data(f'{BASE_PATH}/arc-agi_test_challenges.json')\n",
    "\n",
    "    PROMPT = '''### Tweet:\n",
    "    {}\n",
    "    \n",
    "    ### Classification:\n",
    "    {}'''\n",
    "\n",
    "    def formatting_prompts_func(examples):\n",
    "        inputs = examples[\"tweet\"]\n",
    "        outputs = [\n",
    "            HATE_TOKEN if label in [1, 0] else NORMAL_TOKEN for label in examples[\"class\"]\n",
    "        ]\n",
    "        texts = []\n",
    "        prompts = []\n",
    "        for input, output in zip(inputs, outputs):\n",
    "            text = PROMPT.format(input.strip(), output) + EOS_TOKEN\n",
    "            texts.append(text)\n",
    "            prompts.append(PROMPT.format(input, \"\"))\n",
    "        return {\"ref\": texts, 'prompt': prompts}\n",
    "\n",
    "    dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "    # Split the training set into training (80%) and validation (20%) sets\n",
    "    train_testvalid = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "    test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "    # Assign datasets\n",
    "    train_dataset = train_testvalid['train']\n",
    "    val_dataset = test_valid['test']\n",
    "    test_dataset = test_valid['train']\n",
    "\n",
    "    dataset = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'test': test_dataset,\n",
    "        'val': val_dataset\n",
    "    })\n",
    "    return dataset.rename_columns({'class': 'label'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43b165ef37158ac5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T22:30:46.792503Z",
     "start_time": "2024-08-24T22:30:45.035102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['count', 'hate_speech_count', 'offensive_language_count', 'neither_count', 'label', 'tweet', 'ref', 'prompt'],\n",
       "        num_rows: 19826\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['count', 'hate_speech_count', 'offensive_language_count', 'neither_count', 'label', 'tweet', 'ref', 'prompt'],\n",
       "        num_rows: 2478\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['count', 'hate_speech_count', 'offensive_language_count', 'neither_count', 'label', 'tweet', 'ref', 'prompt'],\n",
       "        num_rows: 2479\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = prepare_dataset(tokenizer, f\"cuda:{device_id}\", seed=3407)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "b750e3fc0e966d2e",
   "metadata": {},
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        dataset['test']['prompt'][0],\n",
    "    ], return_tensors=\"pt\").to(f\"cuda:{device_id}\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=1)\n",
    "res = tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "163e212c7cd0ef16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T22:31:23.773147Z",
     "start_time": "2024-08-24T22:31:23.768328Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f57fb960f62f776",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T22:31:24.340875Z",
     "start_time": "2024-08-24T22:31:24.336610Z"
    }
   },
   "outputs": [],
   "source": [
    "def gpu_stats(device_id=0):\n",
    "    #@title Show current memory stats\n",
    "    gpu_stats = torch.cuda.get_device_properties(device_id)\n",
    "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "    return {'gpu': gpu_stats.name, 'max_memory': max_memory, 'start_gpu_memory': start_gpu_memory}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "933a0aa405f661fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T22:31:24.921934Z",
     "start_time": "2024-08-24T22:31:24.918741Z"
    }
   },
   "outputs": [],
   "source": [
    "TOKENS = {\n",
    "    'Positive': ...,\n",
    "    'Negative': ...,\n",
    "\n",
    "    'Hate': ...,\n",
    "    'Normal': ...,\n",
    "    'Fake': ...,\n",
    "    'Truth': ...\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d84dfeec8947bdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T22:31:25.866114Z",
     "start_time": "2024-08-24T22:31:25.861216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: [35202]\n",
      "Negative: [39654]\n",
      "Hate: [88060]\n",
      "Normal: [15273]\n",
      "Fake: [41181]\n",
      "Truth: [55882]\n"
     ]
    }
   ],
   "source": [
    "for key, val in TOKENS.items():\n",
    "    code = tokenizer.encode(key, add_special_tokens=False)\n",
    "    print(f\"{key}: {code}\")\n",
    "    TOKENS[key] = code[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3db1b9294cac810c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T22:31:26.681599Z",
     "start_time": "2024-08-24T22:31:26.677074Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(x, axis=None):\n",
    "    # Subtract the max for numerical stability\n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    x_stable = x - x_max\n",
    "\n",
    "    # Compute the exponentials\n",
    "    exp_x = np.exp(x_stable)\n",
    "\n",
    "    # Compute the softmax\n",
    "    sum_exp_x = np.sum(exp_x, axis=axis, keepdims=True)\n",
    "    softmax_x = exp_x / sum_exp_x\n",
    "\n",
    "    return softmax_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2deff542f908e222",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T22:31:27.357868Z",
     "start_time": "2024-08-24T22:31:27.349125Z"
    }
   },
   "outputs": [],
   "source": [
    "@eval\n",
    "def evaluate(model, tokenizer, dataset, batch_size, threshold=0.5):\n",
    "    eval_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for i, batch in tqdm(enumerate(eval_dataloader), total=len(eval_dataloader)):\n",
    "        texts = batch[\"prompt\"]\n",
    "\n",
    "        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True\n",
    "        )  # (B, len, vocab_size)\n",
    "\n",
    "        pos = outputs.scores[0][:, TOKENS['Hate']].cpu().float().numpy()\n",
    "        neg = outputs.scores[0][:, TOKENS['Normal']].cpu().float().numpy()\n",
    "\n",
    "        preds.extend(np.array([neg, pos]).T)\n",
    "        labels.extend(batch[\"label\"])\n",
    "\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            log.warn(f'GPU Stats: {gpu_stats(device_id)}')\n",
    "\n",
    "    preds = softmax(np.array(preds), axis=-1)[:, 1] > threshold\n",
    "\n",
    "    val_acc = accuracy_score(labels, preds)\n",
    "    val_f1 = f1_score(labels, preds, average='macro')\n",
    "    val_precision = precision_score(labels, preds, average='macro')\n",
    "    val_recall = recall_score(labels, preds, average='macro')\n",
    "\n",
    "    log.info(f\"Accuracy: {val_acc}, F1: {val_f1}, Precision: {val_precision}, Recall: {val_recall}\")\n",
    "    return val_acc, val_f1, val_precision, val_recall, preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af06f073951d6fd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T22:31:28.272151Z",
     "start_time": "2024-08-24T22:31:28.269178Z"
    }
   },
   "outputs": [],
   "source": [
    "# eval_acc, eval_f1, eval_precision, eval_recall, preds, labels = evaluate(model, tokenizer, dataset[\"test\"],\n",
    "#                                                                          batch_size=EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bee9c46da8f039f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T22:31:30.896928Z",
     "start_time": "2024-08-24T22:31:30.892049Z"
    }
   },
   "outputs": [],
   "source": [
    "@train\n",
    "def training(model, tokenizer, dataset, max_seq_length):\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        dataset_text_field=\"ref\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dataset_num_proc=2,\n",
    "        packing=False,  # Can make training 5x faster for short sequences.\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=4,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=500,\n",
    "            num_train_epochs=1,  # Set this for 1 full training run.\n",
    "            # max_steps=60,\n",
    "            learning_rate=2e-5,\n",
    "            fp16=not is_bfloat16_supported(),\n",
    "            bf16=is_bfloat16_supported(),\n",
    "            logging_steps=200,\n",
    "            optim=\"adamw_8bit\",\n",
    "            weight_decay=0.01,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            seed=3407,\n",
    "            output_dir=\"models/gemma-2-2b\",\n",
    "        ),\n",
    "    )\n",
    "    return trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d5a022397f2aa31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T22:50:24.436134Z",
     "start_time": "2024-08-24T22:31:38.129240Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19826/19826 [00:01<00:00, 9955.52 examples/s] \n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 19,826 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 16 | Total steps = 1,239\n",
      " \"-____-\"     Number of trainable parameters = 20,766,720\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1239' max='1239' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1239/1239 18:12, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.655400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.426300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.349800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.338600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.319600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.292200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1239, training_loss=2.5560759574391363, metrics={'train_runtime': 1122.5902, 'train_samples_per_second': 17.661, 'train_steps_per_second': 1.104, 'total_flos': 1.9255666156886016e+16, 'train_loss': 2.5560759574391363, 'epoch': 0.9997982650796853})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = training(model, tokenizer, dataset[\"train\"], max_seq_length=MAX_SEQ_LENGTH)\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47fe0562771eeacc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T22:52:43.927055Z",
     "start_time": "2024-08-24T22:50:24.437474Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 998/2478 [00:56<01:23, 17.71it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.677, 'start_gpu_memory': 6.328}\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1998/2478 [01:52<00:26, 17.85it/s]GPU Stats: {'gpu': 'NVIDIA RTX A5000', 'max_memory': 23.677, 'start_gpu_memory': 6.328}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2478/2478 [02:19<00:00, 17.79it/s]\n",
      "/home/stepan/.conda/envs/llm-py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Accuracy: 0.761501210653753, F1: 0.3198172890128062, Precision: 0.30552159442649357, Recall: 0.3397935378674577\n"
     ]
    }
   ],
   "source": [
    "eval_acc, eval_f1, eval_precision, eval_recall, preds, labels = evaluate(model, tokenizer, dataset[\"test\"], batch_size=EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecdb8c5f99c2907",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
