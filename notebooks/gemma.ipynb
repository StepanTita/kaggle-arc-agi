{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture\n",
    "! pip install pip3-autoremove\n",
    "! pip-autoremove torch torchvision torchaudio -y\n",
    "! pip install torch xformers triton\n",
    "# ! pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "! pip install unsloth"
   ],
   "id": "9c420919fcdbe178"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": "%env CUDA_VISIBLE_DEVICES=0",
   "id": "df5c9ea186d5046a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "BASE_PATH = '..'",
   "id": "2b6b886cea3ea3aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "execution_count": 2,
   "source": [
    "import os\n",
    "\n",
    "print(os.getenv('CUDA_VISIBLE_DEVICES'))"
   ],
   "id": "2082de4ba001df2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys\n",
    "sys.path.append(BASE_PATH)"
   ],
   "id": "5d515f651d366f5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stepan/.conda/envs/llm-py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "execution_count": 4,
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from logger import get_logger\n"
   ],
   "id": "148d053f55d36a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 5,
   "source": "# ! pip install --no-deps --upgrade \"flash-attn>=2.6.3\"",
   "id": "ad676c0a805fd38d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 6,
   "source": [
    "EVAL_BATCH_SIZE = 1\n",
    "MAX_SEQ_LENGTH = 1024"
   ],
   "id": "57cbcb707d80bea0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 7,
   "source": "log = get_logger(f'logs/gemma-2-2b', 'arc-agi')",
   "id": "31950c8f19e622c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 8,
   "source": "device_id = 0",
   "id": "8599147d39964c61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 9,
   "source": [
    "def get_model_tokenizer(max_seq_length=1024, dtype=None, load_in_4bit=True, add_lora=False):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"unsloth/gemma-2-2b-it\",\n",
    "        # model_name=\"models/gemma-2-2b/checkpoint-1562\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        device_map={'': 0},\n",
    "        # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    )\n",
    "\n",
    "    if add_lora:\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r=16,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                            \"gate_proj\", \"up_proj\", \"down_proj\", ],\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "            bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
    "            # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "            use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "            random_state=3407,\n",
    "            use_rslora=False,  # We support rank stabilized LoRA\n",
    "            loftq_config=None,  # And LoftQ\n",
    "        )\n",
    "\n",
    "    tokenizer.truncation_side = 'left'\n",
    "    tokenizer.padding_side = 'left'\n",
    "\n",
    "    return model, tokenizer"
   ],
   "id": "988ec45b558ea6b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 10,
   "source": [
    "def eval(f):\n",
    "    def wrapper(model, *args, **kwargs):\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        return f(model, *args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def train(f):\n",
    "    def wrapper(model, *args, **kwargs):\n",
    "        FastLanguageModel.for_training(model)\n",
    "        return f(model, *args, **kwargs)\n",
    "\n",
    "    return wrapper"
   ],
   "id": "da726f0b32fd2573"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: If you want to finetune Gemma 2, install flash-attn to make it faster!\n",
      "To install flash-attn, do the below:\n",
      "\n",
      "pip install --no-deps --upgrade \"flash-attn>=2.6.3\"\n",
      "==((====))==  Unsloth 2024.8: Fast Gemma2 patching. Transformers = 4.43.4.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.677 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 26 layers with 26 QKV layers, 26 O layers and 26 MLP layers.\n"
     ]
    }
   ],
   "execution_count": 11,
   "source": "model, tokenizer = get_model_tokenizer(max_seq_length=MAX_SEQ_LENGTH, add_lora=True)",
   "id": "bb5c5c94712a79c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 12,
   "source": "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN",
   "id": "9dd620a660faed9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 13,
   "source": [
    "def prepare_dataset(tokenizer, device, seed):\n",
    "    # 3 classes\n",
    "    # Load data from JSON files\n",
    "    def load_data(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    \n",
    "    # Load all datasets\n",
    "    training_challenges = load_data(f'{BASE_PATH}/arc-agi_training_challenges.json')\n",
    "    training_solutions = load_data(f'{BASE_PATH}/arc-agi_training_solutions.json')\n",
    "    evaluation_challenges = load_data(f'{BASE_PATH}/arc-agi_evaluation_challenges.json')\n",
    "    evaluation_solutions = load_data(f'{BASE_PATH}/arc-agi_evaluation_solutions.json')\n",
    "    test_challenges = load_data(f'{BASE_PATH}/arc-agi_test_challenges.json')\n",
    "    \n",
    "    train_dataset = Dataset.from_dict(training_challenges)\n",
    "\n",
    "    PROMPT = '''### Tweet:\n",
    "    {}\n",
    "    \n",
    "    ### Classification:\n",
    "    {}'''\n",
    "\n",
    "    def formatting_prompts_func(examples):\n",
    "        inputs = examples[\"tweet\"]\n",
    "        outputs = [\n",
    "            HATE_TOKEN if label in [1, 0] else NORMAL_TOKEN for label in examples[\"class\"]\n",
    "        ]\n",
    "        texts = []\n",
    "        prompts = []\n",
    "        for input, output in zip(inputs, outputs):\n",
    "            text = PROMPT.format(input.strip(), output) + EOS_TOKEN\n",
    "            texts.append(text)\n",
    "            prompts.append(PROMPT.format(input, \"\"))\n",
    "        return {\"ref\": texts, 'prompt': prompts}\n",
    "\n",
    "    dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "    # Split the training set into training (80%) and validation (20%) sets\n",
    "    train_testvalid = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "    test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "    # Assign datasets\n",
    "    train_dataset = train_testvalid['train']\n",
    "    val_dataset = test_valid['test']\n",
    "    test_dataset = test_valid['train']\n",
    "\n",
    "    dataset = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'test': test_dataset,\n",
    "        'val': val_dataset\n",
    "    })\n",
    "    return dataset.rename_columns({'class': 'label'})\n"
   ],
   "id": "8706765cbf1f81d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['count', 'hate_speech_count', 'offensive_language_count', 'neither_count', 'label', 'tweet', 'ref', 'prompt'],\n",
       "        num_rows: 19826\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['count', 'hate_speech_count', 'offensive_language_count', 'neither_count', 'label', 'tweet', 'ref', 'prompt'],\n",
       "        num_rows: 2478\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['count', 'hate_speech_count', 'offensive_language_count', 'neither_count', 'label', 'tweet', 'ref', 'prompt'],\n",
       "        num_rows: 2479\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14,
   "source": [
    "dataset = prepare_dataset(tokenizer, f\"cuda:{device_id}\", seed=3407)\n",
    "dataset"
   ],
   "id": "fe44745c16e12d3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        dataset['test']['prompt'][0],\n",
    "    ], return_tensors=\"pt\").to(f\"cuda:{device_id}\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=1)\n",
    "res = tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ],
   "id": "cb5fabf99c6ca12f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17,
   "source": "model.device",
   "id": "22b6258adc8c421d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 18,
   "source": [
    "def gpu_stats(device_id=0):\n",
    "    #@title Show current memory stats\n",
    "    gpu_stats = torch.cuda.get_device_properties(device_id)\n",
    "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "    return {'gpu': gpu_stats.name, 'max_memory': max_memory, 'start_gpu_memory': start_gpu_memory}"
   ],
   "id": "f5686a097ef10973"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 19,
   "source": [
    "TOKENS = {\n",
    "    'Positive': ...,\n",
    "    'Negative': ...,\n",
    "\n",
    "    'Hate': ...,\n",
    "    'Normal': ...,\n",
    "    'Fake': ...,\n",
    "    'Truth': ...\n",
    "}"
   ],
   "id": "4f67b9b00a4e5986"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: [35202]\n",
      "Negative: [39654]\n",
      "Hate: [88060]\n",
      "Normal: [15273]\n",
      "Fake: [41181]\n",
      "Truth: [55882]\n"
     ]
    }
   ],
   "execution_count": 20,
   "source": [
    "for key, val in TOKENS.items():\n",
    "    code = tokenizer.encode(key, add_special_tokens=False)\n",
    "    print(f\"{key}: {code}\")\n",
    "    TOKENS[key] = code[0]"
   ],
   "id": "59e44f62c21960c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 21,
   "source": [
    "def softmax(x, axis=None):\n",
    "    # Subtract the max for numerical stability\n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    x_stable = x - x_max\n",
    "\n",
    "    # Compute the exponentials\n",
    "    exp_x = np.exp(x_stable)\n",
    "\n",
    "    # Compute the softmax\n",
    "    sum_exp_x = np.sum(exp_x, axis=axis, keepdims=True)\n",
    "    softmax_x = exp_x / sum_exp_x\n",
    "\n",
    "    return softmax_x"
   ],
   "id": "c0554cadf71a1466"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 22,
   "source": [
    "@eval\n",
    "def evaluate(model, tokenizer, dataset, batch_size, threshold=0.5):\n",
    "    eval_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for i, batch in tqdm(enumerate(eval_dataloader), total=len(eval_dataloader)):\n",
    "        texts = batch[\"prompt\"]\n",
    "\n",
    "        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True\n",
    "        )  # (B, len, vocab_size)\n",
    "\n",
    "        pos = outputs.scores[0][:, TOKENS['Hate']].cpu().float().numpy()\n",
    "        neg = outputs.scores[0][:, TOKENS['Normal']].cpu().float().numpy()\n",
    "\n",
    "        preds.extend(np.array([neg, pos]).T)\n",
    "        labels.extend(batch[\"label\"])\n",
    "\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            log.warn(f'GPU Stats: {gpu_stats(device_id)}')\n",
    "\n",
    "    preds = softmax(np.array(preds), axis=-1)[:, 1] > threshold\n",
    "\n",
    "    val_acc = accuracy_score(labels, preds)\n",
    "\n",
    "    log.info(f\"Accuracy: {val_acc}\")\n",
    "    return val_acc, preds, labels"
   ],
   "id": "217070e6743f193f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 23,
   "source": "eval_acc, preds, labels = evaluate(model, tokenizer, dataset[\"test\"], batch_size=EVAL_BATCH_SIZE)",
   "id": "9ba2d673b92fa15e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
