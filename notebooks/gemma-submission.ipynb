{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%env HF_DATASETS_OFFLINE=1\n","%env HF_HUB_OFFLINE=1\n","%env TRANSFORMERS_OFFLINE=1\n","%env TOKENIZERS_PARALLELISM=false"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["deps_path = \"/kaggle/input/unsloth-library-install-v2\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%capture\n","! pip install --no-index --find-links {deps_path} pip3-autoremove -y\n","! pip-autoremove torch -y\n","! pip install --no-index --find-links {deps_path} torch\n","! pip install --no-index --find-links {deps_path} triton\n","! pip install --no-index --find-links {deps_path} \"unsloth[kaggle-new]\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%capture\n","deps_path_2 = '/kaggle/input/llama-3-arc-deps'\n","! pip install --no-index --find-links {deps_path_2} --requirement {deps_path_2}/requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T03:44:30.107035Z","iopub.status.busy":"2024-09-21T03:44:30.106652Z","iopub.status.idle":"2024-09-21T03:44:30.119080Z","shell.execute_reply":"2024-09-21T03:44:30.118063Z","shell.execute_reply.started":"2024-09-21T03:44:30.106996Z"},"trusted":true},"outputs":[],"source":["BASE_PATH = \"/kaggle/input\"\n","MODEL_ID = \"/kaggle/input/gemma-2-2b-it-baseline/pytorch/default/3/home/stepan/kaggle-arc-agi/models/gemma-2-2b-it/baseline\"\n","MAX_NEW_TOKENS = 2048\n","MAX_SEQ_LENGTH = 8192 - MAX_NEW_TOKENS"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T03:44:30.792317Z","iopub.status.busy":"2024-09-21T03:44:30.791142Z","iopub.status.idle":"2024-09-21T03:44:34.110886Z","shell.execute_reply":"2024-09-21T03:44:34.109752Z","shell.execute_reply.started":"2024-09-21T03:44:30.792266Z"},"trusted":true},"outputs":[],"source":["import json\n","\n","import torch  # type: ignore\n","import numpy as np  # type: ignore\n","\n","from datasets import DatasetDict, Dataset  # type: ignore\n","\n","from tqdm.auto import tqdm  # type: ignore\n","\n","from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig  # type: ignore\n","import train_utils  # type: ignore\n","import data_utils  # type: ignore"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T03:44:34.113596Z","iopub.status.busy":"2024-09-21T03:44:34.112955Z","iopub.status.idle":"2024-09-21T03:44:34.120104Z","shell.execute_reply":"2024-09-21T03:44:34.119159Z","shell.execute_reply.started":"2024-09-21T03:44:34.113547Z"},"trusted":true},"outputs":[],"source":["def get_model_tokenizer():\n","    quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n","    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n","    config = AutoConfig.from_pretrained(MODEL_ID, local_files_only=True)\n","    model = AutoModelForCausalLM.from_pretrained(\n","        MODEL_ID,\n","        quantization_config=quantization_config,\n","        torch_dtype=torch.bfloat16,\n","        device_map=\"auto\",\n","        local_files_only=True,\n","        config=config,\n","        max_memory = {0: \"15.5GiB\", \"cpu\": \"16GiB\"}\n","    )\n","\n","    model.eval()\n","\n","    return model, tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T03:44:34.122244Z","iopub.status.busy":"2024-09-21T03:44:34.121327Z","iopub.status.idle":"2024-09-21T03:44:38.746644Z","shell.execute_reply":"2024-09-21T03:44:38.745682Z","shell.execute_reply.started":"2024-09-21T03:44:34.122196Z"},"trusted":true},"outputs":[],"source":["model, tokenizer = get_model_tokenizer()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T03:44:48.917208Z","iopub.status.busy":"2024-09-21T03:44:48.916752Z","iopub.status.idle":"2024-09-21T03:44:53.864305Z","shell.execute_reply":"2024-09-21T03:44:53.863353Z","shell.execute_reply.started":"2024-09-21T03:44:48.917148Z"},"trusted":true},"outputs":[],"source":["dataset = data_utils.prepare_dataset(tokenizer, fit_dataset=True, base_path=BASE_PATH)\n","dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T03:44:53.905814Z","iopub.status.busy":"2024-09-21T03:44:53.905516Z","iopub.status.idle":"2024-09-21T03:44:53.919551Z","shell.execute_reply":"2024-09-21T03:44:53.918524Z","shell.execute_reply.started":"2024-09-21T03:44:53.905784Z"},"trusted":true},"outputs":[],"source":["def generate_with_temp(model, inputs, temperature):\n","    outputs = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS, do_sample=True, temperature=temperature, top_k=50, use_cache=True)\n","    return outputs\n","\n","\n","def evaluate_batch(model, tokenizer, batch):\n","    inputs = {\n","        \"input_ids\": batch[\"input_ids\"],\n","        \"attention_mask\": batch[\"attention_mask\"],\n","    }\n","\n","    with torch.no_grad():\n","        outputs1 = generate_with_temp(model, inputs, 0.3)\n","        outputs2 = generate_with_temp(model, inputs, 0.7)\n","\n","    input_ids_length = inputs[\"input_ids\"].shape[1]  # sequence length without new tokens\n","    new_tokens1 = outputs1[:, input_ids_length:]\n","    new_tokens2 = outputs2[:, input_ids_length:]\n","\n","    generated_texts1 = tokenizer.batch_decode(new_tokens1, skip_special_tokens=True)\n","    generated_texts2 = tokenizer.batch_decode(new_tokens2, skip_special_tokens=True)\n","\n","    return generated_texts1, generated_texts2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T03:44:53.921473Z","iopub.status.busy":"2024-09-21T03:44:53.920808Z","iopub.status.idle":"2024-09-21T03:44:53.931929Z","shell.execute_reply":"2024-09-21T03:44:53.931138Z","shell.execute_reply.started":"2024-09-21T03:44:53.921424Z"},"trusted":true},"outputs":[],"source":["def predict(model, tokenizer, dataset, batch_size):\n","    eval_dataloader = torch.utils.data.DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        collate_fn=train_utils.collate(mode=\"predict\", tokenizer=tokenizer),\n","    )\n","\n","    challenge_ids = []\n","    preds = []\n","    for i, batch in tqdm(enumerate(eval_dataloader), total=len(eval_dataloader)):\n","        generated_texts1, generated_texts2 = evaluate_batch(model, tokenizer, batch)\n","\n","        ids = batch[\"id\"]\n","        challenges = batch[\"challenge\"]\n","\n","        for gen_text1, gen_text2, challenge_id, challenge in zip(generated_texts1, generated_texts2, ids, challenges):\n","            parsed_output1 = train_utils.parse_output(gen_text1)\n","            parsed_output2 = train_utils.parse_output(gen_text2)\n","\n","            if parsed_output1 is None and parsed_output2 is None:\n","                print(f\"Failed to parse both outputs: {gen_text1} and {gen_text2}\")\n","                preds.append({\"attempt_1\": [[0]], \"attempt_2\": [[0]]})\n","            else:\n","                parsed_output1 = parsed_output1 if parsed_output1 is not None else [[0]]\n","                parsed_output2 = parsed_output2 if parsed_output2 is not None else [[0]]\n","                preds.append({\"attempt_1\": parsed_output1, \"attempt_2\": parsed_output2})\n","            challenge_ids.append((challenge_id, challenge[\"order\"]))\n","    return {\"ids\": challenge_ids, \"preds\": preds}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T03:44:53.933388Z","iopub.status.busy":"2024-09-21T03:44:53.933055Z","iopub.status.idle":"2024-09-21T03:44:53.944134Z","shell.execute_reply":"2024-09-21T03:44:53.943357Z","shell.execute_reply.started":"2024-09-21T03:44:53.933351Z"},"trusted":true},"outputs":[],"source":["def group_preds_by_challenge_id(challenge_ids, preds):\n","    grouped_preds = {}\n","    for (challenge_id, order), pred in zip(challenge_ids, preds):\n","        if challenge_id not in grouped_preds:\n","            grouped_preds[challenge_id] = []\n","\n","        # Check if we already have a prediction for this order\n","        existing_pred = next((p for p in grouped_preds[challenge_id] if p[0] == order), None)\n","\n","        if existing_pred:\n","            # If we have a duplicate (same id and order), choose any (here, we keep the first one)\n","            continue\n","        else:\n","            # Add the new prediction with its order\n","            grouped_preds[challenge_id].append((order, pred))\n","\n","    # Sort predictions by order for each challenge_id\n","    for challenge_id in grouped_preds:\n","        grouped_preds[challenge_id].sort(key=lambda x: x[0])\n","        # Remove the order information, keeping only the predictions\n","        grouped_preds[challenge_id] = [pred for _, pred in grouped_preds[challenge_id]]\n","\n","    return grouped_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T03:44:53.945821Z","iopub.status.busy":"2024-09-21T03:44:53.945378Z"},"trusted":true},"outputs":[],"source":["pred_results = predict(model, tokenizer, dataset[\"predict\"], batch_size=1)\n","grouped_preds = group_preds_by_challenge_id(pred_results[\"ids\"], pred_results[\"preds\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(grouped_preds)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# compare solutions with sample_submission.json\n","with open(f\"{BASE_PATH}/arc-prize-2024/sample_submission.json\", \"r\") as json_file:\n","    sample_submission = json.load(json_file)\n","\n","# Check if all challenge_ids in sample_submission are in grouped_preds, and all tests have correct number of predictions\n","# also check if all predictions are 2d matrices of at least 1x1 size\n","for challenge_id in sample_submission:\n","    if challenge_id not in grouped_preds:\n","        print(f\"Challenge ID {challenge_id} in sample_submission is not in grouped_preds.\")\n","    elif len(grouped_preds[challenge_id]) != len(sample_submission[challenge_id]):\n","        print(\n","            f\"Challenge ID {challenge_id} in sample_submission has {len(sample_submission[challenge_id])} predictions, but grouped_preds has {len(grouped_preds[challenge_id])}.\"\n","        )\n","\n","    for pred in grouped_preds[challenge_id]:\n","        if not isinstance(pred, dict):\n","            print(f\"Challenge ID {challenge_id} in sample_submission has invalid predictions: {pred}\")\n","            continue\n","        if not isinstance(pred[\"attempt_1\"], list) or not isinstance(pred[\"attempt_2\"], list):\n","            print(f\"Challenge ID {challenge_id} in sample_submission has invalid predictions: {pred}\")\n","        if pred[\"attempt_1\"] is None or pred[\"attempt_2\"] is None:\n","            print(f\"Challenge ID {challenge_id} in sample_submission has invalid predictions: {pred}\")\n","        elif pred[\"attempt_1\"] is None or len(pred[\"attempt_1\"]) < 1 or len(pred[\"attempt_1\"][0]) < 1:\n","            print(f\"Challenge ID {challenge_id} in sample_submission has invalid predictions: {pred['attempt_1']}\")\n","        elif pred[\"attempt_2\"] is None or len(pred[\"attempt_2\"]) < 1 or len(pred[\"attempt_2\"][0]) < 1:\n","            print(f\"Challenge ID {challenge_id} in sample_submission has invalid predictions: {pred['attempt_2']}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open(\"submission.json\", \"w\") as json_file:\n","    json.dump(grouped_preds, json_file)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":8951125,"sourceId":67357,"sourceType":"competition"},{"datasetId":5123959,"sourceId":8622192,"sourceType":"datasetVersion"},{"datasetId":5657270,"sourceId":9335918,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelId":122615,"modelInstanceId":98435,"sourceId":117680,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
