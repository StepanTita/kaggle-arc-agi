{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":67357,"databundleVersionId":8951125,"sourceType":"competition"},{"sourceId":8622192,"sourceType":"datasetVersion","datasetId":5123959},{"sourceId":9335918,"sourceType":"datasetVersion","datasetId":5657270},{"sourceId":117680,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":98435,"modelId":122615}],"dockerImageVersionId":30762,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%env HF_DATASETS_OFFLINE=1\n%env HF_HUB_OFFLINE=1\n%env TRANSFORMERS_OFFLINE=1\n%env TOKENIZERS_PARALLELISM=false\n# %env CUDA_VISIBLE_DEVICES=0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deps_path = \"/kaggle/input/unsloth-library-install-v2\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n! pip install --no-index --find-links {deps_path} pip3-autoremove -y\n! pip-autoremove torch -y\n! pip install --no-index --find-links {deps_path} torch\n! pip install --no-index --find-links {deps_path} triton\n! pip install --no-index --find-links {deps_path} \"unsloth[kaggle-new]\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%capture\n# deps_path_2 = '/kaggle/input/llama-3-arc-deps'\n# ! pip install --no-index --find-links {deps_path_2} --requirement {deps_path_2}/requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE_PATH = '/kaggle/input'\nMODEL_ID = '/kaggle/input/gemma-2-2b-it-baseline/pytorch/default/3/home/stepan/kaggle-arc-agi/models/gemma-2-2b-it/baseline'\nMAX_NEW_TOKENS = 2048\nMAX_SEQ_LENGTH = 8192 - MAX_NEW_TOKENS","metadata":{"execution":{"iopub.status.busy":"2024-09-21T03:44:30.106652Z","iopub.execute_input":"2024-09-21T03:44:30.107035Z","iopub.status.idle":"2024-09-21T03:44:30.119080Z","shell.execute_reply.started":"2024-09-21T03:44:30.106996Z","shell.execute_reply":"2024-09-21T03:44:30.118063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport re\n\nimport torch # type: ignore\nimport numpy as np # type: ignore\n\nfrom datasets import DatasetDict, Dataset # type: ignore\n\nfrom tqdm.auto import tqdm # type: ignore\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig # type: ignore","metadata":{"execution":{"iopub.status.busy":"2024-09-21T03:44:30.791142Z","iopub.execute_input":"2024-09-21T03:44:30.792317Z","iopub.status.idle":"2024-09-21T03:44:34.110886Z","shell.execute_reply.started":"2024-09-21T03:44:30.792266Z","shell.execute_reply":"2024-09-21T03:44:34.109752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model_tokenizer():\n    quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side='left')\n    config = AutoConfig.from_pretrained(MODEL_ID, local_files_only=True)\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_ID, \n        quantization_config=quantization_config,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n        local_files_only=True,\n        config=config\n    )\n    \n    model.eval()\n\n    return model, tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-09-21T03:44:34.112955Z","iopub.execute_input":"2024-09-21T03:44:34.113596Z","iopub.status.idle":"2024-09-21T03:44:34.120104Z","shell.execute_reply.started":"2024-09-21T03:44:34.113547Z","shell.execute_reply":"2024-09-21T03:44:34.119159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, tokenizer = get_model_tokenizer()","metadata":{"execution":{"iopub.status.busy":"2024-09-21T03:44:34.121327Z","iopub.execute_input":"2024-09-21T03:44:34.122244Z","iopub.status.idle":"2024-09-21T03:44:38.746644Z","shell.execute_reply.started":"2024-09-21T03:44:34.122196Z","shell.execute_reply":"2024-09-21T03:44:38.745682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load data from JSON files\ndef load_data(file_path):\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n    return data\n\n# Function to calculate the number of tokens in a text\ndef count_tokens(text):\n    \"\"\"\n    Calculate the number of tokens in a given text using the tokenizer.\n\n    Parameters:\n    text (str): The input text to be tokenized.\n\n    Returns:\n    int: The number of tokens in the input text.\n    \"\"\"\n    return len(tokenizer.encode(text))\n\ndef split_train_examples(train_examples, max_size=4096-32):\n    total_size = sum(len(example['input']) * len(example['input'][0]) + len(example['output']) * len(example['output'][0]) for example in train_examples)\n    if total_size <= max_size:\n        return [train_examples]\n    \n    split_size = max(1, max_size // total_size)\n    return [train_examples[i:i+split_size] for i in range(0, len(train_examples), split_size)]\n\ndef to_dataset(data, solutions=None, fit_dataset=False):\n    restructured_data = {\n        'id': [],\n        'challenge': [],\n    }\n    if solutions is not None:\n        restructured_data['solution'] = []\n\n    for challenge_id, challenge_data in data.items(): # for all challenges\n        for test_id, task in enumerate(challenge_data['test']): # for all test tasks in this challenge we want to expand dataset so that each test task is separate dataset record\n            if fit_dataset:\n                for split_id, split_train in enumerate(split_train_examples(challenge_data['train'])): # if fit_dataset is true, we split each training example into multiple records so that each record has less than MAX_SEQ_LENGTH tokens\n                    restructured_data['id'].append(challenge_id)\n                    restructured_data['challenge'].append({'train': split_train, 'test': task, 'order': test_id})\n                    if solutions is not None:\n                        restructured_data['solution'].append(solutions[challenge_id][test_id])\n            else:\n                restructured_data['id'].append(challenge_id)\n                restructured_data['challenge'].append({'train': challenge_data['train'], 'test': task, 'order': test_id})\n                if solutions is not None:\n                    restructured_data['solution'].append(solutions[challenge_id][test_id])\n\n    return Dataset.from_dict(restructured_data)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T03:44:46.930586Z","iopub.execute_input":"2024-09-21T03:44:46.931006Z","iopub.status.idle":"2024-09-21T03:44:46.946036Z","shell.execute_reply.started":"2024-09-21T03:44:46.930968Z","shell.execute_reply":"2024-09-21T03:44:46.944987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_inputs(dct):\n    input_str = '\\n'.join(''.join(map(str, row)) for row in dct[\"input\"])\n    output_str = '\\n'.join(''.join(map(str, row)) for row in dct[\"output\"]) if \"output\" in dct else \"\"\n    text = f'<input>\\n{input_str}\\n</input>\\n\\n<output>\\n{output_str}\\n</output>'\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-09-21T03:44:47.818354Z","iopub.execute_input":"2024-09-21T03:44:47.818785Z","iopub.status.idle":"2024-09-21T03:44:47.824917Z","shell.execute_reply.started":"2024-09-21T03:44:47.818745Z","shell.execute_reply":"2024-09-21T03:44:47.823886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_dataset(tokenizer, use_system_prompt=False, fit_dataset=False):\n    # The system_prompt defines the initial instructions for the model, setting the context for solving ARC tasks.\n    system_prompt = '''You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.'''\n\n    # User message template is a template for creating user prompts. It includes placeholders for training data and test input data, guiding the model to learn the rule and apply it to solve the given puzzle.\n    user_message_template = '''Here are the example input and output pairs from which you should learn the underlying rule to later predict the output for the given test input:\n-----------------\n{training_data}\n-----------------\nNow, solve the following puzzle based on its input grid by applying the rules you have learned from the training data.:\n-----------------\n{input_test_data}\n-----------------\nWhat is the output grid? Only provide the output grid in the form as in the example input and output pairs. Do not provide any additional information:'''\n    \n    # Load all datasets\n    training_challenges = load_data(f'{BASE_PATH}/arc-prize-2024/arc-agi_training_challenges.json')\n    training_solutions = load_data(f'{BASE_PATH}/arc-prize-2024/arc-agi_training_solutions.json')\n    evaluation_challenges = load_data(f'{BASE_PATH}/arc-prize-2024/arc-agi_evaluation_challenges.json')\n    evaluation_solutions = load_data(f'{BASE_PATH}/arc-prize-2024/arc-agi_evaluation_solutions.json')\n    test_challenges = load_data(f'{BASE_PATH}/arc-prize-2024/arc-agi_test_challenges.json')\n    \n    train_dataset = to_dataset(training_challenges, training_solutions, fit_dataset=fit_dataset)\n    eval_dataset = to_dataset(evaluation_challenges, evaluation_solutions, fit_dataset=fit_dataset)\n    pred_dataset = to_dataset(test_challenges, fit_dataset=fit_dataset)\n\n    def create_chat(challenge, solution=None):\n        user_content = user_message_template.format(\n            training_data='\\n\\n'.join([prepare_inputs(ex) for ex in challenge['train']]),\n            input_test_data=prepare_inputs(challenge['test'])\n        )\n        \n        if use_system_prompt:\n            messages = [\n                {'role': 'system', 'content': system_prompt},\n                {'role': 'user', 'content': user_content}\n            ]\n        else:\n            messages = [{'role': 'user', 'content': f\"{system_prompt}\\n\\n{user_content}\"}]\n        \n        if solution:\n            messages.append({'role': 'assistant', 'content': \"<output>\\n\" + '\\n'.join(''.join(map(str, row)) for row in solution) + \"\\n</output>\"})\n        \n        return messages\n\n    def process_dataset(examples, solutions=None):\n        # Create messages for each challenge-solution pair\n        chats = []\n        for challenge, solution in zip(examples['challenge'], solutions or [None] * len(examples['challenge'])):\n            chat = create_chat(challenge, solution)\n            chats.append(chat)\n        \n        # Apply chat template to each message\n        texts = [tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False) for chat in chats]\n        \n        return {\n            'texts': texts,\n            'messages': chats\n        }\n\n    train_dataset = train_dataset.map(lambda x: process_dataset(x, train_dataset['solution']), batched=True)\n    pred_dataset = pred_dataset.map(lambda x: process_dataset(x), batched=True)\n\n    eval_dataset = eval_dataset.map(lambda x: process_dataset(x, eval_dataset['solution']), batched=True)\n    test_dataset = eval_dataset.train_test_split(test_size=0.3)\n\n    dataset = DatasetDict({\n        'train': train_dataset,\n        'test': test_dataset['train'],\n        'val': test_dataset['test'],\n        'predict': pred_dataset\n    })\n\n    return dataset\n","metadata":{"execution":{"iopub.status.busy":"2024-09-21T03:44:48.337823Z","iopub.execute_input":"2024-09-21T03:44:48.338242Z","iopub.status.idle":"2024-09-21T03:44:48.354492Z","shell.execute_reply.started":"2024-09-21T03:44:48.338195Z","shell.execute_reply":"2024-09-21T03:44:48.353444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = prepare_dataset(tokenizer, fit_dataset=True)\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-09-21T03:44:48.916752Z","iopub.execute_input":"2024-09-21T03:44:48.917208Z","iopub.status.idle":"2024-09-21T03:44:53.864305Z","shell.execute_reply.started":"2024-09-21T03:44:48.917148Z","shell.execute_reply":"2024-09-21T03:44:53.863353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gpu_stats(device_id=0):\n    #@title Show current memory stats\n    gpu_stats = torch.cuda.get_device_properties(device_id)\n    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n    return {'gpu': gpu_stats.name, 'max_memory': max_memory, 'start_gpu_memory': start_gpu_memory}","metadata":{"execution":{"iopub.status.busy":"2024-09-21T03:44:53.866114Z","iopub.execute_input":"2024-09-21T03:44:53.866833Z","iopub.status.idle":"2024-09-21T03:44:53.872187Z","shell.execute_reply.started":"2024-09-21T03:44:53.866781Z","shell.execute_reply":"2024-09-21T03:44:53.871328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_output(text):\n    # Extract the content inside <output></output> tags\n    output_match = re.search(r'<output>(.*?)</output>', text, re.DOTALL)\n    if not output_match:\n        return None\n    \n    output_content = output_match.group(1).strip()\n    \n    # Split the content into lines and convert each line to a list of single-digit integers\n    try:\n        grid = []\n        for line in output_content.split('\\n'):\n            row = [int(char) for char in line.strip() if char.isdigit()]\n            if row:\n                grid.append(row)\n        \n        # Ensure all rows have the same length\n        if grid and all(len(row) == len(grid[0]) for row in grid):\n            return grid\n        else:\n            return None\n    except ValueError:\n        return None\n    \ndef tensor_to_int(value):\n    if isinstance(value, torch.Tensor):\n        return tensor_to_int(value.item())\n    elif isinstance(value, list):\n        return [tensor_to_int(item) for item in value]\n    else:\n        return value\n    \ndef calculate_partial_match(pred, label):\n    if not isinstance(pred, list) or not isinstance(label, list):\n        return 0  # No match if either is not a list\n\n    if len(pred) != len(label):\n        return 0  # No match if outer dimensions differ\n\n    total_elements = 0\n    correct_elements = 0\n\n    for p_row, l_row in zip(pred, label):\n        if not isinstance(p_row, list) or not isinstance(l_row, list) or len(p_row) != len(l_row):\n            return 0  # No match if any row is not a list or dimensions differ\n\n        total_elements += len(l_row)\n        correct_elements += sum(p == l for p, l in zip(p_row, l_row))\n\n    return correct_elements / total_elements if total_elements > 0 else 0\n\ndef calculate_metrics(preds, labels):\n    total_samples = len(labels)\n    \n    correct = sum(1 for p, l in zip(preds, labels) if p == l)\n    accuracy = correct / total_samples\n    \n    partial_match_scores = [calculate_partial_match(p, l) if p is not None else 0 for p, l in zip(preds, labels)]\n    \n    avg_partial_match = sum(partial_match_scores) / total_samples\n    \n    return accuracy, avg_partial_match","metadata":{"execution":{"iopub.status.busy":"2024-09-21T03:44:53.873399Z","iopub.execute_input":"2024-09-21T03:44:53.873729Z","iopub.status.idle":"2024-09-21T03:44:53.889228Z","shell.execute_reply.started":"2024-09-21T03:44:53.873681Z","shell.execute_reply":"2024-09-21T03:44:53.888432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate(mode, tokenizer):\n    def collate_fn(batch):\n        # Separate the different components of the batch\n        ids = [item['id'] for item in batch]\n        challenges = [item['challenge'] for item in batch]\n        \n        # For 'test' mode, remove the last assistant message from each entry\n        if mode == 'test':\n            messages = [item['messages'][:-1] for item in batch] # last message is always assistant message - solution, we don't need it for evaluation\n        else:\n            messages = [item['messages'] for item in batch]\n        \n        # Tokenize the texts\n        encodings = tokenizer.apply_chat_template(\n            messages,\n            tokenize=True,\n            add_generation_prompt=True,\n            return_tensors=\"pt\",\n            return_dict=True,\n            padding=True, \n            # truncation=True\n        )\n        \n        # If 'solution' is present (for training/validation data)\n        if 'solution' in batch[0]:\n            solutions = [item['solution'] for item in batch]\n            return {\n                'id': ids,\n                'challenge': challenges,\n                'solution': solutions,\n                'input_ids': encodings['input_ids'].to(\"cuda\"),\n                'attention_mask': encodings['attention_mask'].to(\"cuda\")\n            }\n        else:\n            return {\n                'id': ids,\n                'challenge': challenges,\n                'input_ids': encodings['input_ids'].to(\"cuda\"),\n                'attention_mask': encodings['attention_mask'].to(\"cuda\")\n            }\n    return collate_fn","metadata":{"execution":{"iopub.status.busy":"2024-09-21T03:44:53.891301Z","iopub.execute_input":"2024-09-21T03:44:53.891592Z","iopub.status.idle":"2024-09-21T03:44:53.904144Z","shell.execute_reply.started":"2024-09-21T03:44:53.891562Z","shell.execute_reply":"2024-09-21T03:44:53.903386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_with_temp(model, inputs, temperature):\n    outputs = model.generate(\n        **inputs, \n        max_new_tokens=MAX_NEW_TOKENS, \n        do_sample=True, \n        temperature=temperature,\n        top_k=50,\n        use_cache=True\n    )\n    return outputs\n\n\ndef evaluate_batch(model, tokenizer, batch):\n    inputs = {\n        'input_ids': batch['input_ids'],\n        'attention_mask': batch['attention_mask']\n    }\n\n    with torch.no_grad():\n        outputs1 = generate_with_temp(model, inputs, 0.3)\n        outputs2 = generate_with_temp(model, inputs, 0.7)\n\n    input_ids_length = inputs['input_ids'].shape[1] # sequence length without new tokens\n    new_tokens1 = outputs1[:, input_ids_length:]\n    new_tokens2 = outputs2[:, input_ids_length:]\n    \n    generated_texts1 = tokenizer.batch_decode(new_tokens1, skip_special_tokens=True)\n    generated_texts2 = tokenizer.batch_decode(new_tokens2, skip_special_tokens=True)\n    \n    return generated_texts1, generated_texts2","metadata":{"execution":{"iopub.status.busy":"2024-09-21T03:44:53.905516Z","iopub.execute_input":"2024-09-21T03:44:53.905814Z","iopub.status.idle":"2024-09-21T03:44:53.919551Z","shell.execute_reply.started":"2024-09-21T03:44:53.905784Z","shell.execute_reply":"2024-09-21T03:44:53.918524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, tokenizer, dataset, batch_size):\n    eval_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate(mode='predict', tokenizer=tokenizer))\n\n    challenge_ids = []\n    preds = []\n    for i, batch in tqdm(enumerate(eval_dataloader), total=len(eval_dataloader)):\n        generated_texts1, generated_texts2 = evaluate_batch(model, tokenizer, batch)\n\n        ids = batch[\"id\"]\n        challenges = batch[\"challenge\"]\n        \n        for gen_text1, gen_text2, challenge_id, challenge in zip(generated_texts1, generated_texts2, ids, challenges):\n            parsed_output1 = parse_output(gen_text1)\n            parsed_output2 = parse_output(gen_text2)\n            \n            if parsed_output1 is None and parsed_output2 is None:\n                print(f\"Failed to parse both outputs: {gen_text1} and {gen_text2}\")\n                preds.append([[0]])\n            else:\n                preds.append({'attempt_1': parsed_output1, 'attempt_2': parsed_output2})\n            challenge_ids.append((challenge_id, challenge['order']))\n    return {'ids': challenge_ids, 'preds': preds}","metadata":{"execution":{"iopub.status.busy":"2024-09-21T03:44:53.920808Z","iopub.execute_input":"2024-09-21T03:44:53.921473Z","iopub.status.idle":"2024-09-21T03:44:53.931929Z","shell.execute_reply.started":"2024-09-21T03:44:53.921424Z","shell.execute_reply":"2024-09-21T03:44:53.931138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def group_preds_by_challenge_id(challenge_ids, preds):\n    grouped_preds = {}\n    for (challenge_id, order), pred in zip(challenge_ids, preds):\n        if challenge_id not in grouped_preds:\n            grouped_preds[challenge_id] = []\n        \n        # Check if we already have a prediction for this order\n        existing_pred = next((p for p in grouped_preds[challenge_id] if p[0] == order), None)\n        \n        if existing_pred:\n            # If we have a duplicate (same id and order), choose any (here, we keep the first one)\n            continue\n        else:\n            # Add the new prediction with its order\n            grouped_preds[challenge_id].append((order, pred))\n    \n    # Sort predictions by order for each challenge_id\n    for challenge_id in grouped_preds:\n        grouped_preds[challenge_id].sort(key=lambda x: x[0])\n        # Remove the order information, keeping only the predictions\n        grouped_preds[challenge_id] = [pred for _, pred in grouped_preds[challenge_id]]\n    \n    return grouped_preds","metadata":{"execution":{"iopub.status.busy":"2024-09-21T03:44:53.933055Z","iopub.execute_input":"2024-09-21T03:44:53.933388Z","iopub.status.idle":"2024-09-21T03:44:53.944134Z","shell.execute_reply.started":"2024-09-21T03:44:53.933351Z","shell.execute_reply":"2024-09-21T03:44:53.943357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_results = predict(model, tokenizer, dataset['predict'], batch_size=1)\ngrouped_preds = group_preds_by_challenge_id(pred_results['ids'], pred_results['preds'])","metadata":{"execution":{"iopub.status.busy":"2024-09-21T03:44:53.945378Z","iopub.execute_input":"2024-09-21T03:44:53.945821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(grouped_preds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('submission.json', 'w') as json_file:\n    json.dump(grouped_preds, json_file) ","metadata":{},"execution_count":null,"outputs":[]}]}