{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%env HF_DATASETS_OFFLINE=1\n","%env HF_HUB_OFFLINE=1\n","%env TRANSFORMERS_OFFLINE=1\n","%env TOKENIZERS_PARALLELISM=false"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["deps_path = \"/kaggle/input/unsloth-library-install-v2\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%capture\n","! pip install --no-index --find-links {deps_path} pip3-autoremove -y\n","! pip-autoremove torch -y\n","! pip install --no-index --find-links {deps_path} torch\n","! pip install --no-index --find-links {deps_path} triton\n","! pip install --no-index --find-links {deps_path} \"unsloth[kaggle-new]\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%capture\n","deps_path_2 = '/kaggle/input/llama-3-arc-deps'\n","! pip install --no-index --find-links {deps_path_2} --requirement {deps_path_2}/requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T03:44:30.107035Z","iopub.status.busy":"2024-09-21T03:44:30.106652Z","iopub.status.idle":"2024-09-21T03:44:30.119080Z","shell.execute_reply":"2024-09-21T03:44:30.118063Z","shell.execute_reply.started":"2024-09-21T03:44:30.106996Z"},"trusted":true},"outputs":[],"source":["BASE_PATH = \"/kaggle/input\"\n","MODEL_ID = \"/kaggle/input/gemma-2-2b-it-baseline/pytorch/default/3/home/stepan/kaggle-arc-agi/models/gemma-2-2b-it/baseline\"\n","MAX_NEW_TOKENS = 2048\n","MAX_SEQ_LENGTH = 8192 - MAX_NEW_TOKENS"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T03:44:30.792317Z","iopub.status.busy":"2024-09-21T03:44:30.791142Z","iopub.status.idle":"2024-09-21T03:44:34.110886Z","shell.execute_reply":"2024-09-21T03:44:34.109752Z","shell.execute_reply.started":"2024-09-21T03:44:30.792266Z"},"trusted":true},"outputs":[],"source":["import json\n","import re\n","\n","import torch  # type: ignore\n","import numpy as np  # type: ignore\n","\n","from datasets import DatasetDict, Dataset  # type: ignore\n","\n","from tqdm.auto import tqdm  # type: ignore\n","\n","from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig  # type: ignore"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T03:44:34.113596Z","iopub.status.busy":"2024-09-21T03:44:34.112955Z","iopub.status.idle":"2024-09-21T03:44:34.120104Z","shell.execute_reply":"2024-09-21T03:44:34.119159Z","shell.execute_reply.started":"2024-09-21T03:44:34.113547Z"},"trusted":true},"outputs":[],"source":["def get_model_tokenizer():\n","    quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n","    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n","    config = AutoConfig.from_pretrained(MODEL_ID, local_files_only=True)\n","    model = AutoModelForCausalLM.from_pretrained(\n","        MODEL_ID,\n","        quantization_config=quantization_config,\n","        torch_dtype=torch.bfloat16,\n","        device_map=\"auto\",\n","        local_files_only=True,\n","        config=config,\n","    )\n","\n","    model.eval()\n","\n","    return model, tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T03:44:34.122244Z","iopub.status.busy":"2024-09-21T03:44:34.121327Z","iopub.status.idle":"2024-09-21T03:44:38.746644Z","shell.execute_reply":"2024-09-21T03:44:38.745682Z","shell.execute_reply.started":"2024-09-21T03:44:34.122196Z"},"trusted":true},"outputs":[],"source":["model, tokenizer = get_model_tokenizer()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T03:44:46.931006Z","iopub.status.busy":"2024-09-21T03:44:46.930586Z","iopub.status.idle":"2024-09-21T03:44:46.946036Z","shell.execute_reply":"2024-09-21T03:44:46.944987Z","shell.execute_reply.started":"2024-09-21T03:44:46.930968Z"},"trusted":true},"outputs":[],"source":["# Load data from JSON files\n","def load_data(file_path):\n","    with open(file_path, \"r\") as f:\n","        data = json.load(f)\n","    return data\n","\n","\n","# Function to calculate the number of tokens in a text\n","def count_tokens(text):\n","    \"\"\"\n","    Calculate the number of tokens in a given text using the tokenizer.\n","\n","    Parameters:\n","    text (str): The input text to be tokenized.\n","\n","    Returns:\n","    int: The number of tokens in the input text.\n","    \"\"\"\n","    return len(tokenizer.encode(text))\n","\n","\n","def split_train_examples(train_examples, max_size=4096 - 32):\n","    total_size = sum(\n","        len(example[\"input\"]) * len(example[\"input\"][0]) + len(example[\"output\"]) * len(example[\"output\"][0]) for example in train_examples\n","    )\n","    if total_size <= max_size:\n","        return [train_examples]\n","\n","    split_size = max(1, max_size // total_size)\n","    return [train_examples[i : i + split_size] for i in range(0, len(train_examples), split_size)]\n","\n","\n","def to_dataset(data, solutions=None, fit_dataset=False):\n","    restructured_data = {\n","        \"id\": [],\n","        \"challenge\": [],\n","    }\n","    if solutions is not None:\n","        restructured_data[\"solution\"] = []\n","\n","    for challenge_id, challenge_data in data.items():  # for all challenges\n","        for test_id, task in enumerate(\n","            challenge_data[\"test\"]\n","        ):  # for all test tasks in this challenge we want to expand dataset so that each test task is separate dataset record\n","            if fit_dataset:\n","                for split_id, split_train in enumerate(\n","                    split_train_examples(challenge_data[\"train\"])\n","                ):  # if fit_dataset is true, we split each training example into multiple records so that each record has less than MAX_SEQ_LENGTH tokens\n","                    restructured_data[\"id\"].append(challenge_id)\n","                    restructured_data[\"challenge\"].append({\"train\": split_train, \"test\": task, \"order\": test_id})\n","                    if solutions is not None:\n","                        restructured_data[\"solution\"].append(solutions[challenge_id][test_id])\n","            else:\n","                restructured_data[\"id\"].append(challenge_id)\n","                restructured_data[\"challenge\"].append({\"train\": challenge_data[\"train\"], \"test\": task, \"order\": test_id})\n","                if solutions is not None:\n","                    restructured_data[\"solution\"].append(solutions[challenge_id][test_id])\n","\n","    return Dataset.from_dict(restructured_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T03:44:47.818785Z","iopub.status.busy":"2024-09-21T03:44:47.818354Z","iopub.status.idle":"2024-09-21T03:44:47.824917Z","shell.execute_reply":"2024-09-21T03:44:47.823886Z","shell.execute_reply.started":"2024-09-21T03:44:47.818745Z"},"trusted":true},"outputs":[],"source":["def prepare_inputs(dct):\n","    input_str = \"\\n\".join(\"\".join(map(str, row)) for row in dct[\"input\"])\n","    output_str = \"\\n\".join(\"\".join(map(str, row)) for row in dct[\"output\"]) if \"output\" in dct else \"\"\n","    text = f\"<input>\\n{input_str}\\n</input>\\n\\n<output>\\n{output_str}\\n</output>\"\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T03:44:48.338242Z","iopub.status.busy":"2024-09-21T03:44:48.337823Z","iopub.status.idle":"2024-09-21T03:44:48.354492Z","shell.execute_reply":"2024-09-21T03:44:48.353444Z","shell.execute_reply.started":"2024-09-21T03:44:48.338195Z"},"trusted":true},"outputs":[],"source":["def prepare_dataset(tokenizer, use_system_prompt=False, fit_dataset=False):\n","    # The system_prompt defines the initial instructions for the model, setting the context for solving ARC tasks.\n","    system_prompt = (\n","        \"\"\"You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.\"\"\"\n","    )\n","\n","    # User message template is a template for creating user prompts. It includes placeholders for training data and test input data, guiding the model to learn the rule and apply it to solve the given puzzle.\n","    user_message_template = \"\"\"Here are the example input and output pairs from which you should learn the underlying rule to later predict the output for the given test input:\n","-----------------\n","{training_data}\n","-----------------\n","Now, solve the following puzzle based on its input grid by applying the rules you have learned from the training data.:\n","-----------------\n","{input_test_data}\n","-----------------\n","What is the output grid? Only provide the output grid in the form as in the example input and output pairs. Do not provide any additional information:\"\"\"\n","\n","    # Load all datasets\n","    training_challenges = load_data(f\"{BASE_PATH}/arc-prize-2024/arc-agi_training_challenges.json\")\n","    training_solutions = load_data(f\"{BASE_PATH}/arc-prize-2024/arc-agi_training_solutions.json\")\n","    evaluation_challenges = load_data(f\"{BASE_PATH}/arc-prize-2024/arc-agi_evaluation_challenges.json\")\n","    evaluation_solutions = load_data(f\"{BASE_PATH}/arc-prize-2024/arc-agi_evaluation_solutions.json\")\n","    test_challenges = load_data(f\"{BASE_PATH}/arc-prize-2024/arc-agi_test_challenges.json\")\n","\n","    train_dataset = to_dataset(training_challenges, training_solutions, fit_dataset=fit_dataset)\n","    eval_dataset = to_dataset(evaluation_challenges, evaluation_solutions, fit_dataset=fit_dataset)\n","    pred_dataset = to_dataset(test_challenges, fit_dataset=fit_dataset)\n","\n","    def create_chat(challenge, solution=None):\n","        user_content = user_message_template.format(\n","            training_data=\"\\n\\n\".join([prepare_inputs(ex) for ex in challenge[\"train\"]]),\n","            input_test_data=prepare_inputs(challenge[\"test\"]),\n","        )\n","\n","        if use_system_prompt:\n","            messages = [\n","                {\"role\": \"system\", \"content\": system_prompt},\n","                {\"role\": \"user\", \"content\": user_content},\n","            ]\n","        else:\n","            messages = [{\"role\": \"user\", \"content\": f\"{system_prompt}\\n\\n{user_content}\"}]\n","\n","        if solution:\n","            messages.append(\n","                {\n","                    \"role\": \"assistant\",\n","                    \"content\": \"<output>\\n\" + \"\\n\".join(\"\".join(map(str, row)) for row in solution) + \"\\n</output>\",\n","                }\n","            )\n","\n","        return messages\n","\n","    def process_dataset(examples, solutions=None):\n","        # Create messages for each challenge-solution pair\n","        chats = []\n","        for challenge, solution in zip(examples[\"challenge\"], solutions or [None] * len(examples[\"challenge\"])):\n","            chat = create_chat(challenge, solution)\n","            chats.append(chat)\n","\n","        # Apply chat template to each message\n","        texts = [tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False) for chat in chats]\n","\n","        return {\"texts\": texts, \"messages\": chats}\n","\n","    train_dataset = train_dataset.map(lambda x: process_dataset(x, train_dataset[\"solution\"]), batched=True)\n","    pred_dataset = pred_dataset.map(lambda x: process_dataset(x), batched=True)\n","\n","    eval_dataset = eval_dataset.map(lambda x: process_dataset(x, eval_dataset[\"solution\"]), batched=True)\n","    test_dataset = eval_dataset.train_test_split(test_size=0.3)\n","\n","    dataset = DatasetDict(\n","        {\n","            \"train\": train_dataset,\n","            \"test\": test_dataset[\"train\"],\n","            \"val\": test_dataset[\"test\"],\n","            \"predict\": pred_dataset,\n","        }\n","    )\n","\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T03:44:48.917208Z","iopub.status.busy":"2024-09-21T03:44:48.916752Z","iopub.status.idle":"2024-09-21T03:44:53.864305Z","shell.execute_reply":"2024-09-21T03:44:53.863353Z","shell.execute_reply.started":"2024-09-21T03:44:48.917148Z"},"trusted":true},"outputs":[],"source":["dataset = prepare_dataset(tokenizer, fit_dataset=True)\n","dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T03:44:53.866833Z","iopub.status.busy":"2024-09-21T03:44:53.866114Z","iopub.status.idle":"2024-09-21T03:44:53.872187Z","shell.execute_reply":"2024-09-21T03:44:53.871328Z","shell.execute_reply.started":"2024-09-21T03:44:53.866781Z"},"trusted":true},"outputs":[],"source":["def gpu_stats(device_id=0):\n","    # @title Show current memory stats\n","    gpu_stats = torch.cuda.get_device_properties(device_id)\n","    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n","    return {\n","        \"gpu\": gpu_stats.name,\n","        \"max_memory\": max_memory,\n","        \"start_gpu_memory\": start_gpu_memory,\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T03:44:53.873729Z","iopub.status.busy":"2024-09-21T03:44:53.873399Z","iopub.status.idle":"2024-09-21T03:44:53.889228Z","shell.execute_reply":"2024-09-21T03:44:53.888432Z","shell.execute_reply.started":"2024-09-21T03:44:53.873681Z"},"trusted":true},"outputs":[],"source":["def parse_output(text):\n","    # Extract the content inside <output></output> tags\n","    output_match = re.search(r\"<output>(.*?)</output>\", text, re.DOTALL)\n","    if not output_match:\n","        return None\n","\n","    output_content = output_match.group(1).strip()\n","\n","    # Split the content into lines and convert each line to a list of single-digit integers\n","    try:\n","        grid = []\n","        for line in output_content.split(\"\\n\"):\n","            row = [int(char) for char in line.strip() if char.isdigit()]\n","            if row:\n","                grid.append(row)\n","\n","        # Ensure all rows have the same length\n","        if grid and all(len(row) == len(grid[0]) for row in grid):\n","            return grid\n","        else:\n","            return None\n","    except ValueError:\n","        return None\n","\n","\n","def tensor_to_int(value):\n","    if isinstance(value, torch.Tensor):\n","        return tensor_to_int(value.item())\n","    elif isinstance(value, list):\n","        return [tensor_to_int(item) for item in value]\n","    else:\n","        return value\n","\n","\n","def calculate_partial_match(pred, label):\n","    if not isinstance(pred, list) or not isinstance(label, list):\n","        return 0  # No match if either is not a list\n","\n","    if len(pred) != len(label):\n","        return 0  # No match if outer dimensions differ\n","\n","    total_elements = 0\n","    correct_elements = 0\n","\n","    for p_row, l_row in zip(pred, label):\n","        if not isinstance(p_row, list) or not isinstance(l_row, list) or len(p_row) != len(l_row):\n","            return 0  # No match if any row is not a list or dimensions differ\n","\n","        total_elements += len(l_row)\n","        correct_elements += sum(p == l for p, l in zip(p_row, l_row))\n","\n","    return correct_elements / total_elements if total_elements > 0 else 0\n","\n","\n","def calculate_metrics(preds, labels):\n","    total_samples = len(labels)\n","\n","    correct = sum(1 for p, l in zip(preds, labels) if p == l)\n","    accuracy = correct / total_samples\n","\n","    partial_match_scores = [calculate_partial_match(p, l) if p is not None else 0 for p, l in zip(preds, labels)]\n","\n","    avg_partial_match = sum(partial_match_scores) / total_samples\n","\n","    return accuracy, avg_partial_match"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T03:44:53.891592Z","iopub.status.busy":"2024-09-21T03:44:53.891301Z","iopub.status.idle":"2024-09-21T03:44:53.904144Z","shell.execute_reply":"2024-09-21T03:44:53.903386Z","shell.execute_reply.started":"2024-09-21T03:44:53.891562Z"},"trusted":true},"outputs":[],"source":["def collate(mode, tokenizer):\n","    def collate_fn(batch):\n","        # Separate the different components of the batch\n","        ids = [item[\"id\"] for item in batch]\n","        challenges = [item[\"challenge\"] for item in batch]\n","\n","        # For 'test' mode, remove the last assistant message from each entry\n","        if mode == \"test\":\n","            messages = [\n","                item[\"messages\"][:-1] for item in batch\n","            ]  # last message is always assistant message - solution, we don't need it for evaluation\n","        else:\n","            messages = [item[\"messages\"] for item in batch]\n","\n","        # Tokenize the texts\n","        encodings = tokenizer.apply_chat_template(\n","            messages,\n","            tokenize=True,\n","            add_generation_prompt=True,\n","            return_tensors=\"pt\",\n","            return_dict=True,\n","            padding=True,\n","            # truncation=True\n","        )\n","\n","        # If 'solution' is present (for training/validation data)\n","        if \"solution\" in batch[0]:\n","            solutions = [item[\"solution\"] for item in batch]\n","            return {\n","                \"id\": ids,\n","                \"challenge\": challenges,\n","                \"solution\": solutions,\n","                \"input_ids\": encodings[\"input_ids\"].to(\"cuda\"),\n","                \"attention_mask\": encodings[\"attention_mask\"].to(\"cuda\"),\n","            }\n","        else:\n","            return {\n","                \"id\": ids,\n","                \"challenge\": challenges,\n","                \"input_ids\": encodings[\"input_ids\"].to(\"cuda\"),\n","                \"attention_mask\": encodings[\"attention_mask\"].to(\"cuda\"),\n","            }\n","\n","    return collate_fn"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T03:44:53.905814Z","iopub.status.busy":"2024-09-21T03:44:53.905516Z","iopub.status.idle":"2024-09-21T03:44:53.919551Z","shell.execute_reply":"2024-09-21T03:44:53.918524Z","shell.execute_reply.started":"2024-09-21T03:44:53.905784Z"},"trusted":true},"outputs":[],"source":["def generate_with_temp(model, inputs, temperature):\n","    outputs = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS, do_sample=True, temperature=temperature, top_k=50, use_cache=True)\n","    return outputs\n","\n","\n","def evaluate_batch(model, tokenizer, batch):\n","    inputs = {\n","        \"input_ids\": batch[\"input_ids\"],\n","        \"attention_mask\": batch[\"attention_mask\"],\n","    }\n","\n","    with torch.no_grad():\n","        outputs1 = generate_with_temp(model, inputs, 0.3)\n","        outputs2 = generate_with_temp(model, inputs, 0.7)\n","\n","    input_ids_length = inputs[\"input_ids\"].shape[1]  # sequence length without new tokens\n","    new_tokens1 = outputs1[:, input_ids_length:]\n","    new_tokens2 = outputs2[:, input_ids_length:]\n","\n","    generated_texts1 = tokenizer.batch_decode(new_tokens1, skip_special_tokens=True)\n","    generated_texts2 = tokenizer.batch_decode(new_tokens2, skip_special_tokens=True)\n","\n","    return generated_texts1, generated_texts2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T03:44:53.921473Z","iopub.status.busy":"2024-09-21T03:44:53.920808Z","iopub.status.idle":"2024-09-21T03:44:53.931929Z","shell.execute_reply":"2024-09-21T03:44:53.931138Z","shell.execute_reply.started":"2024-09-21T03:44:53.921424Z"},"trusted":true},"outputs":[],"source":["def predict(model, tokenizer, dataset, batch_size):\n","    eval_dataloader = torch.utils.data.DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        collate_fn=collate(mode=\"predict\", tokenizer=tokenizer),\n","    )\n","\n","    challenge_ids = []\n","    preds = []\n","    for i, batch in tqdm(enumerate(eval_dataloader), total=len(eval_dataloader)):\n","        generated_texts1, generated_texts2 = evaluate_batch(model, tokenizer, batch)\n","\n","        ids = batch[\"id\"]\n","        challenges = batch[\"challenge\"]\n","\n","        for gen_text1, gen_text2, challenge_id, challenge in zip(generated_texts1, generated_texts2, ids, challenges):\n","            parsed_output1 = parse_output(gen_text1)\n","            parsed_output2 = parse_output(gen_text2)\n","\n","            if parsed_output1 is None and parsed_output2 is None:\n","                print(f\"Failed to parse both outputs: {gen_text1} and {gen_text2}\")\n","                preds.append({\"attempt_1\": [[0]], \"attempt_2\": [[0]]})\n","            else:\n","                parsed_output1 = parsed_output1 if parsed_output1 is not None else [[0]]\n","                parsed_output2 = parsed_output2 if parsed_output2 is not None else [[0]]\n","                preds.append({\"attempt_1\": parsed_output1, \"attempt_2\": parsed_output2})\n","            challenge_ids.append((challenge_id, challenge[\"order\"]))\n","    return {\"ids\": challenge_ids, \"preds\": preds}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T03:44:53.933388Z","iopub.status.busy":"2024-09-21T03:44:53.933055Z","iopub.status.idle":"2024-09-21T03:44:53.944134Z","shell.execute_reply":"2024-09-21T03:44:53.943357Z","shell.execute_reply.started":"2024-09-21T03:44:53.933351Z"},"trusted":true},"outputs":[],"source":["def group_preds_by_challenge_id(challenge_ids, preds):\n","    grouped_preds = {}\n","    for (challenge_id, order), pred in zip(challenge_ids, preds):\n","        if challenge_id not in grouped_preds:\n","            grouped_preds[challenge_id] = []\n","\n","        # Check if we already have a prediction for this order\n","        existing_pred = next((p for p in grouped_preds[challenge_id] if p[0] == order), None)\n","\n","        if existing_pred:\n","            # If we have a duplicate (same id and order), choose any (here, we keep the first one)\n","            continue\n","        else:\n","            # Add the new prediction with its order\n","            grouped_preds[challenge_id].append((order, pred))\n","\n","    # Sort predictions by order for each challenge_id\n","    for challenge_id in grouped_preds:\n","        grouped_preds[challenge_id].sort(key=lambda x: x[0])\n","        # Remove the order information, keeping only the predictions\n","        grouped_preds[challenge_id] = [pred for _, pred in grouped_preds[challenge_id]]\n","\n","    return grouped_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T03:44:53.945821Z","iopub.status.busy":"2024-09-21T03:44:53.945378Z"},"trusted":true},"outputs":[],"source":["pred_results = predict(model, tokenizer, dataset[\"predict\"], batch_size=1)\n","grouped_preds = group_preds_by_challenge_id(pred_results[\"ids\"], pred_results[\"preds\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(grouped_preds)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# compare solutions with sample_submission.json\n","with open(f\"{BASE_PATH}/arc-prize-2024/sample_submission.json\", \"r\") as json_file:\n","    sample_submission = json.load(json_file)\n","\n","# Check if all challenge_ids in sample_submission are in grouped_preds, and all tests have correct number of predictions\n","# also check if all predictions are 2d matrices of at least 1x1 size\n","for challenge_id in sample_submission:\n","    if challenge_id not in grouped_preds:\n","        print(f\"Challenge ID {challenge_id} in sample_submission is not in grouped_preds.\")\n","    elif len(grouped_preds[challenge_id]) != len(sample_submission[challenge_id]):\n","        print(\n","            f\"Challenge ID {challenge_id} in sample_submission has {len(sample_submission[challenge_id])} predictions, but grouped_preds has {len(grouped_preds[challenge_id])}.\"\n","        )\n","\n","    for pred in grouped_preds[challenge_id]:\n","        if not isinstance(pred, dict):\n","            print(f\"Challenge ID {challenge_id} in sample_submission has invalid predictions: {pred}\")\n","            continue\n","        if not isinstance(pred[\"attempt_1\"], list) or not isinstance(pred[\"attempt_2\"], list):\n","            print(f\"Challenge ID {challenge_id} in sample_submission has invalid predictions: {pred}\")\n","        if pred[\"attempt_1\"] is None or pred[\"attempt_2\"] is None:\n","            print(f\"Challenge ID {challenge_id} in sample_submission has invalid predictions: {pred}\")\n","        elif pred[\"attempt_1\"] is None or len(pred[\"attempt_1\"]) < 1 or len(pred[\"attempt_1\"][0]) < 1:\n","            print(f\"Challenge ID {challenge_id} in sample_submission has invalid predictions: {pred['attempt_1']}\")\n","        elif pred[\"attempt_2\"] is None or len(pred[\"attempt_2\"]) < 1 or len(pred[\"attempt_2\"][0]) < 1:\n","            print(f\"Challenge ID {challenge_id} in sample_submission has invalid predictions: {pred['attempt_2']}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open(\"submission.json\", \"w\") as json_file:\n","    json.dump(grouped_preds, json_file)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":8951125,"sourceId":67357,"sourceType":"competition"},{"datasetId":5123959,"sourceId":8622192,"sourceType":"datasetVersion"},{"datasetId":5657270,"sourceId":9335918,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelId":122615,"modelInstanceId":98435,"sourceId":117680,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
